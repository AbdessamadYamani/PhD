“What’s my model inside of?”:
Exploring the role of
environments for grounded
natural language understanding
Thesis for the degree of
“Doctor of Philosophy”
by
Ronen Tamari
Submitted to the Senate of the Hebrew University of Jerusalem
October 2023arXiv:2402.02548v1  [cs.CL]  4 Feb 2024

This work was carried out under the supervision of
Prof. Dafna Shahaf and Prof. Reut Tsarfaty

Acknowledgments
First, I would like to thank my advisors, Prof. Dafna Shahaf and Prof. Reut
Tsarfaty. Dafna provided sharp and constructive criticism as well as patience
and support. Both aspects have been instrumental to my academic growth. I
am especially appreciative of her willingness to let me follow my heart on far-
ranging explorations, and the trust she placed in me. She also inspired me with
her super-human clarity during 3am paper writing sessions before submission dead-
lines. Reut opened my mind to the intriguing concept of semantics, and relations
between natural and formal languages. She connected me to mainstream NLP and
encouraged me to pay attention to up-and-coming pre-trained models while I was
pre-occupied with other topics, and this turned out to be fateful advice. She quite
literally helped me ground my thinking around grounding in natural language, and
I am grateful to her for her support and “yes, and” approach to research. I am
very fortunate to have had the guidance of both Dafna and Reut throughout my
PhD.
I am grateful for the support of my committee, Dr. Gabriel (Gabi) Stanovsky
and Jason Baldridge. Beyond a committee member, Gabi mentored me during my
first internship at the Allen Institute for Artificial Intelligence in Seattle. Gabi
taught me to appreciate clarity and simplicity and showed me how to apply the
KISS principle in my research. He helped me advance my research far beyond
where I could take it alone, and connected me with Prof. Alan Ritter and his
PhD student Fan Bai at Georgia Tech, who I am also grateful to have collabo-
rated with. Gabi was very generous to me in and out of the lab, and inspired
me to try and pay it forward. Jason was a great inspiration to me as I started
my PhD journey. Watching his fantastic keynote at the 2019 NeurIPS conference
was a formative experience that deeply shaped my research interests. I especially
appreciated his inter-disciplinary approach and ability to skillfully integrate lin-
guistics and cognitive science research with complex computational modelling and
engineering challenges.
I also want to extend a special thanks to Dr. Kyle Richardson, who was not
officially on my PhD committee but very well could have been - he was also a friend
and mentor who I met in Seattle and later collaborated with. From Kyle I learned
what it takes to build NLP pipelines at industrial scale and precision, and I would
not have been able to develop and run experiments on large pre-trained models
without his significant help. Kyle invested significant time in helping me develop
my ideas at both the conceptual and empirical level and I benefited greatly from
being able to work closely with him.
I feel very fortunate to have worked with a wide group of collaborators at He-
brew University, especially Dr. Tom Hope, Chen Shani and Oren Sultan. Thanks
to Prof. Omri Abend for helping me in my first orientation steps in cognitive sci-
ence and linguistics. Thanks to all of Dafna Shahaf’s Hyadata Lab, which was my
home away from home. Indeed, during some stretches of my PhD, I spent more
time there than at home. Thanks especially to lab-mates Tom, Chen and Moran
Mizrahi for their friendship and inspiration.
I was also very fortunate to begin my PhD journey in a summer internship
at the RIKEN Institute in Japan, working with Prof. Yuji Matsumoto at the
Nara Institute of Technology (NAIST). In Japan I learnt the haiku “distant minds
meet, cherries blossom,” which describes well my time there and since. Thanks
to Prof. Matsumoto for providing me the generous support, peaceful setting and
challenging problem which sowed the seeds of inspiration for the rest of my PhD.
My experiments around the Dyna-bAbI benchmark required large-scale engi-
neering and experimentation efforts I would not have been able to undertake alone.
Thanks to Noam Kahlon and Aviad Sar Shalom who volunteered time and effort
to help me build the Dyna-bAbI data generator. Thanks also to Nelson Liu at
Stanford who pitched in to help with running experiments on the new dataset we
created.
Thanks to Prof. Judy Fan and her wonderful cogtools lab at the University
of California in San Diego (now Stanford). Judy hosted me generously for an
exchange program, and I deeply appreciated the opportunity to gain a cognitive
science perspective on NLP.
For the last phase of my thesis, I am very grateful to have partnered with
William Fischer and Lauren Hebert from Veeo who inspired me to go all in on
collective sensemaking. I am also grateful to Matan Field from DAOstack who
supported me and connected me with the wonderful, wild world of decentralized
collaboration. Special thanks to Daniel Friedman at UC Davis who introduced me
to the mind-expanding concept of stigmergy and who has continued to expand my
mind at a regular basis since then. With the closing of my thesis I have gained
new appreciation and wonder at the truly collective nature of intelligence, but a
side effect is a feeling of frustration with the incompleteness of acknowledgements.
We are but individual “neurons” often pitifully unaware of our role in the larger
collective network. Thanks to all those many friends, colleagues and collaborators
who have been a part of this network and supported me throughout.
A special thanks to my close and extended family who have supported me
faithfully through what at times seemed a never-ending journey. To my parents
Cathy and Yoav, who reminded me to get out and breathe fresh air once in a
while. To my brother Natan, who got me hooked on simulations and multi-agent
systems (SimCity, Civilization, etc) somewhere around preschool. Finally, this
thesis owes its existence due to my wife and love of my life, Shiran. We met
during an internship in Seattle, and my life has changed forever since, and with it
my thesis too. I can’t imagine either without you.

Abstract
In recent years, deep learning based approaches to natural language processing
(NLP) have made impressive progress. A particularly important achievement has
been the development of Large Language Models (LLMs), massive artificial neu-
ral networks trained on internet scale linguistic data. LLMs have demonstrated
remarkable performance across a wide variety of tasks, including long standing
challenges like few-shot learning and coherent long-form text generation.
LLMs seem to be doing more than just “processing” natural language (NLP),
perhaps they are also understanding natural language (NLU)? Indeed, a core on-
going debate being fueled by these advances is the Symbol Grounding Problem;
computational models of language process only linguistic input (symbols), so how
can their outputs be grounded to the external world to which the language refers
to? Or in other words, since meaning also includes extra-linguistic referents (ac-
tions, percepts, semantic knowledge), can LLMs reliably understand language, i.e.,
extract the meaning conveyed by linguistic symbols? The debate is far from set-
tled, and has significant implications for guiding the future of NLP research as well
as real-world applications; some are claiming LLM research is climbing the wrong
hill altogether, some advocate for more cognitively inspired architectures, while
others believe that LLMs are early demonstrations of so-called “Artificial General
Intelligence”.
This PhD thesis proposes a novel environment-oriented perspective on the lan-
guage grounding debate and towards NLU research more broadly. Our approach
is inspired by ecological accounts of cognition, which provide a more holistic ac-
count of the role of the body and environment in shaping cognition, in contrast to
classical cognitive science which focused on studying brains in isolation. Similarly,
we adopt an ecological approach to NLU research. Where current NLU research
tends to focus on models in isolation, we developed a more holistic perspective
accounting for the deep yet under-explored coupling between models and the envi-
ronments with which they interact. Wittgenstein famously wrote that “The limits
of my language mean the limits of my world”. The thesis can be stated simply, in
an inversion of that dictum, that “The limits of my environments mean the limits
of my language (models)”. Environments are the computational spaces in which
i
models are embedded, which support data annotation as well as model training,
development and evaluation. The ecological perspective provides novel insights
on each of those critical components of the NLU research pipeline, and also con-
tributes to the broader debate around how to pursue more reliable and grounded
natural language understanding systems.
The thesis is divided into three parts. The first part synthesizes research from
cognitive science, linguistics, reinforcement learning and NLU to propose a con-
ceptual framework for the idea of ecological natural language understanding. The
framework highlights metaphor comprehension, world model learning and men-
tal simulation as core capacities of importance for achieving human-like natural
language understanding. The framework makes predictions about the limitations
of statistical language models (such as LLMs) with respect to these abilities, and
also about the limitations of benchmarks used to evaluate them. Importantly, the
ecological NLU approach suggests that many debates about language grounding
are hamstrung by the lack of appropriate benchmarks for evaluation of language
understanding, both in terms of complexity and rigor. The framework is used to
outline an NLU research roadmap for addressing the described limitations.
The second part of the thesis begins advancing on the roadmap described in
the first section, applying the theoretical framework towards more practical appli-
cations. We developed text-based game environments supporting novel training
and annotation methods for procedural text understanding. We found that the
more detailed annotations made possible using text based games enabled more
faithful modelling of process-level comprehension tasks, compared with existing
approaches that addressed mainly sentence-level comprehension. We also used
text-based games to construct a new benchmark for measuring the progress of lan-
guage models on challenging commonsense reasoning tasks. We used the bench-
mark to identify limitations of state-of-the-art models in tasks requiring com-
positional generalization, and explored data augmentation strategies to improve
models’ generalization. Finally, we leveraged the richer supervision provided by
text-based game environments to develop Breakpoint Transformers, an extension
of the Transformer architecture designed to model intermediate semantic informa-
tion in long narrative or procedural texts. We applied Breakpoint Transformers on
a challenging common-sense reasoning task and achieved significant improvements
ii
over existing approaches, both in terms of accuracy (close to 300% performance
increase in the primary sub-task) as well as architecture generality.
In the third and final part of the thesis, we explored the implications of eco-
logical cognition for the design of online epistemic environments for humans. If
indeed environments play such an integral role in human (and machine) cogni-
tion, perhaps many of the “epistemic ills” plaguing societies (e.g., polarization,
misinformation) today can be traced back to the online social media environments
from which humans increasingly acquire their information about the world? What
would healthier epistemic environments look like? We integrated Semantic Web
research with theories from epistemology and distributed cognition to provide a
novel, ecological perspective, on collective intelligence (or lack thereof). We high-
lighted risks inherent to current centralized social media platforms and proposed
a new kind of epistemic environment addressing their limitations, in the form of
decentralized, AI-augmented collective intelligence networks.
iii
Contents
1 Introduction 1
1.1 Environments: the cognitive, the computional and the meta . . . . 3
1.1.1 Environments in ecological cognitive science . . . . . . . . . 3
1.1.2 Environments in cognitive linguistics . . . . . . . . . . . . . 4
1.1.3 Environments in reinforcement learning . . . . . . . . . . . . 5
1.1.4 How are environments seen (or not) in AI research? . . . . . 7
1.2 Ecological Natural Language Understanding . . . . . . . . . . . . . 8
1.2.1 Incorporating environments into NLU theory . . . . . . . . . 8
1.2.2 Empirical applications: putting environments into practice . 9
1.3 Environments for AI-augmented collective human thinking . . . . . 12
2 Language (Re)modelling: Towards Embodied Language Under-
standing 14
3 Playing by the Book: An Interactive Game Approach for Action
Graph Extraction from Text 29
4 Process-Level Representation of Scientific Protocols with Inter-
active Annotation 40
5 Dyna-bAbI: unlocking bAbI’s potential with dynamic synthetic
benchmarking 54
6 Breakpoint Transformers for Modeling and Tracking Intermediate
Beliefs 77
iv
7 From Users to (Sense)Makers: On the Pivotal Role of Stigmergic
Social Annotation in the Quest for Collective Sensemaking 95
8 Discussion and Conclusions 102
A Ecological Semantics: Programming Environments for Situated
Language Understanding 114
v
vi
Chapter 1
Introduction
“Ask not what’s inside your head, but what your head’s inside of.” –
W.M. Mace
In recent years, deep learning based approaches to natural language processing
(NLP) have made impressive progress. A particularly important achievement has
been the development of Large Language Models (LLMs), massive artificial neu-
ral networks trained on internet scale linguistic data. LLMs have demonstrated
remarkable performance across a wide variety of tasks, including long standing
challenges like few-shot learning and coherent dialog and long-form text genera-
tion [1, 2, 3].
The success of LLMs has significant practical and theoretical implications. On
the practical side, LLMs are increasingly capable of performing natural language
“understanding” (NLU) tasks, beyond merely natural language processing (NLP).
We use “understanding” to distinguish between human-like understanding and
the more technical usage of the term in computational linguistics1; NLP tasks
traditionally focus on simpler supporting operations like part-of-speech tagging or
entity recognition, whereas NLU tasks involve extracting some form of meaning
from an utterance (e.g., mapping a natural language question to a formal database
query) [5]. On the theoretical side, LLMs’ success has fueled long-standing debates
about the nature of linguistic meaning [6], leading many cognitive scientists and
linguists to update their theories of language production and understanding.
1See also Melanie Mitchell on the use of “wishful mnemonics” by AI researchers [4].
1
These advances also raise pressing questions. For practical NLU applications,
despite their impressive performance, LLMs still display a stubborn tendency for
commonsense reasoning failures and hallucination of fake facts [7, 8, 9]. As ob-
served by John Oliver, “The problem with AI isn’t that it’s smart, but that it is
dumb in ways we can’t predict.”2Can we get better at predicting the limitations
of LLMs? In what settings can they be safely deployed, and how can their relia-
bility be improved? These more practical questions are in turn informed by more
philosophical and theoretical questions: what is linguistic meaning, and how is
meaning conveyed by linguistic utterances? To what degree is meaning reducible
to statistical patterns in language? What does it mean to understand language,
and are machines capable of human-like language understanding? What model
architectures might achieve more human-like NLU capabilities?
Our thesis contributes to both the practical and theoretical questions, by in-
troducing a novel, ecological perspective on natural language understanding. Our
approach is inspired by ecological accounts of cognition and linguistics. Common
to those theories is a more holistic account of the role of the environment in shap-
ing cognition and language, in contrast to classical cognitive science which focused
on studying brains in isolation. We observe a similar parallel in NLU research,
where most of the focus in mainstream research is on modelling artifical neural
networks, and environments fade unnoticed into the background. In our thesis,
we ask what happens when we foreground environments, and treat them as “first-
class research citizens” alongside models. In particular, the thesis addresses the
following research questions:
•Chapter 2: how can we account for the role of the environment in a compu-
tational framework of natural language understanding?
•Chapters 3,4: how can environments inform more realistic training, evalua-
tion and annotation methods in procedural text understanding tasks? How
can such environments be created efficiently?
•Chapter 5: what are the world modelling capabilities of neural language
models, and how can we judge if a world modelling benchmark is effective?
2https://www.youtube.com/watch?v=Sqa8Zo2XWc4
2
•Chapter 6: how can existing LLM architectures be extended to better incor-
porate world-modelling capabilities without harming existing performance
or compromising on computational efficiency?
•Chapter 7: how can networked annotation tools support AI-augmented col-
lective (human) sensemaking environments, as a healthier and more open
alternative to centralized social media platforms with opaque data and algo-
rithms?
The following sections provide further background on environments to motivate
and contextualize these questions: §1.1 discusses environments in cognitive science,
linguistics and reinforcement learning, as well as from a meta-science perspective
asking how environments are seen by AI researchers. §1.2 discusses how the thesis
translates those accounts translate into current NLU research, and §1.3 discusses
environments for social learning and sensemaking in humans.
1.1 Environments: the cognitive, the computional
and the meta
1.1.1 Environments in ecological cognitive science
The human brain is perhaps the most complex object in the known universe, and
as a result, has been the focus of intense research spanning many disciplines, from
philosophy through cognitive science, neuroscience, and increasingly also artificial
intelligence. A realization emerging across these fields is that brains and cognition
cannot be understood in isolation; “cognition does not occur exclusively inside
the head” [10]. Rather, the explanation of many cognitive phenomena (including
language) necessitates a more ecological approach accounting for the role of the
body and wider environment.3While the simple observation that bodies and
environments matter may seem trivial at first glance (brain need bodies as an
3We use ecological cognitive science for brevity to refer to a range of approaches sharing the
ecological focus, including ecological psychology, 4E-cognition (embodied, embedded, extended
and enactive), grounded cognition and embodied cognitive linguistics. See Chapter 2 and Ap-
pendix A for more background.
3
energy supply), the ways anddegree to which they matter often turn out to be
surprising, and with radical implications for both theory and methodology.
Outfielder Problem: an example of ecological cognition in action
An illustrative example is the “outfielder problem:” [11, 12] how does a baseball
outfielder catch a fly ball? A traditional “in-the-head cognition” approach would
attempt to solve the problem assuming that all that was available to solve the
problem was the brain. It would break the problem into subproblems correspond-
ing to cognitive modules implemented by the brain: a sensor module to detect the
ball, an inference module to calculate its trajectory given a physics world model,
and an action planner to produce and execute a motor plan to bring the player
to the predicted landing site. Such an approach is plausible on the surface, but
requires highly accurate measurements that are difficult to estimate in practice.
Ecological cognition takes a radically different perspective (which turns out to be
an accurate account for how real baseball players solve the problem), and asks -
what if the body and the environment were part of the solution? In baseball terms:
as long as you keep your eye on the ball and move to keep your gaze steady, then
the ball will fall into a glove brought in front of your face. The outfielder is leverag-
ing rich and constant feedback from the environment, as well as the ability to act
all the while: simply moving so as to make the world appear a certain way now,
which leads them to be in the right place later. This example demonstrates how
ecological cognition embodies (literally) a very different approach both in terms
of theory and methodology; imagine how different a robot outfielder design would
look whether informed by ecological or traditional cognitive science.
1.1.2 Environments in cognitive linguistics
The field of linguistics has also been deeply impacted by ecological cognitive the-
ories. George Lakoff’s work [13, 14, 15], particularly in cognitive linguistics and
conceptual metaphor theory, has been instrumental in this transformation. Lakoff
argued that language is deeply rooted in our sensory experiences and bodily inter-
actions with the world. His exploration of metaphors, such as ”time is money” or
”love is a journey,” illustrated how abstract concepts are pervasively structured by
4
our embodied experiences, highlighting the fundamental role of the body and en-
vironment in shaping linguistic expressions. These theories ushered in a paradigm
shift that has challenged the traditional views advocated by cognitive scientists
like Noam Chomsky and Jerry Fodor, who championed more innate and abstract
approaches to language that did not account for the role of the environment [16].
Of particular interest in the context of NLU research, environments feature
centrally in the symbol grounding problem, a central debate in AI and cogni-
tive science [17, 6]. The debate seeks to understand how systems processing only
symbols, such as words, come to acquire the meaning, or semantics, of those sym-
bols. Ecological theories of cognition contend that language is grounded through
its referring to an extra-linguistic world of objects, actions, events and semantic
knowledge. The environment thus provides the context and sensory input that al-
lows symbols to become grounded in real-world experiences. These theories raise
significant questions for NLU research: computational models of language exposed
only to linguistic data have no direct experience of the world, thus it is unclear
whether they can reliably connect between linguistic inputs to the world the lan-
guage refers to. The degree to which environments are necessary is still an open
question: theories of indirect grounding [18] suggest that perhaps only some part
of language must be grounded directly to sensorimotor experience, while other
language can be grounded indirectly through this base.
1.1.3 Environments in reinforcement learning
Reinforcement Learning (RL) is one of the branches of AI in which environments
play the most prominent role. Indeed, RL is foundational to the sub-field of em-
bodied AI [19], which focuses on developing agents which can solve tasks involving
interaction in real (e.g., production line robots) or virtual environments (e.g., a
web browsing agent). As such, RL is a natural starting point for a formal com-
putational account of environments. The standard formulation of RL is given
by the Partially Observable Markov Decision Process [20], which describes the
agent-environment interaction.
As can be seen from Figure 1.1, the computational core of the environment is
the transition function, T. The transition function represents the world dynamics:
5
T st 
at St+1 
R st 
at rt O st ot O-1 st ot 𝜋 st at 
st 
ot at 
rt St+1 St+2 
ot+1 ot+2 at+1 at+2 
rt+1 rt+2 … Environment Agent 
Policy function 
Recognition function 
Reward function Observation function Transition function Agent-Environment Interaction 
(Partially Observable Markov Decision Process) 
T ̃ st 
at St+1 World model 
~ 
~ 
~ Figure 1.1: Agent-environment interaction dynamics, modelled by a Partially Ob-
servable Markov Decision Process (POMDP). Figure based on [21].
how will the world change given a particular action atin state st. In partially
observable environments, agents may not have access to the full world state; the
observation function Ois the process by which sensory data (the observation the
agent has access to) are generated from the world state st. For example, in a
3D computer game, stwill contain the full scene specification (objects, locations,
physical forces, etc) and otwill contain a rendering of those data as a frame of
pixels representing the visual field of the agent. Finally, environments may also
include a reward function R, providing agents with a reward signal rtbased on
their current action atand world state st. Agents interact with the environment by
means of a policy function, π, which generates an action atgiven the current world
state st. In the case of a partially observable environment, the agent must first
employ a recognition function O−1to parse the observation otinto recognizable
state st. Model-based reinforcement learning approaches employ a world model ˜T
that is learned through interaction with Tand can be used for mental simulation
and planning with mental states ˜ sand actions ˜ a[21].
Simulators are a classic kind of environment, used extensively in RL research.
Simulators include graphical interfaces (e.g., a 3D game engine) or can be purely
text-based (discussed at length in Chapters 2,3,4). Beyond simulators, environ-
6
ments include any interface that exposes an API (Application Programming In-
terface) that can be used by an agent.
Note that not all environment functions even need to be implemented to sup-
port learning algorithms. The next-word prediction training framework of LLMs
can also be framed as an environment, where actions and observations correspond
to words, and the learner is exposed to a stream of such observations. There is
no meaningful interaction in the sense that the next observation is independent of
the learner’s action (word prediction). Yet, as discussed in Chapter 5, even this
impoverished environment can drive powerful learning algorithms given enough
parameters and data.
1.1.4 How are environments seen (or not) in AI research?
Similar to traditional “in the head” cognitive science discussed in §1.1.1, main-
stream AI and NLP research are markedly model-centric [22, 23], focusing on
topics such as neural architectures, optimization methods and learning algorithms.
Interestingly, this focus is not just due to the perceived relative importance of
models, but rather has sociological factors as well. Data work, a close relative of
environment work, is “de-glamorised and undervalued” [22]; “everyone wants to
do the model work, not the data work,” despite wide-spread acknowledgement of
the critical importance of data work.
Work on environments by AI researchers is additionally complicated by a high
engineering overhead, limiting environment development mainly to industrial labs
or large-scale academic efforts. Furthermore, from a narrow academic disciplinary
lens, environment design falls outside of AI, and is closer to fields such as game
design or human-computer interaction (HCI).
Finally, recent years have seen mainstream AI and NLU research drifting apart
from other related fields such as cognitive science and neuroscience [24, 25], possi-
bly leading to under-appreciation of certain insights from those fields. For exam-
ple, in [25], an inter-disciplinary group of AI and neuroscience researchers recently
proposed updating the Turing Test (seen in AI as a canonical test of NLU) to an
“Embodied Turing Test,” to better reflect the importance of embodied interaction
within an environment as a measure of intelligence.
7
To summarize, for a variety of reasons, of which many are not scientific (but
rather sociological, meta-scientific, etc), environments are not yet seen as “first
class citizens” in AI or NLU research.
1.2 Ecological Natural Language Understanding
“Once you see the boundaries of your environment, they are no longer
the boundaries of your environment.” – Marshall McLuhan
Primed with the cognitive science background, a computational formulation,
and McLuhan, we can begin to see environments everywhere in NLU, and where
inattention to them is limiting research progress. The unifying thread of the
thesis is the identification of “environment bottlenecks” and experimentation with
environments to contribute to theoretical and empirical research questions in NLU
and AI.
1.2.1 Incorporating environments into NLU theory
In Chapter 2 (with extensions in Appendix A), we revisit the language ground-
ing debate with the ecological lens and propose a new conceptual framework for
grounded NLU systems, i.e., systems in which language acquires meaning in the
context of an external environment including events, actions, perceptions and the
mental models of interlocutors. The standard language modelling formulation used
in NLU considers language in isolation: a language model is, at its core, a prob-
abilistic model estimating the likelihood of a future utterance given a history of
past utterances. We expanded that formulation to an embodied language model,
which incorporates the mental models of communicators beyond just the linguistic
signals of the standard model. Incorporating mental models thus provides a more
realistic computational formulation, and also enables leveraging work on world
models from embodied AI (specifically, model-based RL) research, which account
for the role of the environment: world models are learned through embodied in-
teraction with the environment ( T). Language thus acquires meaning by its effect
on the (mental) world models of listeners situated in real or virtual environments.
8
Understanding can then be construed as a “meeting of the minds;” the speaker
and listener come to share a similar mental state, and the speaker is using words
(and their knowledge of the listener’s mental model) to “program the mind” or
elicit a desired mental state in the listener.
The theoretical framework above also suggests a roadmap for empirical re-
search, namely, creation of richer and more interactive training environments for
grounded NLU models, and using those environments for developing and evaluat-
ing world-modelling capabilities of models.
1.2.2 Empirical applications: putting environments into
practice
Procedural text understanding constitutes a natural starting point for empirical
research; procedures are typically situated in some environment in which the pro-
cedure is meant to be executed, for example instructions for furniture assembly
or a scientific experiment. Chapters 3 and 4 focus on the setting of procedu-
ral text understanding for laboratory experimental protocols. Prior work focused
mainly on extracting structured graph representations, called action graphs, from
unstructured procedural texts [26, 27]. In this setting, models are trained to la-
bel text spans and relations between them as the nodes and edges of the graph.
In Chapter 3 we present TextLabs, a novel, interactive approach to action graph
extraction, where procedural texts are interpreted as instructions in a text-based
lab simulator. Our approach provides a number of advantages compared to the
standard action-graph setting: support for interactive training methods like RL,
a causal, learnable world-model of lab operations, and the ability to generate syn-
thetic data of controllable complexity. We also implemented a simple RL agent as
a sanity check for our environment, and found it could successfully execute simple
procedures, while failing to complete longer and more complicated tasks.
In Chapter 4, we demonstrated a novel use of text-based games as annotation
environments. We found that action graphs, while useful for tasks like semantic
search, were not sufficient for other important downstream applications, such as
parsing of protocols to code for execution in robotic “cloud labs” [28, 29]. Here
too, we identified environments as the limiting factor; brat [30], the annotation
9
tool used to annotate action graphs, was suited for sentence-level, span-based
annotation, and not for converting texts into process-level and executable code.
Using TextLabs we annotated a new dataset of executable protocols, by recording
the action sequences of annotators. The text-based game environment enabled
collection of comparatively long and detailed action sequences, while capturing
complex phenomena such as long range co-reference, common-sense reasoning and
implicit arguments. We used the data to develop graph-prediction models, finding
them to be good at entity identification and local relation extraction, yet challenged
by long-range relations.
Evaluating and enhancing world-modelling capabilities of NLU systems
Chapters 5 and 6 explored questions related to world-modelling capabilities in
neural language models. World-modelling (also called situation modelling) refers
to the ability to construct a model of a situation described in natural language.
It requires common-sense knowledge about agents and objects as well as reason-
ing about the effects of events over time. World-modelling is a classic example of
an “environment bottleneck:” despite its importance as a cornerstone of human
cognition (Chapter 2), there is a notable lack of NLU benchmarks for rigorously
evaluating it. World-modelling benchmarks are challenging to create as they typ-
ically require a micro-world simulator environment that is both complex enough
to construct interesting tasks, and controllable enough to facilitate precise experi-
mentation. The nearest existing benchmark, bAbI [31], provided only limited task
complexity and controllability. In Chapter 5, we developed a new task generator
called Dyna-bAbI, and used it to create bAbI 2.0, a challenging new suite of tasks,
with a particular focus on compositional generalization, an important evaluation
setting absent from the original benchmark. We evaluated a wide array of models
including both specialized architectures developed for bAbI, as well as general pur-
pose pre-trained models such as RoBERTa [32] and T5 [33]. We found that while
pre-trained models far outperformed specialized models, neither class of model per-
formed well on the compositional generalization tasks. These results indicate the
limitations of those models as well as the data included in the original benchmark.
We explored ways to augment the original data, and found that though diversify-
10
ing training data was far more useful than simply increasing dataset size, it was
still insufficient for driving robust compositional generalization. We also evaluated
our new benchmark with a newly developed quality metric called concurrence [34].
We found that bAbI 2.0 significantly outperformed the original bAbI dataset and
achieved concurrence results on par with purpose-built high concurrence synthetic
benchmarks, as well as widely used natural language benchmarks. Our results un-
derscore the importance of highly controllable task generators for creating robust
NLU systems through a virtuous cycle of model and data development.
Chapter 6 leveraged the Dyna-bAbI task generator to improve the world-
modelling capabilities of existing NLU models. Drawing inspiration from the con-
cept of breakpoints in programming, we developed a new approach called Break-
point Modelling that enables models to learn representations of semantic infor-
mation at intermediate points throughout long-form text. Breakpoints are simply
special tokens (similar to the widely used [CLS] tokens) inserted after each sen-
tence, and the resulting encoded breakpoint vector bcan then be queried against
a natural language proposition vector encoding pto obtain a true/false/unknown
prediction for each ( b, p) pair. Our original goal was to explore whether providing
dense world-state annotations as supervision would improve compositional gener-
alization performance on bAbI 2.0. Though we observed only marginal gains on
compositional generalization tasks, breakpoints proved valuable for interpretabil-
ity, e.g., understanding model behavior and anticipating potential failure modes.
More importantly, we found that the breakpoint modelling training objective can
be added to the sequence to sequence loss as used in state-of-the-art Transformer
models like T5. Such models can be trained jointly to predict intermediate propo-
sitions alongside any existing capabilities like question answering, without harming
their original performance. This means that the resulting model, which we called
a Breakpoint Transformer, can answer questions and efficiently predict hundreds
of propositions in a single forward pass, as opposed to baseline methods which
can typically only predict one breakpoint-proposition pair per forward pass. We
applied Breakpoint Transformers to the TRIP benchmark [35], a multi-step reason-
ing task evaluating world-modelling capabilities using short stories with reasoning
“bugs” or implausible sequences of events (e.g., a telephone rang after it had been
11
unplugged). Our model obtained state-of-the-art performance on TRIP, including
20-30% absolute improvement on 2 out of the 3 sub-tasks. The TRIP experiments
additionally demonstrated the versatility of our approach: we converted the full 3-
task pipeline to breakpoint modelling format (proposition prediction and question
answering) without further architectural changes, whereas the baseline approach
involved task-specific architectural adaptations to a RoBERTa model.
1.3 Environments for AI-augmented collective hu-
man thinking
“If you want to teach people a new way of thinking, don’t bother trying
to teach them. Instead, give them a tool, the use of which will lead to
new ways of thinking.” – R. Buckminster Fuller
Chapter 7, the final chapter of the thesis, was, in many respects, written in
a very different world than the one from which I set out on my thesis journey in
2018. Outside the window of my lab the world was quite literally burning, rocked
by climate catastrophes, a global pandemic, and rapidly deteriorating social co-
hesion world-wide and in my home country of Israel. Rampant misinformation
and information weaponization highlighted the perils of outsourcing crucial soci-
etal communication infrastructure to centralized social media platforms with little
scientific oversight, and monopolistic control of data and algorithms. If indeed en-
vironments play such an integral role in human (and machine) cognition, perhaps
many of those “epistemic ills” plaguing societies (e.g., fragmentation, misinfor-
mation) today can be traced back to the online social media environments from
which humans increasingly acquire their information about the world? In Chapter
7 we ask, what would healthier epistemic environments look like, that support the
learning and sensemaking journeys of individuals and collectives? How to create
environments in which AI would serve to augment cognition rather than manipu-
late it for ad-driven engagement? Our proposal hinges on the concept of stigmergy
which originated from studies of collective intelligence in ant colonies [36]. Stig-
mergy is the phenomena of large scale coordination mechanism mediated by local
environment modifications (e.g., ant pheromone trails). Theories of stigmergic
12
cognition foreground the role of the environment, which functions as a distributed
memory system for a collective organism. Annotations also play a pivotal role in
stigmergy, as they function as markers of meaning (e.g., annotations such as likes
on social media are “digital pheromone trails”). We applied principles of stig-
mergy to propose a new kind of social network designed to address the limitations
of centralized platforms: an open, decentralized social annotation network where
participants control their annotations, and can share them with AI-driven content
discovery services. Decentralization provides resilience against platform capture,
and open data encourages a plurality of content discovery services, as opposed to
platforms’ monolithic feed algorithm.
13
Chapter 2
Language (Re)modelling:
Towards Embodied Language
Understanding
Ronen Tamari, Chen Shani, Tom Hope, Miriam Petruck,
Omri Abend, Dafna Shahaf
Published in the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2020
14
Language (Re)modelling: Towards Embodied Language Understanding
Ronen Tamari†Chen Shani†Tom Hope⋆∗
Miriam R. L. Petruck‡Omri Abend†Dafna Shahaf†
†The Hebrew University of Jerusalem
⋆Allen Institute for Artiﬁcial Intelligence
∗Paul G. Allen School of Computer Science & Engineering, University of Washington
‡International Computer Science Institute, Berkeley, CA
{ronent,chenxshani,oabend,dshahaf }@cs.huji.ac.il
tomh@allenai.org miriamp@icsi.berkeley.edu
Abstract
While natural language understanding (NLU)
is advancing rapidly, today’s technology dif-
fers from human-like language understanding
in fundamental ways, notably in its inferior
efﬁciency, interpretability, and generalization.
This work proposes an approach to represen-
tation and learning based on the tenets of em-
bodied cognitive linguistics (ECL). According
to ECL, natural language is inherently exe-
cutable (like programming languages), driven
by mental simulation and metaphoric map-
pings over hierarchical compositions of struc-
tures and schemata learned through embodied
interaction. This position paper argues that the
use of grounding by metaphoric inference and
simulation will greatly beneﬁt NLU systems,
and proposes a system architecture along with
a roadmap towards realizing this vision.
1 Introduction
“Not those speaking the same language,
but those sharing the same feeling under-
stand each other.” – Jalal ad-Din Rumi
While current NLU systems “speak” human lan-
guage by learning strong statistical models, they
do not possess anything like the rich mental repre-
sentations that people utilize for language under-
standing. Indeed, despite the tremendous progress
in NLU, recent work shows that today’s state-of-
the-art (SOTA) systems differ from human-like lan-
guage understanding in crucial ways, in particular
in their generalization, grounding, reasoning, and
explainability capabilities (Glockner et al., 2018;
McCoy et al., 2019a,b; Nie et al., 2019; Yogatama
et al., 2019; Lake et al., 2019).
Question-answering (QA) is currently one of
the predominant methods of training deep-learning
models for general, open-domain language under-
standing (Gardner et al., 2019b). While QA is a ver-satile, broadly-applicable framework, recent stud-
ies have shown it to be fraught with pitfalls (Gard-
ner et al., 2019a; Mudrakarta et al., 2018). A recent
workshop on QA for reading comprehension sug-
gested that “There is growing realization that the
traditional supervised learning paradigm is broken
[...] – we’re ﬁtting artifacts” (Gardner, 2019).
In many respects, the problems of NLU mirror
those of artiﬁcial intelligence (AI) research in gen-
eral. Lake et al.’s (2017a) seminal work identiﬁed
a signiﬁcant common factor at the root of problems
in general AI. The current deep-learning paradigm
is a statistical pattern-recognition approach predom-
inantly applied to relatively narrow task-speciﬁc
prediction . In contrast, human cognition supports
a wide range of inferences (planning, action, ex-
plaining, etc.), hinting at a view of intelligence fo-
cused on model-building , speciﬁcally, mental mod-
els: rich, structured, manipulable, and explainable
representations useful for performing in dynamic,
uncertain environments. This distinction motivates
the quest for a new cognitively-inspired model-
building learning paradigm for general AI, which
has inspired fruitful subsequent research and dis-
cussion (e.g., Lake et al. (2017b)).
The observation that NLU and general AI share a
common central problem (task-speciﬁc prediction-
based learning), and the growing realization that
deeper text understanding requires building men-
tal models (Gardner et al., 2019a; Forbes et al.,
2019), motivate the search for an NLU analog of
the cognitively-inspired model building paradigm.
Amid recent position papers highlighting signif-
icant differences between human language under-
standing and current NLU systems (McClelland
et al., 2019; Bisk et al., 2020), here we take a more
focused look at mental models; challenges arising
due to their embodied nature, their importance in
general NLU, and how we might begin integrating
them into current approaches.
Mainstream NLU work, be it entirely distribu-
tional, such as BERT (Devlin et al., 2019), or also
involving symbolic knowledge representation (Liu
et al., 2019a; Bosselut et al., 2019), seldom ad-
dresses mental models directly. Crucially, such ap-
proaches lack the interactive worlds within which
mental models1are learned jointly through lan-
guage and embodied action. The most closely
related lines of work to the present proposal are
grounded approaches, which feature worlds in the
form of interactive environments, and address map-
ping text to programs (executable semantic parses)
(e.g., Mooney, 2008; Artzi and Zettlemoyer, 2013;
Gauthier and Mordatch, 2016; Liang, 2016; Kiela
et al., 2016; Chevalier-Boisvert et al., 2019). How-
ever, while well-aligned with a model-building
paradigm, typically such approaches have been lim-
ited to short or synthetic literal language and nar-
row domains assuming predeﬁned environments.
Embodied approaches to general NLU, as advo-
cated here, are few and far between. Mostly, exam-
ples fall under the construction grammar frame-
work (Steels and de Beule, 2006; Bergen and
Chang, 2005). However, despite their intellectual
merit, they were not operationalized to scale readily
for mainstream applications (see §3).
This position paper argues that executable se-
mantic parsing and grounded approaches to NLU
constitute a ﬁrst step in a much larger program,
whose outline is set forth, for general language un-
derstanding through embodied cognitive linguis-
tics (ECL) . Following much cognitive science re-
search (see§3,§4), this paper posits that (1) execu-
tion or simulation is a central part of semantics,
essential for addressing some of the persistent difﬁ-
culties in text understanding, and (2) metaphoric
inference capabilities are central to knowledge
representation, and facilitate grounded understand-
ing of general language. Importantly, capacities for
both simulation and metaphor are emergent, borne
of embodied interaction within an external world.
Our contributions are: we analyze inherent limi-
tations of SOTA statistical language models applied
to NLU and propose a framework to address these
limitations. The novelty of this approach stems
from bringing together ideas from the cognitive
science literature, the wider AI community, and
NLU. This framework constitutes a path to general-
ize current execution-based methods towards more
1Typically, mental models are construed as “world simula-
tors”; see §3.
The world contains 2 crates. Each crate contains 4 boxes.
Oranges and apples are objects. Each box may contain up
to 5 objects. Objects can be moved from one box to
another . Objects can be removed from boxes or crates.
There are two apples in the ﬁrst box in the ﬁrst crate. There
is one orange and one apple in the second box of the
second crate. First, the apples were transferred from the
ﬁrst box of the ﬁrst crate to the ﬁrst box of the second crate.
Next, all apples were removed from the second crate.
Initial W orld StateC1 C2Figure 1: Open-domain challenge – a world with
boxes, crates and objects.
general language understanding.
This paper proposes a system architecture and a
roadmap towards implementing the vision outlined
here, suggesting preliminary directions for future
work (learned world models, incorporating interac-
tion into datasets). We believe that this framework
will facilitate consolidation with multiple related
lines of research across the different communities,
particularly embodied AI and NLU (Luketina et al.,
2019).
2 Challenges for Current NLU Systems
This section presents concrete example problems
demonstrating inherent limitations in SOTA NLU.
2.1 Open-domain Literal Language
Simulation
Fig. 1 includes a short story about a world with
crates, boxes, and objects inside them. It is a short
and simple narrative, far from capturing the full-
blown complexity of natural language. Following
Gardner et al. (2019a), we assume that a system
understands the story if it can correctly answer ar-
bitrary questions about it. To do so requires basic
commonsense and mathematical reasoning, refer-
ent grounding, tracking events, handling declara-
tive knowledge, and more.
The task is similar to narrative comprehension
tasks in datasets such as bAbI (Bordes et al., 2015)
and SCONE (Long et al., 2016), and could be
solved given large amounts of annotated training
data. But, the goal here is different, speciﬁcally, to
develop models that, like humans, can understand
such language on-the-ﬂy (like zero-shot learning).
QA approaches. Current QA systems, used in
an off-the-shelf manner, do not generalize well
to tasks on which they have not been trained;
NLU models are known to be brittle even to slight
changes in style and vocabulary (Gardner et al.,
2020; Keysers et al., 2020). The closest QA setting
is the DROP challenge (Dua et al., 2019), requiring
reading comprehension and basic numerical reason-
ing over paragraphs. As a simple sanity check, we
tested a near-SOTA model and baseline2on this ex-
ample, asking questions about the initial and ﬁnal
state. The models were notably better answering
questions about the initial state than about the ﬁnal
state. This result is perhaps expected, as the an-
swers to questions about the initial state are closer
to the input text. Answering questions about later
states is more challenging. A key missing compo-
nent of these systems is the ability to simulate the
effects of actions, especially commonsense effects
(e.g., moving a container moves the elements in it).
Executable semantic parsing approaches. The
problem of Fig. 1 could also naturally be cast as an
executable semantic parsing task. Similar tasks al-
ready exist, for example, the “Alchemy” sub-task of
the SCONE dataset features beakers of chemicals
that are mixed, poured, and drained. Executable
approaches can leverage simulation to learn struc-
tured world models, but are limited by hard-coded,
domain-speciﬁc executors; adding tasks requires
substantial manual effort.
For humans, through largely subconscious
metaphorical inference (related to transfer and
meta-learning in general AI (Lake et al., 2017a)),
it is obvious that both SCONE and Fig. 1 share
much the same structure. This similarity allows for
effortless generalization, effectively re-purposing
a relatively simple executor (for literal language)
ﬂexibly across many tasks.
2.2 Non-literal Language
The previous challenge involved literal language,
amenable to symbolic execution. However,
non-literal language is pervasive in everyday
speech (Lakoff and Johnson, 1980). Consider the
example in Fig. 2: the phrase “head of the French
Army” is non-literal, implying that the army can
be treated as a human body. The execution seman-
tics of verbs like “attacked” and “defend” are also
non-literal; they are highly contextual, requiring
interpretation beyond word-sense disambiguation
alone. “Russian hackers attacked the Pentagon
networks” or “The senator attacked the media” en-
tail very different simulations . This ambiguity is
2Segal et al. (2019) and Dua et al. (2019), respectively.
COUNTER FORCE
French Army
NapoleonHEAD  of
Attack
FORCE, MOTION
Fort
BODYLOCA TION"Napoleon, the head of the French Army , attacked the 
Russian fort,      but found it well defended
and had to turn back."
Russian Army
HEAD  of
French ArmyRussian Army
Fort
BODY ABOR TED ACTIONNapoleon
HEAD  of
DefendFrench ArmyRussian Army
Fort
BODY1
2
3
1
2
3LOCA TION
LOCA TION NapoleonFigure 2: Non-literal language challenge. To un-
derstand this sentence, humans rely on metaphoric in-
ference over embodied concepts (in blue, also called
schema; see§3). For example, here “attack” evokes a
FORCE or MOTION schema, used to construct a men-
tal model of the scene via mental simulation ( §4).
challenging for non-neural (symbolic) simulation-
based approaches. Humans compose a structured
mental model from the language through schemata
and mental simulation, as discussed in §3,§4.
To summarize, the limitations outlined above
motivate the attempt to extend the capability of
simulation to general linguistic inputs. Doing so
would enable the construction of grounded, manip-
ulable, and interpretable representations from text.
Two desiderata follow from the challenges: (1)
more ﬂexible utilization of symbolic executors by
exploiting shared (analogical) structures between
texts (§2.1), and (2) learned, neural executors for
non-literal language comprehension ( §2.2).
3 Embodied Cognitive Linguistics: A
Model Building Paradigm
Turning to cognitive science for inspiration, we fo-
cus on embodied cognitive linguistics (ECL), an im-
portant paradigm directly addressing both desider-
ata. This section presents a brief overview and key
tenets of ECL, speciﬁcally the theoretical founda-
tions Lakoff and Johnson (1980) and Feldman
and Narayanan (2004) developed. Most contem-
porary cognitive accounts of language incorporate
concepts from ECL to some degree. A full review
is out of scope of this work; see G ¨ardenfors (2014)
and§4,§5 for discussion in the NLU context.
Early cognitive theories assumed a disembod-
ied, symbolic representation of knowledge (Lewis,
1976; Kintsch and Van Dijk, 1978), separate from
the brain’s modal systems (vision, motor con-
trol, etc.). In contrast, the embodied cognition
(EC) view, based on widespread empirical ﬁnd-
ings, focuses on the role of the body in cogni-
tion. In this view, knowledge is stored using multi-
modal representations (mental imagery, memories,
etc.) that arise from embodied experience and ac-
tion in the world (Barsalou, 2008; Profﬁtt, 2006).
ECL postulates that linguistic representations and
other, higher-level cognitive functions are deeply
grounded in neural modal systems (Lakoff and
Johnson, 1980; Barsalou, 2008). This view is com-
pelling, as it addresses the grounding problem (Har-
nad, 1990) by linking between high-level symbolic
constituents of mental representations and experi-
ence or action in the physical world (Varela et al.,
2017). Note that embodiment is far from an end-all
for language comprehension: for example, social
and cultural aspects too are crucial (Arbib et al.,
2014). Still, ECL laid important conceptual foun-
dations also underlying subsequent accounts:
•Embodied schemata: Pre-linguistic structures
formed from bodily interactions and recurring
experience, such as CONTAINMENT, PART-
WHOLE, FORCE, MOVEMENT (Langacker,
1987; Talmy, 1985, 1983).
•Metaphoric inference3:The process by which
new information may be inferred via structural
similarities to a better-understood instantiated
system (Lakoff and Johnson, 1980; Gallese and
Lakoff, 2005; Day and Gentner, 2007). For ex-
ample, “I have an example IN mind” suggests
that the abstract concept mind is mapped to the
more concrete domain of containers .
•Mental simulation: The reenactment of percep-
tual, motor, and introspective states acquired
during experience with the world, body, and
mind. In EC, diverse simulation mechanisms
(also called mental or forward models (Rumle-
hart et al., 1986; Grush, 2004)) support a wide
spectrum of cognitive activities, including lan-
guage and decision making (Barsalou, 2008).
We believe that ECL is a useful paradigm for
addressing the challenges of §2, as it articulates
the role of analogy and mental simulation in NLU.
3Also called analogical reasoning, we use “metaphorical”
and “analogical” interchangeably.The following two ECL hypotheses summarize
them (Lakoff and Johnson, 1980; Feldman and
Narayanan, 2004):
Hypothesis 1 (Simulation): Humans understand
the meaning of language by mentally simulating its
content. Language in context evokes a simulation
structured by embodied schemata and metaphoric
mappings, utilizing the same neural structures for
action and perception in the environment. Under-
standing involves inferring and running the best
ﬁtting simulation.
Hypothesis 2 (Metaphoric Representation):
Human concepts are expressible through hierarchi-
cal, compositional, metaphoric mappings over a
limited vocabulary of embodied schema. Abstract
concepts are expressed using more literal concepts.
Early ECL Implementations. Early attempts to
implement ECL in actual language understand-
ing systems were founded on Narayanan (1997)’s
x-schema simulation framework and Embodied
Construction Grammar (Bergen and Chang, 2005).
While notable for approaching challenging prob-
lems involving mental simulation, and complex,
metaphoric language, early implementation efforts
were not operationalized to scale to mainstream
applications (Lakoff and Narayanan, 2010). These
works also focused on a particular type of sim-
ulation (sensorimotor), understood as only one
mechanism of many used in language understand-
ing (Stolk et al., 2016).
FrameNet (Ruppenhofer et al., 2016) and
MetaNet (David and Dodge, 2014) are closely
related projects in that each provides an exten-
sive collection of schemata used in everyday and
metaphoric language comprehension, respectively,
via the concept of a semantic frame (Fillmore,
1985). However, neither incorporates simulation
semantics, as needed for a full realization of the
ECL vision (Chang et al., 2002).
4 Linking ECL to NLU and Embodied
AI Research
We propose a unifying view of ECL, bringing it
closer to contemporary cognitive science and deep
learning approaches. This section presents nota-
tions and motivating intuitions, further developing
the computational framework in §5,§6. The pro-
posal centers around the view of natural language
as a kind of neural programming language (Lupyan
and Bergen, 2016), or higher-level cognitive con-
Concept Symbolic ECL Embodied AI
Primitives Basic data structures,
operators, variables...Schemata: MOVE,
CONTAINER,
PART-WHOLE... Deep neural world &
action representations
(learned through interaction)Knowledge Organization a) Composition, inheritence
b) Librariesa) Hierarchical,
compositional metaphoric
mappings
b) Compiled Knowledge
Executable Unit Instruction Semantic parse ˜a
Execution Trace Intermediate program states Mental models ˜T(˜s,˜a)
Simulation Executor Emulator† ˜T
Semantic parsing /
groundingParser to executable
symbolic programParser to executable neural
programO−1,π
Table 1: Natural language as a neural programming language conceptualization, with correspondence between
symbolic programming, ECL, and embodied AI, using standard POMDP notation. Tilde notation refers to internal
counterparts of T,s,a used in mental simulation.
†Also called mental simulation (Bergen and Chang, 2005), we adopt emulator (Glenberg, 2008) to conform with
contemporary cognitive science accounts.
trol system for systematically querying and induc-
ing changes in the mental and physical states of
recipients (Elman, 2004; Stolk et al., 2016; Borghi
et al., 2018). This approach builds on the ECL
hypotheses and suggests a broader view of mental
simulation, one that is readily amenable to the same
computational formulation as current embodied AI
and executable semantic parsing approaches.
Preliminaries. At the core of embodied ap-
proaches is the Partially Observable Markov De-
cision Process (POMDP; Kaelbling et al., 1998).
It governs the relations between states ( s), actions
(a), observations ( o), and rewards ( r). Of particular
interest are the recognition O−1:O→S, policy
π:S→A, and transition T:S×A→Sfunc-
tions. Focusing on mental simulation rather than
actual external action, we assume a degree of equiv-
alence between external and internal representa-
tions (Rumlehart et al., 1986; Hamrick, 2019). We
consider internal mental states and actions ( ˜s,˜a),
effecting change to mental models via a learned
neural emulator ˜T(Grush, 2004). Finally, lan-
guage is considered a form of action (Glenberg,
2008) via external and internal utterances (i.e., se-
mantic parses).
Connecting symbolic & embodied language un-
derstanding. Table 1 presents a structured version
of the neural programming language conceptualiza-
tion. Importantly, this view highlights the impor-
tant commonalities and differences between ECL
and both symbolic programming languages , as
well as embodied neural mechanisms , for percep-
tion and action. We illustrate these relations more
explicitly through a comparison between ECL andexecutable semantic parsing (Table 1, bottom).
Executable semantic parsing. Involves parsing a
novel linguistic input ointo a symbolic program
a, whose execution4yields a desired goal state:
T/parenleftbig
O−1(o),a/parenrightbig
=s∗. Executable semantic pars-
ing focuses on action in an external, symbolic en-
vironmentT, and typically doesn’t address ˜T, e.g.,
mapping a natural language question odirectly to
an executable query aon an SQL engine T.
ECL semantic parsing. Shares the same structure
as executable semantic parsing, with the impor-
tant distinction that simulation is enacted via inter-
nal neural representations: ˜T/parenleftbig
O−1(o),˜a/parenrightbig
= ˜s∗.
The fully neural formulation enables grounded un-
derstanding of non-literal language, demonstrated
here for the Fig. 2 example. Metaphoric infer-
ence (hyp. 2) facilitates parsing a novel linguis-
tic inputointointernal, structured, neural state
representations ˜s,˜a. Accordingly, the utterance
u=“Napoleon, the head of the French Army” might
be parsed to an internal state ˜scomposed of a PART-
WHOLE schema as shown in the ﬁgure. The phrase
“attacked the Russian fort” could be grounded to
a parse ˜adriving simulation over MOTION and
FORCE schemata. The requirement that ˜sand˜a
should afford mental simulation (hyp. 1) by the
neural world emulator ˜Tmarks an important dif-
ference from current neural word embeddings, one
that contributes to deeper language understanding;
in the resulting mental model ˜T(˜s,˜a), Napoleon
and the French Army likely moved together due
to the PART-WHOLE relation between them. This
4Slightly abusing notation, we apply Titeratively on a
sequence of actions a= (a0,...,a L−1).
inference is non-trivial since it requires implicit
knowledge (heads and bodies often move together).
Indeed, a SOTA NLI model5considers it “very
likely” that the Fig. 2 sentence contradicts the en-
tailment that “The French Army moved towards
the fort but did not enter it.” To summarize:
•Executable semantic parsing approaches address
grounding literal language to symbolic prim-
itives, whereas metaphoric inference suggests
a mechanism for grounding general language
using neural primitives (schemata).
•Executable semantic parsing approaches uti-
lizehard-coded, external symbolic executors ,
whereas ECL highlights the role of learned neu-
ral world emulators , as in current embodied
research AI efforts (see §7.2).
5 Proposal for an Embodied Language
Understanding Model
Formalizing the view characterized above suggests
a novel computational model of language under-
standing. While current statistical models focus
on the linguistic signal , research shows that most
of the relevant information required for under-
standing a linguistic message is not present in the
words (Stolk et al., 2016; David et al., 2016). Ac-
cordingly, the ECL view suggests shifting the focus
to the mental models that communicators use, and
the neural mechanisms used to construct them, e.g.,
mental simulation.
What follows here adapts a relevant cognitive-
inspired framework from general AI to the present
NLU setting (§5.1), and discusses computational
challenges (§5.2). Note that similar insights have
been applied to multi-agent communication prob-
lems (Andreas et al., 2017), but their application to
general NLU has been limited.
5.1 Formal Framework
The recently introduced Consciousness Prior (CP;
Bengio, 2017) is a framework to represent the men-
tal model of a single agent, through the notion of
abstract state representations .6Here, an abstract
state corresponds with ˜s(§4), a low-dimensional,
structured, interpretable state encoding, useful for
planning, communication, and predicting upcom-
ing observations (Fran c ¸ois-Lavet et al., 2019). One
5We use Liu et al. (2019b) with https://demo.
allennlp.org/textual-entailment/ .
6For brevity we omit discussion of deriving abstract states
from the full mental state, see Bengio (2017) for details.example is a dynamic knowledge graph embedding
to represent a scene (Kipf et al., 2020).
We adapt CP to a two-player cooperative lin-
guistic communication setting (Tomasello, 2008).
We assume a communicator ( A) and recipient
(B), as shown in Fig. 3. The computational
problem of communicators is a “meeting of
minds” (G ¨ardenfors, 2014), or achieving some
alignment of their mental models (Rumelhart,
1981; Stolk et al., 2016): the communicator A
wishes to induce in Bsome (possibly ordered) set
of goal abstract states G∗.
We leave exploration of the communicator side
to future work, and focus here on understanding.
We assume that Asequentially generates utterances
ut∈ U (we assume equivalence between utter-
ancesuand observations o) using an utterance
model (Bengio, 2017). Analogously, Buses a
comprehension modelCs.t.,˜st=C(˜st−1,ut).
We assume that alignment is possible: there exists
some sequence of utterances that will induce G∗.
This framework is readily applicable to static
text (reading comprehension). For example, in
Fig. 1,G∗would be the sequence of desired states,
and each sentence corresponds to an utterance
(u1=“The world contains 2 crates.”,...).
5.2 Computational challenges of embodiment
We can now more precisely characterize the chal-
lenges that the recipient faces. At the root of the
problem is the embodiment principle (Lawrence,
2017): human internal representations and com-
putation capacity, as represented by ˜sand˜T, re-
spectively, are many orders of magnitude larger
than their linguistic communication “bandwidth”.
We note that though ˜stis only a subspace of the
full mental state, following Stolk et al. (2016);
Bengio (2017) we assume that it still holds that
dim(˜st)/greatermuchdim(ut).The embodiment principle
dictates extreme economy in language use (Grice
et al., 1975), and results in three major challenges:
Common ground (prior world knowledge).
Meaning cannot be spelled out in words but rather
must be evoked in the listener (Rumelhart, 1981)
by assuming and exploiting common ground (Clark
and Schaefer, 1989; Tomasello, 2008), i.e., shared
structures of mental representations. In other
words, to achieve some aligned goal state g∗, the
communicators must rely heavily on pre-existing
similarities in ˜s,˜a, and ˜T. Developing computa-
tional versions of human world models ( ˜T) is likely
Mental Model
 
Intents
 
Linguistic 
ChannelUtterance Model
 Communicator  World State
 RecipientC2C2
"Remove all apples
from the second crate"Comprehension
 C1
1
2 34Figure 3: Schema of linguistic communication framework. Communicator’s intent (1) is a high dimensional
mental state, i.e., remove apples from the second crate. The low capacity of the linguistic channel (2) leaves the
burden of understanding primarily on Communicator and Recipient (embodiment principle). The Recipient’s goal
is to understand (3), i.e., reconstruct the intent by integrating linguistic input, knowledge of the state of the world,
and internal knowledge (memories, commonsense). Reconstruction results in a successful alignment (4).
AI-complete or close, but useful middle ground
may be attained by partial approximations.
Common ground (discourse). In the context of
discourse, new information must be accumulated
efﬁciently to update the mental model (Clark and
Schaefer, 1989; Stolk et al., 2016). Consider “Re-
move all apples from the second crate” (Figure 1).
Full comprehension is only possible in the context
of a sufﬁciently accurate mental model. Using our
previous notations, the comprehension of utde-
pends both on the previous utterances u1:(t−1)and
intermediate mental model ˜st−1.
Abstract vs. Literal Language. Interpretation
of literal language is relatively straightforward –
it is the language ﬁrst acquired by children, di-
rectly related to the physical world. However,
much of human language is more abstract, re-
lying on metaphors borne of embodiment. The
symbolic programming analog fails for utterances
like “these elections seem like a circus”. Sym-
bolic programming languages cannot handle non-
literal interpretations: how areelections like a
circus ? This is related to selective analogical in-
ference (Gentner and Forbus, 2011), closely related
to ECL: not everything in the source domain (cir-
cus) is mapped to the target (elections). Humans
easily perceive the salient metaphoric mappings
(clown→candidate ), but this feat remains ex-
tremely complex for machines.
6 Architecture Sketch
This section presents a schematic ECL-inspired ar-
chitecture towards the implementation of the com-prehension model (C), which addresses the chal-
lenges presented in §5.2. Fig. 4 shows the proposed
architecture. For simplicity, the focus is on a static
reading comprehension setting, but the architecture
supports richer environments as well.
6.1 Environment
The environment provides an “interaction API” to
the agent, as well as the reward signal. The sup-
ported interaction may vary considerably depend-
ing on the task; for reading comprehension, it al-
lows structured access to the text while support-
ing ﬂexible reading strategies (Yuan et al., 2019).
The ﬂexibility is important for long documents,
where navigation may be required (Geva and Be-
rant, 2018). For executable semantic parsing, there
might be external systems to interact with besides
the text, such as a database (Liang et al., 2016).
6.2 Agent
The agent architecture approximates the important
ECL functions outlined in §4, and consists of four
main modules:
Memory .We distinguish between two forms of
memory, the ﬁrst an episodic, short-term mental
model – the system’s current abstract state repre-
sentation ( ˜st). The symbolic programming analog
is the execution trace of a program, containing the
states of relevant working variables at each execu-
tion step. Fig. 4 displays the updated mental model,
after the removal of the apples. Compiled knowl-
edge , or long-term memory, reﬂects highly famil-
iar object representations, behaviors and schemata,
such as common sense, intuitive psychology and
EmulatorNatural
Language
EnvironmentAgent
"Remove all apples
from the second crate."C2
Sub-goal 2
Read next sentenceSem. parse                                               Global MemoryMental Model
(Short-term)
.Compiled
Knowledge
(Long-term)
Action"Library
functions"
 importsSub-goal 3
C2C1
for	box	in	crate2:	remove	apples	from	box	
Parsing: high-level perception, control
Sub-goal 1Figure 4: Architecture for comprehender ( §5), demonstrated on a symbolic version of the example task of Fig.
1. The agent receives natural language input from the environment. The agent has global memory – short-term,
keeping track of the mental model of the world, and long-term, containing compiled knowledge (“library classes
and functions”). The parser interprets input to parse ˜atenacting mental simulation using emulator. The mental
model is then updated, ready for the next input. The sub-goals refer to the order in which components are learned
(as opposed to hard-coded) in our proposed roadmap ( §7).
physics. The symbolic programming language
analogs of this are libraries; largely static, hierarchi-
cal and compositional repositories of functions and
classes. In the course of language interpretation,
these libraries are “importable”: for the symbolic
example in Fig. 4, the parser might instantiate a
new variable of an imported type (e.g., crate2
= Container() ). Both types of memory are
accessible for all components of the agent.
Parser .Abstraction of higher-level perception,
control, reasoning and linguistic functions. Han-
dles interpretation of new linguistic inputs based
on prior knowledge and the current mental state.
Consonant with the view of analogy-making as
a kind of higher-level perception or recogni-
tion (Mitchell, 1993), metaphoric inference is in-
volved in grounding a novel input utintointernal,
neural state representations ˜st,˜ataffording simu-
lation. See Fig. 4 and Fig. 2 for examples on literal
and non-literal language, respectively.
Emulator .Functionally similar to the executor
module in executable semantic parsing, but learned,
and obviously far greater in scale. This mod-
ule is an abstraction of neural emulation mecha-
nisms ( ˜T), representing a wide range of functions,
from lower-level motor control and imagery to
higher-level models used for planning and theory
of mind (Grush, 2004). It operates over the current
mental model and semantic parse from the parser.
The output is then an updated mental model.Importantly, the proposed architecture is de-
signed to address the challenges outlined in §5.2;
compiled knowledge underlies human common
ground , the building blocks of ˜s,˜aand˜T. Memory
and emulation are instrumental for accumulation
in discourse . The ability to understand abstract
language involves all modules in the system.
7 Implementation Roadmap
The architecture outlined in §6 is very ambitious;
its implementation requires much further research.
This section proposes a roadmap to this goal, identi-
fying three sub-goals (Fig. 4), presented in order of
increasing difﬁculty. Broadly speaking, the level of
difﬁculty is determined by which components are
assumed as given in the input (here this also means
they are hard-coded in a symbolic programming
language), and which must be learned .
7.1 Sub-goal 1: learning open-domain
simulation
Observing that literal language is close to the em-
bodied primitives level, its interpretation is simpler
(than that of non-literal language, see §4). There-
fore, in this phase, the emulator and compiled
knowledge are hard-coded; here the focus is learn-
ing the parser. In other words, this sub-goal focuses
on extending executable semantic parsing from rel-
atively narrow domains to handle more general
literal language on-the-ﬂy, similarly to zero-shot
semantic parsing (Givoli and Reichart, 2019).
For the example in §2.1, the parser could be
expected to infer the types (boxes as containers,
fruits as objects) either by context (Yao et al. (2018)
explore a preliminary schema-based approach) or
explicit declarative language, using them to conﬁg-
ure the emulator to handle the speciﬁc required
problem setting (Tamari et al., 2020).
As in similar projects exploring embodied under-
standing (Pustejovsky and Krishnaswamy, 2016;
Baldridge et al., 2018), new simulator frame-
works must be developed. While full embodiment
calls for multiple modalities, the degree to which
it is required remains an important open ques-
tion (Lupyan and Lewis, 2019). Accordingly, and
for immediate applicability to purely textual NLU
problems we propose also focusing on the simpler
setting of interactive text (Nelson, 2005). Recent
research on text-based games shows how agents
can learn to “program” in such languages (C ˆot´e
et al., 2019; Ammanabrolu and Riedl, 2019), and
how real language understanding problems can be
framed as executable semantic parsing using conﬁg-
urable text-based simulators (Tamari et al., 2019).
7.2 Sub-goal 2: learning to simulate
This phase assumes that the compiled knowledge is
given (hard-coded), and the parsing and emulator
modules are neural (learned). A hard-coded emula-
tor will likely be needed to train a learned emulator.
The learned event execution of Narayanan (1997)
provides a useful starting point towards computa-
tional models capable of such inference. In general,
learned simulation is relatively unexplored in the
context of natural language, though recent work has
explored it in generated instruction following se-
tups (Gaddy and Klein, 2019; Adhikari et al., 2020).
Outside of NLU, learning structured world models
is a long-studied, fast-growing ﬁeld in embodied
AI research (Schmidhuber, 1990; Ha and Schmid-
huber, 2018; Hamrick, 2019; Kipf et al., 2020), and
recently also in learned executors for neural pro-
gramming (Kant, 2018). We expect much useful
cross fertilization with these ﬁelds.
7.3 Sub-goal 3: learning compiled knowledge
This phase focuses on the component seemingly
hardest to learn – compiled knowledge. Out of
scope here is fully neural setting where all compo-
nents are jointly learned, as in continual learning
research (Parisi et al., 2019). Instead, we focus on
a simpler setting, in which the compiled knowledge
is learned but represented by symbolic code; i.e.,learning the static code library underlying the sim-
ulation framework. This sub-goal is relevant for
training the parser ( §7.1) as well as the emulator
(§7.2), and can be pursued in parallel to them.
In this setting, learning compiled knowledge
is closely related to never-ending language learn-
ing (Mitchell et al., 2018), interactive task learn-
ing (She and Chai, 2016), automated knowledge
base construction (Winn et al., 2019), and frame
induction from text (QasemiZadeh et al., 2019).
Our proposed paradigm suggests enriching classic
symbolic knowledge representations (Speer et al.,
2017) to executable form (Tamari et al., 2020). Pre-
liminary steps in this direction are seen in inferen-
tial knowledge bases such as ATOMIC (Sap et al.,
2019), which provides limited execution logic us-
ing edges typed with if-then relations.
Alongside FrameNet and MetaNet, others have
collected schema and metaphor mappings, by learn-
ing them from large corpora (Beigman Klebanov
et al., 2016; Gao et al., 2018). Pastra et al.
(2011) built a database of concepts directly ground-
able to sensorimotor representations, primarily for
robotics applications.
8 Conclusions
This position paper has proposed an approach to
representation and learning based on the tenets of
ECL. The proposed architecture, drawing on con-
temporary cognitive science, aims to address key
limitations of current NLU systems through mental
simulation and grounded metaphoric inference. We
outlined major challenges and suggested a roadmap
towards realizing the proposed vision.
Growing empirical evidence shows that language
is intricately intertwined with a vast range of other
neural processes. Accordingly, this work suggests
a symbiotic view of cognitive science, embodied
AI, and computational linguistics. By sharing com-
mon foundational problems, these ﬁelds may better
share and co-evolve common solutions. Finally,
we believe that attaining deeper language under-
standing must be a large scale effort, beyond the
scope of any one research group. We hope that
the paradigm presented here will help provide co-
herence to such efforts. One of our main goals
was to stimulate a discussion; moving forward, we
welcome comments, feedback, and suggestions.
Acknowledgments
We thank the reviewers for their insightful com-
ments. We further thank Ari Rappoport, the Hya-
data Lab at HUJI, Yoav Goldberg, Ido Dagan,
Jonathan Berant, and the BIU & TAU NLP seminar
audiences for interesting discussion and thoughtful
remarks. This work was supported by the Euro-
pean Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation pro-
gramme (grant no. 852686, SIAM) and NSF-BSF
grant no. 2017741 (Shahaf), as well as the Israel
Science Foundation grant no. 929/17 (Abend).
References
Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre
Cˆot´e, Mikul ´aˇs Zelinka, Marc-Antoine Rondeau, Ro-
main Laroche, Pascal Poupart, Jian Tang, Adam
Trischler, and William L. Hamilton. 2020. Learn-
ing dynamic knowledge graphs to generalize on
text-based games. Computing Research Repository ,
arXiv:2002.09127.
Prithviraj Ammanabrolu and Mark Riedl. 2019. Play-
ing text-adventure games with graph-based deep re-
inforcement learning. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short
Papers) , pages 3557–3565, Minneapolis, Minnesota.
Association for Computational Linguistics.
Jacob Andreas, Anca Dragan, and Dan Klein. 2017.
Translating neuralese. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers) , pages
232–242, Vancouver, Canada. Association for Com-
putational Linguistics.
Michael A. Arbib, Brad Gasser, and Victor Barr `es.
2014. Language is handy but is it embodied? Neu-
ropsychologia , 55(1):57–70.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics , 1:49–62.
Jason Baldridge, Tania Bedrax-Weiss, Daphne Luong,
Srini Narayanan, Bo Pang, Fernando Pereira, Radu
Soricut, Michael Tseng, and Yuan Zhang. 2018.
Points, paths, and playscapes: Large-scale spatial
language understanding tasks set in the real world.
InProceedings of the First International Workshop
on Spatial Language Understanding , pages 46–52,
New Orleans, Louisiana, USA.
Lawrence W. Barsalou. 2008. Grounded Cognition.
Annual Review of Psychology , 59(1):617–645.Beata Beigman Klebanov, Chee Wee Leong, E. Dario
Gutierrez, Ekaterina Shutova, and Michael Flor.
2016. Semantic classiﬁcations for detection of verb
metaphors. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers) , pages 101–106, Berlin,
Germany. Association for Computational Linguis-
tics.
Yoshua Bengio. 2017. The Consciousness Prior. Com-
puting Research Repository , arXiv:1709.08568.
Benjamin K Bergen and Nancy Chang. 2005. Embod-
ied construction grammar in simulation-based lan-
guage understanding. Construction grammars: Cog-
nitive grounding and theoretical extensions , 3:147.
Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob
Andreas, Yoshua Bengio, Joyce Chai, Mirella Lap-
ata, Angeliki Lazaridou, Jonathan May, Aleksandr
Nisnevich, Nicolas Pinto, and Joseph Turian. 2020.
Experience grounds language. Computing Research
Repository , arXiv:2004.10151.
Antoine Bordes, Jason Weston, Sumit Chopra, Tomas
Mikolov, Arman Joulin, S Rush, and L Bottou. 2015.
Artiﬁcial tasks for artiﬁcial intelligence. Facebook
AI Research. ICLR–San Diego–May , 7:2015.
Anna M. Borghi, Laura Barca, Ferdinand Binkofski,
Cristiano Castelfranchi, Giovanni Pezzulo, and Luca
Tummolini. 2018. Words as social tools: Language,
sociality and inner grounding in abstract concepts.
Physics of Life Reviews , 29:120–153.
Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-
tanya Malaviya, Asli Celikyilmaz, and Yejin Choi.
2019. COMET: Commonsense transformers for au-
tomatic knowledge graph construction. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics , pages 4762–4779,
Florence, Italy. Association for Computational Lin-
guistics.
Nancy Chang, Srini Narayanan, and Miriam R.L.
Petruck. 2002. Putting frames in perspective. In
COLING 2002: The 19th International Conference
on Computational Linguistics .
Maxime Chevalier-Boisvert, Dzmitry Bahdanau,
Salem Lahlou, Lucas Willems, Chitwan Saharia,
Thien Huu Nguyen, and Yoshua Bengio. 2019.
BabyAI: First steps towards grounded language
learning with a human in the loop. In International
Conference on Learning Representations .
Herbert H Clark and Edward F Schaefer. 1989. Con-
tributing to discourse. Cognitive science , 13(2):259–
294.
Marc-Alexandre C ˆot´e,´Akos K ´ad´ar, Xingdi Yuan, Ben
Kybartas, Tavian Barnes, Emery Fine, James Moore,
Matthew Hausknecht, Layla El Asri, Mahmoud
Adada, and et al. 2019. Textworld: A learning en-
vironment for text-based games. Computer Games ,
page 41–75.
Oana David and Ellen Dodge. 2014. Building the
metanet metaphor repository: The natural symbiosis
of metaphor analysis and construction grammar. In
The 8th International Construction Grammar Con-
ference (ICCG 8), 3-6 September, Osnarbr ¨uck, Ger-
many .
Oana David, George Lakoff, and Elise Stickles. 2016.
Cascades in metaphor and grammar. Constructions
and Frames , 8(2):214–255.
Samuel B. Day and Dedre Gentner. 2007. Noninten-
tional analogical inference in text comprehension.
Memory & Cognition , 35(1):39–49.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel
Stanovsky, Sameer Singh, and Matt Gardner. 2019.
DROP: A reading comprehension benchmark requir-
ing discrete reasoning over paragraphs. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers) , pages 2368–2378, Min-
neapolis, Minnesota. Association for Computational
Linguistics.
Jeffrey L Elman. 2004. An alternative view of the men-
tal lexicon. Trends in cognitive sciences , 8(7):301–
306.
Jerome Feldman and Srinivas Narayanan. 2004. Em-
bodied meaning in a neural theory of language.
Brain and Language , 89(2):385–392.
Charles J Fillmore. 1985. Frames and the semantics
of understanding. Quaderni di semantica , 6(2):222–
254.
Maxwell Forbes, Ari Holtzman, and Yejin Choi. 2019.
Do neural language representations learn physical
commonsense? Proceedings of the 41st Annual
Conference of the Cognitive Science Society .
Vincent Franc ¸ois-Lavet, Yoshua Bengio, Doina Precup,
and Joelle Pineau. 2019. Combined reinforcement
learning via abstract representations. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelli-
gence , volume 33, pages 3582–3589.
David Gaddy and Dan Klein. 2019. Pre-learning envi-
ronment representations for data-efﬁcient neural in-
struction following. In Proceedings of the 57th An-
nual Meeting of the Association for Computational
Linguistics , pages 1946–1956, Florence, Italy. Asso-
ciation for Computational Linguistics.Vittorio Gallese and George Lakoff. 2005. The brain’s
concepts: The role of the sensory-motor system in
conceptual knowledge. Cognitive Neuropsychology ,
22(3-4):455–479.
Ge Gao, Eunsol Choi, Yejin Choi, and Luke Zettle-
moyer. 2018. Neural metaphor detection in context.
InProceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing , pages
607–613.
P. G¨ardenfors. 2014. The Geometry of Meaning: Se-
mantics Based on Conceptual Spaces . The MIT
Press. MIT Press.
Matt Gardner. 2019. How will we know when machines
can read?
Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan
Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi,
Dheeru Dua, Yanai Elazar, Ananth Gottumukkala,
Nitish Gupta, Hanna Hajishirzi, Gabriel Ilharco,
Daniel Khashabi, Kevin Lin, Jiangming Liu, Nel-
son F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer
Singh, Noah A. Smith, Sanjay Subramanian, Reut
Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou.
2020. Evaluating nlp models via contrast sets. Com-
puting Research Repository , arXiv:2004.02709.
Matt Gardner, Jonathan Berant, Hannaneh Hajishirzi,
Alon Talmor, and Sewon Min. 2019a. On Making
Reading Comprehension More Comprehensive.
Matt Gardner, Jonathan Berant, Hannaneh Hajishirzi,
Alon Talmor, and Sewon Min. 2019b. Question an-
swering is a format; when is it useful? Computing
Research Repository , arXiv:1909.11291.
Jon Gauthier and Igor Mordatch. 2016. A paradigm for
situated and goal-driven language learning. Comput-
ing Research Repository , arXiv:1610.03585.
Dedre Gentner and Kenneth D Forbus. 2011. Compu-
tational models of analogy. Wiley interdisciplinary
reviews: cognitive science , 2(3):266–276.
Mor Geva and Jonathan Berant. 2018. Learning to
search in long documents using document structure.
InProceedings of the 27th International Confer-
ence on Computational Linguistics , pages 161–176,
Santa Fe, New Mexico, USA. Association for Com-
putational Linguistics.
Ofer Givoli and Roi Reichart. 2019. Zero-Shot Seman-
tic Parsing for Instructions. In Proceedings of the
57th Annual Meeting of the Association for Compu-
tational Linguistics , pages 4454–4464, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Arthur M. Glenberg. 2008. Toward the Integra-
tion of Bodily States, Language, and Action. In
Gun R. Semin and Eliot R. Smith, editors, Embod-
ied Grounding , pages 43–70. Cambridge University
Press, Cambridge.
Max Glockner, Vered Shwartz, and Yoav Goldberg.
2018. Breaking NLI systems with sentences that re-
quire simple lexical inferences. In Proceedings of
the 56th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers) ,
pages 650–655, Melbourne, Australia. Association
for Computational Linguistics.
H Paul Grice et al. 1975. Logic and conversation.
Rick Grush. 2004. The emulation theory of represen-
tation: Motor control, imagery, and perception. Be-
havioral and Brain Sciences , 27(3):377–396.
David Ha and J ¨urgen Schmidhuber. 2018. Recur-
rent world models facilitate policy evolution. In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances
in Neural Information Processing Systems 31 , pages
2450–2462. Curran Associates, Inc.
Jessica B Hamrick. 2019. Analogues of mental sim-
ulation and imagination in deep learning. Current
Opinion in Behavioral Sciences .
Stevan Harnad. 1990. The symbol grounding problem.
Physica D: Nonlinear Phenomena , 42(1):335 – 346.
Leslie Pack Kaelbling, Michael L Littman, and An-
thony R Cassandra. 1998. Planning and acting in
partially observable stochastic domains. Artiﬁcial
intelligence , 101(1-2):99–134.
Neel Kant. 2018. Recent advances in neural pro-
gram synthesis. Computing Research Repository ,
arXiv:1802.02353.
Daniel Keysers, Nathanael Sch ¨arli, Nathan Scales,
Hylke Buisman, Daniel Furrer, Sergii Kashubin,
Nikola Momchev, Danila Sinopalnikov, Lukasz
Staﬁniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang,
Marc van Zee, and Olivier Bousquet. 2020. Measur-
ing Compositional Generalization: A Comprehen-
sive Method on Realistic Data. In International Con-
ference on Learning Representations (ICLR) , pages
1–38.
Douwe Kiela, Luana Bulat, Anita L Vero, and Stephen
Clark. 2016. Virtual embodiment: A scalable
long-term strategy for artiﬁcial intelligence research.
Computing Research Repository , arXiv:1610.07432.
Walter Kintsch and Teun A Van Dijk. 1978. Toward a
model of text comprehension and production. Psy-
chological review , 85(5):363.
Thomas Kipf, Elise van der Pol, and Max Welling.
2020. Contrastive learning of structured world mod-
els. In International Conference on Learning Repre-
sentations .
Brenden Lake, Tal Linzen, and Marco Baroni. 2019.
Human few-shot learning of compositional instruc-
tions. In Ashok Goel, Colleen Seifert, and Christian
Freksa, editors, Proceedings of the 41st Annual Con-
ference of the Cognitive Science Society , pages 611–
616. Cognitive Science Society, Montreal, Canada.Brenden M. Lake, Tomer D. Ullman, Joshua B.
Tenenbaum, and Samuel J. Gershman. 2017a.
Building machines that learn and think like
people. Computing Research Repository ,
arXiv:arXiv:1604.00289v3.
Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenen-
baum, and Samuel J. Gershman. 2017b. Ingredi-
ents of intelligence: From classic debates to an engi-
neering roadmap. Computing Research Repository ,
arXiv:1604.00289.
George Lakoff and Mark Johnson. 1980. The
metaphorical structure of the human conceptual sys-
tem. Cognitive science , 4(2):195–208.
George Lakoff and Srini Narayanan. 2010. Toward a
computational model of narrative. In 2010 AAAI
Fall Symposium Series .
Ron Langacker. 1987. 1991 foundations of cognitive
grammar, 2 volumes.
Neil D. Lawrence. 2017. Living together: Mind and
machine intelligence. Computing Research Reposi-
tory, arXiv:1705.07996.
David Lewis. 1976. General semantics. In Montague
grammar , pages 1–50. Elsevier.
Chen Liang, Jonathan Berant, Quoc Le, Kenneth D.
Forbus, and Ni Lao. 2016. Neural Symbolic Ma-
chines: Learning Semantic Parsers on Freebase with
Weak Supervision (Short Version). Computing Re-
search Repository , arXiv:1612.01197.
Percy Liang. 2016. Learning executable semantic
parsers for natural language understanding. Commu-
nications of the ACM , 59(9):68–76.
Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang,
Qi Ju, Haotang Deng, and Ping Wang. 2019a. K-
bert: Enabling language representation with knowl-
edge graph. Computing Research Repository ,
arXiv:1909.07606.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
2019b. Roberta: A robustly optimized bert pre-
training approach. Computing Research Repository ,
arXiv:1907.11692.
Reginald Long, Panupong Pasupat, and Percy Liang.
2016. Simpler context-dependent logical forms via
model projections. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 1456–
1465, Berlin, Germany. Association for Computa-
tional Linguistics.
Jelena Luketina, Nantas Nardelli, Gregory Farquhar,
Jakob Foerster, Jacob Andreas, Edward Grefenstette,
Shimon Whiteson, and Tim Rockt ¨aschel. 2019. A
Survey of Reinforcement Learning Informed by Nat-
ural Language. Computing Research Repository ,
arXiv:1906.03926.
Gary Lupyan and Benjamin Bergen. 2016. How Lan-
guage Programs the Mind. Topics in Cognitive Sci-
ence, 8(2):408–424.
Gary Lupyan and Molly Lewis. 2019. From words-as-
mappings to words-as-cues: the role of language in
semantic knowledge. Language, Cognition and Neu-
roscience , 34(10):1319–1337.
James L. McClelland, Felix Hill, Maja Rudolph, Ja-
son Baldridge, and Hinrich Sch ¨utze. 2019. Ex-
tending Machine Language Models toward Human-
Level Language Understanding. Computing Re-
search Repository , arXiv:1912.05877.
R. Thomas McCoy, Junghyun Min, and Tal Linzen.
2019a. Berts of a feather do not generalize to-
gether: Large variability in generalization across
models with similar test set performance. Comput-
ing Research Repository , arXiv:1911.02969.
Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019b.
Right for the wrong reasons: Diagnosing syntactic
heuristics in natural language inference. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics , pages 3428–3448,
Florence, Italy. Association for Computational Lin-
guistics.
Melanie Mitchell. 1993. Analogy-Making as Percep-
tion: A Computer Model . MIT Press, Cambridge,
MA, USA.
Tom Mitchell, William Cohen, Estevam Hruschka,
Partha Talukdar, Bishan Yang, Justin Betteridge, An-
drew Carlson, Bhavana Dalvi, Matt Gardner, Bryan
Kisiel, et al. 2018. Never-ending learning. Commu-
nications of the ACM , 61(5):103–115.
Raymond J. Mooney. 2008. Learning to connect lan-
guage and perception. In Proceedings of the 23rd
National Conference on Artiﬁcial Intelligence - Vol-
ume 3 , AAAI’08, page 1598–1601. AAAI Press.
Pramod Kaushik Mudrakarta, Ankur Taly, Mukund
Sundararajan, and Kedar Dhamdhere. 2018. Did
the model understand the question? In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers) , pages 1896–1906, Melbourne, Australia. As-
sociation for Computational Linguistics.
Srinivas Narayanan. 1997. Knowledge-based Action
Representations for Metaphor and Aspect (KARMA) .
PhD dissertation, The University of California.
Graham Nelson. 2005. Natural Language, Semantic
Analysis and Interactive Fiction. IF Theory Reader ,
(April 2005):141–188.
Yixin Nie, Adina Williams, Emily Dinan, Mohit
Bansal, Jason Weston, and Douwe Kiela. 2019. Ad-
versarial NLI: A New Benchmark for Natural Lan-
guage Understanding. Computing Research Reposi-
tory, arXiv:1910.14599.German I Parisi, Ronald Kemker, Jose L Part, Christo-
pher Kanan, and Stefan Wermter. 2019. Continual
lifelong learning with neural networks: A review.
Neural Networks .
Katerina Pastra, Eirini Balta, Panagiotis Dimitrakis,
and Giorgos Karakatsiotis. 2011. Embodied lan-
guage processing: a new generation of language
technology. In Workshops at the Twenty-Fifth AAAI
Conference on Artiﬁcial Intelligence .
Dennis R Profﬁtt. 2006. Embodied perception and the
economy of action. Perspectives on psychological
science , 1(2):110–122.
James Pustejovsky and Nikhil Krishnaswamy. 2016.
V oxml: A visualization modeling language. In Pro-
ceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC 2016) ,
pages 4606–4613.
Behrang QasemiZadeh, Miriam R. L. Petruck, Regina
Stodden, Laura Kallmeyer, and Marie Candito. 2019.
SemEval-2019 task 2: Unsupervised lexical frame
induction. In Proceedings of the 13th Interna-
tional Workshop on Semantic Evaluation , pages 16–
30, Minneapolis, Minnesota, USA. Association for
Computational Linguistics.
David E Rumelhart. 1981. Understanding understand-
ing. Memories, thoughts and emotions: Essays in
honor of George Mandler , 257:275.
D. E. Rumlehart, P. Smolensky, J. L. McClelland,
and G. E. Hinton. 1986. Schemata and Sequential
Thought Processes in PDP Models , page 7–57. MIT
Press, Cambridge, MA, USA.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, Collin F. Baker,
and Jan Scheffczyk. 2016. FrameNet II: Extended
Theory and Practice . ICSI: Berkeley.
Maarten Sap, Ronan Le Bras, Emily Allaway, Chan-
dra Bhagavatula, Nicholas Lourie, Hannah Rashkin,
Brendan Roof, Noah A Smith, and Yejin Choi. 2019.
Atomic: An atlas of machine commonsense for if-
then reasoning. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence , volume 33, pages
3027–3035.
J¨urgen Schmidhuber. 1990. Making the world differen-
tiable: On using self-supervised fully recurrent neu-
ral networks for dynamic reinforcement learning and
planning in non-stationary environments.
Elad Segal, Avia Efrat, Mor Shoham, Amir Globerson,
and Jonathan Berant. 2019. A simple and effective
model for answering multi-span questions. Comput-
ing Research Repository , arXiv:1909.13375.
Lanbo She and Joyce Chai. 2016. Incremental acquisi-
tion of verb hypothesis space towards physical world
interaction. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 108–117, Berlin,
Germany. Association for Computational Linguis-
tics.
Robert Speer, Joshua Chin, and Catherine Havasi. 2017.
Conceptnet 5.5: An open multilingual graph of gen-
eral knowledge. In Thirty-First AAAI Conference on
Artiﬁcial Intelligence .
Luc Steels and Joachim de Beule. 2006. A (very) brief
introduction to ﬂuid construction grammar. In Pro-
ceedings of the Third Workshop on Scalable Natural
Language Understanding , pages 73–80, New York
City, New York. Association for Computational Lin-
guistics.
Arjen Stolk, Lennart Verhagen, and Ivan Toni. 2016.
Conceptual Alignment: How Brains Achieve Mu-
tual Understanding. Trends in Cognitive Sciences ,
20(3):180–191.
Leonard Talmy. 1983. How language structures space.
InSpatial orientation , pages 225–282. Springer.
Leonard Talmy. 1985. Lexicalization patterns: Seman-
tic structure in lexical forms. Language typology
and syntactic description , 3(99):36–149.
Ronen Tamari, Hiroyuki Shindo, Dafna Shahaf, and
Yuji Matsumoto. 2019. Playing by the book: An in-
teractive game approach for action graph extraction
from text. In Proceedings of the Workshop on Ex-
tracting Structured Knowledge from Scientiﬁc Publi-
cations , pages 62–71, Minneapolis, Minnesota. As-
sociation for Computational Linguistics.
Ronen Tamari, Gabriel Stanovsky, Dafna Shahaf,
and Reut Tsarfaty. 2020. Ecological semantics:
Programming environments for situated language
understanding. Computing Research Repository ,
arXiv:2003.04567.
Michael Tomasello. 2008. Human Cooperative Com-
munication .
Francisco J Varela, Evan Thompson, and Eleanor
Rosch. 2017. The embodied mind: Cognitive sci-
ence and human experience . MIT press.
John Winn, John Guiver, Sam Webster, Yordan Zaykov,
Martin Kukla, and Dany Fabian. 2019. Alexandria:
Unsupervised high-precision knowledge base con-
struction using a probabilistic program. In Auto-
mated Knowledge Base Construction (AKBC) . Best
Research Paper Award.
Yiqun Yao, Jiaming Xu, Jing Shi, and Bo Xu. 2018.
Learning to activate logic rules for textual reasoning.
Neural Networks .
Dani Yogatama, Cyprien de Masson d’Autume, Jerome
Connor, Tomas Kocisky, Mike Chrzanowski, Ling-
peng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu,
Chris Dyer, et al. 2019. Learning and evaluating
general linguistic intelligence. Computing Research
Repository , arXiv:1901.11373.Xingdi Yuan, Jie Fu, Marc-Alexandre Cote, Yi Tay,
Christopher Pal, and Adam Trischler. 2019. In-
teractive machine comprehension with information
seeking agents. Computing Research Repository ,
arXiv:1908.10449.
Chapter 3
Playing by the Book: An
Interactive Game Approach for
Action Graph Extraction from
Text
Ronen Tamari, Hiroyuki Shindo, Dafna Shahaf, Yuji Mat-
sumoto
Published in the Proceedings of the Workshop on Extracting Struc-
tured Knowledge from Scientific Publications (ESSP), 2019
29
Playing by the Book: An Interactive Game Approach for Action Graph
Extraction from Text
Ronen Tamari∗
The Hebrew University of Jerusalem
ronent@cs.huji.ac.ilHiroyuki Shindo
NAIST / RIKEN-AIP
shindo@is.naist.jp
Dafna Shahaf
The Hebrew University of Jerusalem
dshahaf@cs.huji.ac.ilYuji Matsumoto
NAIST / RIKEN-AIP
matsu@is.naist.jp
Abstract
Understanding procedural text requires track-
ing entities, actions and effects as the narra-
tive unfolds. We focus on the challenging real-
world problem of action-graph extraction from
materials science papers, where language is
highly specialized and data annotation is ex-
pensive and scarce. We propose a novel ap-
proach, T EXT2QUEST , where procedural text
is interpreted as instructions for an interactive
game . A learning agent completes the game
by executing the procedure correctly in a text-
based simulated lab environment. The frame-
work can complement existing approaches and
enables richer forms of learning compared to
static texts. We discuss potential limitations
and advantages of the approach, and release a
prototype proof-of-concept, hoping to encour-
age research in this direction.
1 Introduction
Materials science literature includes a vast amount
of synthesis procedures described in natural lan-
guage. The ability to automatically parse these
texts into a structured form could allow for data-
driven synthesis planning, a key enabler in the de-
sign and discovery of novel materials (Kim et al.,
2018; Mysore et al., 2017). A particularly useful
parsing is action graph extraction , which maps
a passage describing a procedure to a symbolic
action-graph representation of the core entities, op-
erations and their accompanying arguments, as they
unfold throughout the text (Fig. 1).
Procedural text understanding is a highly chal-
lenging task for today’s learning algorithms (Lucy
and Gauthier, 2017; Levy et al., 2017). Synthesis
procedures are especially challenging, as they are
written in difﬁcult and highly technical language
assuming prior knowledge. Some texts are long,
∗Work was begun while author was an intern at RIKEN
and continued at the Hebrew University.many follow a non-linear narrative, or include logi-
cal quantiﬁers (“all synthesis steps were performed
in an argon atmosphere...”). Furthermore, anno-
tated data is scarce and expensive to obtain.
Two related research areas are grounded
semantic parsing andstate-tracking reading-
comprehension . Grounded (or executable) seman-
tic parsers map natural language to a symbolic
representation which can also be thought of as a
sequence of instructions in some pre-deﬁned pro-
gramming language. Such “neural-programing” ar-
chitectures offer strong symbolic reasoning capabil-
ities, compositionality modelling, and strong gen-
eralization (Reed and de Freitas), but are typically
applied to simple texts due to prohibitive annotation
costs (Liang et al., 2017). State-tracking models
(Bosselut et al., 2017; Das et al., 2018; Bansal et al.,
2017) can model complex relations between enti-
ties as they unfold, with easier training but less
symbolic reasoning abilities. Their applicability
to longer texts is hindered as well by the lack of
ﬁne-grained annotated data.
In this work we describe an approach,
TEXT2QUEST , that attempts to combine the
strengths of both methods. Instead of trying to
learn from static text, we propose to treat proce-
dural text as instructions for an interactive game
(or “quest”). The learning agent interacts with en-
tities deﬁned in the text by executing symbolic
actions (Fig. 2). A text-based symbolic interpreter
handles execution and tracking of the agent’s state
and actions. The game is completed by “simulating”
the instructions correctly; i.e., mapping instructions
to a sequence of actions. Correct simulation thus
directly yields the desired action graph.
While there is some engineering overhead re-
quired for the simulator, we demonstrate that it
is relatively straightforward to convert an annota-
tion schema to a text-based game. We believe that
the beneﬁts make it worth pursuing: the game for-
Figure 1: Sample surface text (left) and possible corresponding action-graph (right) for typical partial material
synthesis procedure. Operation numbers in parentheses are added for clarity. Nodes are entities, edges are relations
linking them, equivalent to actions in the text-based game.
mat allows applying powerful neural programming
methods, with a signiﬁcantly richer training envi-
ronment, including advances such as curriculum
learning, common-sense and domain-speciﬁc con-
straints, and full state tracking. Such “friendly”
environments that assist the learning agent have
been shown to be valuable (Liang et al., 2017) and
enable learning of patterns that are often hard to
learn from surface annotations alone, such as im-
plicit effects of operations (i.e., ﬁltering a mixture
splits it into two entities).
Interestingly, understanding by simulation aligns
well with models of human cognition; mental sim-
ulation, the ability to construct and manipulate an
internal world model, is a cornerstone of human
intelligence involved in many unique behaviors,
including language comprehension (Marblestone
et al., 2016; Hamrick, 2019). In this work we take
ﬁrst steps towards this idea. Our contributions are:
•We propose a novel formulation of the prob-
lem of procedural text understanding as a text-
based game, enabling the use of neural pro-
gramming and text-based reinforcement learn-
ing (RL) methods.
•We present and release TEXTLABS1, an in-
stance of TEXT2QUEST designed for interac-
tion with synthesis procedure texts. We fo-
cus on the material-science setting, but the
approach is intended to be more generally ap-
plicable.
•We propose to address the problem of obtain-
ing full-graph annotations at scale by cou-
pling the simulator with controllable natural
language generation (NLG) to generate syn-
thetic data, also enabling curriculum learning.
1Code and experiments available at https://github.
com/ronentk/TextLabs
Figure 2: Excerpt from an actual “material synthe-
sis quest” generated by our system with example in-
put/outputs.
While this work is preliminary in nature, neural
programming and text-based reinforcement learn-
ing approaches are attracting signiﬁcant and grow-
ing interest, and we expect advances in these areas
to directly beneﬁt future versions of the system.
2 Related Work
Procedure understanding : Many recent works
have focused on tracking entities and relations in
long texts, such as cooking recipes and scientiﬁc
processes (Bosselut et al., 2017; Das et al., 2018).
However, these methods do not directly extract a
full action graph. For action graph extraction, ear-
lier works use sequence tagging methods (Mysore
et al., 2017). Feng et al. (2018) have applied deep-
RL to the problem of extracting action sequences,
but assume explicit procedural instruction texts. In
Johnson (2017), a graph is constructed from simple
generated stories, using state tracking at each time
step as supervision.
Semantic parsing & Neural Programming :
Research to-date has focused mainly on shorter
and simpler texts which may require complex sym-
bolic reasoning, such as mapping natural language
to queries over knowledge graphs (Liang et al.,
2017). In the case of narrative parsing, the text
itself may be complex while the programs are rel-
atively simple (creating and linking between enti-
ties present in the text). Recent work (Lu et al.,
2018) frames narrative understanding as neural-
programming, the learner converts a document into
a structured form, using a predeﬁned set of data-
structures. This approach is similar to ours, though
with simpler texts and without a simulated environ-
ment. In our approach, the learning architecture is
decoupled from the symbolic interpreter environ-
ment, enabling greater architectural ﬂexibility.
Text-RL : Text-based games are used to study
language grounding and understanding and RL for
combinatorical action spaces (Zahavy et al., 2018;
Narasimhan, 2017) but have not yet been applied
to real world problems. TextWorld (C ˆot´e et al.,
2018) is a recently released reinforcement learning
sandbox environment for creation of custom text-
based games, upon which we base T EXTLABS.
3 Problem Formulation
Entities, Relations & Rules ( E,R,Λ): Assume
two vocabularies deﬁning types of entitiesE=
{e1,...,e N}andrelationsR={r1,...,r K}. A
factfis a grounded predicate of the form f=
r(h,t), h,t∈E, r∈R (single or double argu-
ment predicate relations are allowed). We deﬁne
the set of valid world-states S, where a state s∈S
is a set of facts, and validity is decided by a world-
model Λdeﬁned using linear logic. Λis comprised
of production rules (or transition rules) over enti-
ties and relations governing which new facts can
be produced from a given state. Following the
schema used in the Synthesis Project2(see for ex-
ample MSP), entity types include materials, opera-
tions, and relevant descriptors (like operation condi-
tions, etc.). Relations link between entities (like in-
put(material,operation) or denote single predicate
relations (entity properties such as solid(material) ).
We currently use a simpliﬁed version of the schema
to ease the learning problem. See appendix A.1
for a mapping of relations and entities. Pro-
duction rules correspond to the actions available
to the learner, in our domain these include for
example link-descriptor(descriptor,entity) ,input-
assign(material, operation) . While not currently
2https://www.synthesisproject.org/included, actions such as co-reference linking and
generation of entities can also be incorporated.
Action-Graph ( K): An action sequence is de-
ﬁned to be a sequence of valid actions (or produc-
tion rules) rooted at some initial state s0:K=
(s0,λ0,λ1,...,λ n)(applyingλitosiresults in
si+1, intermediate states are left out for brevity).
Note that actions may apply to implicit entities
not present in the surface text (for example, the
result of an operation). Construction of an action
graph corresponding to Kis straightforward (en-
tities as nodes, actions connecting them as edges),
and henceforth we use Kto denote either the se-
quence or the graph. Note that there can be multiple
possible action sequences resulting in the same ac-
tion graph, equivalent w.r.t the topological ordering
of operations induced by their dependencies.
Surface (X): Asurface is simply a text in natu-
ral language describing a process.
Learning Task : Our objective is to learn a map-
pingΨ :X→K. As this mapping may be highly
complex, we convert the problem to a structured
prediction setting. As an intermediate step we map
an inputXto an enriched text-based-game G
representation (details below), where the solution
ofGis the required action graph K. The game is
modelled as a partially observable Markov Deci-
sion Process (POMDP) G= (S,A,T, Ω,O,R,γ ).
We refer the reader to C ˆot´e et al. (2018) for a
detailed exposition, and focus here on mapping
the game-setting to our approach: Sare states,
Aare actions, Tare conditional state transition
probabilities, where all are constant per domain
and deﬁned byE,R,Λ.Ωare observations, and
Oare conditional observations probabilities. R:
S×A→Ris the reward function, γ∈[0,1]is the
discount factor. As γ,Ω,Oare also preset (with
actual observations dependent on agent actions),
mapping a surface Xto gameGboils down to
providing a list of entities for initializing s0. For
training and evaluation, a reward function must also
be provided (not necessary for applying a trained
model on un-annotated text “in the wild”).
If a fully annotated action graph is available
(whether synthetic or real), this mapping is simple:
the initial game state s0is a room where the agent
is placed alongside all entities. Each edge corre-
sponds to an action in the game. Given an action
sequenceK, a reward function Rcan be automat-
ically computed, giving intermediate rewards and
penalizing wrong actions. A quest in TextWorld
can be deﬁned via a ﬁnal goal state, thus allowing
Figure 3: Proposed solution architecture of
TEXT2QUEST . (i) Flow for training agent on games
from real annotated data. (ii) Flow for training agent
on synthetic games. (iii) Extracting action graph from
un-annotated real data.
multiple possible winning action sequences. See
appendices A.2, A.3 for examples.
For data “in the wild”, entities can be identi-
ﬁed using named entity recognition (NER) as pre-
processing. Future directions include end-to-end
learning to reduce cascading initialization errors.
By default, the TextWorld environment is par-
tially observable. The agent observes the surface
Xat timet= 0 and other textual descriptions
upon executing an “examine” action. Unlike clas-
sic text-based games where partial observability is
part of the challenge, in our case we can adopt the
“friendly-environment” perspective and assist the
learner with information such as state-tracking or
action pruning (Liang et al., 2017; Johnson, 2017).
4 Proposed Solution Architecture
Our system consists of 6 core modules (Fig. 3): a
Knowledge Base deﬁnes entity, relation and action
vocabularies. This is used by the Surface Genera-
tor and Quest Generator modules to generate pairs
(˜X,˜K) of synthetic surfaces and their correspond-
ing action graphs for training. For un-annotated
text, a pre-trained domain speciﬁc NER tagger3
is used to extract an initial game state s0by iden-
tifying the mentioned entities. A learning agent
extractsKfrom a generated game.
TheTEXT2QUEST architecture supports three
central modes of operation: (i) Enrich existing real
world annotated pairs ( X,K ) by converting them
3For the materials synthesis domain we use the tagger
available at https://github.com/olivettigroup/
materials-synthesis-generative-modelsto game instances for training the game-solving
agent. (ii) Produce synthetic training pairs ( ˜X,˜K).
(iii) Convert un-annotated texts to game instances
for action graph extraction “in the wild”.
The current version of TEXTLABS supports
mode (ii). We implemented simple prototypes of
the domain-speciﬁc Knowledge Base, plus Quest
and Surface Generators. See Sec. A.1 for details
about converting the entity and relation annotation
schema into TextWorld. TextWorld is easily ex-
tensible and can support a variety of interaction
semantics. Aside from adding a domain speciﬁc
entity type-tree and actions, most of the underlying
logic engine and interface is handled automatically.
For the game environment, we use Inform7, a pro-
gramming language and interpreter for text-based
games. For quest generation, we currently use sim-
ple forward chaining and heuristic search strategies
to create plausible quests (for example, all start
materials must be incorporated into the synthesis
route). Combining these with a simple rule-based
Surface Generator already allows for creating sim-
ple synthetic training game instances (Fig. 2).
5 Preliminary Evaluation
As a very preliminary sanity check for the TEXT-
LABS environment, we train a simple text-based
RL agent on synthetic games in increasingly dif-
ﬁcult environments. Difﬁculty is measured by
maximum quest length, and the number of en-
tities in the target action graph. See Sec. A.2
for representative examples. We use the basic
LSTM-DQN agent of Narasimhan (2017) adapted
to the TEXTLABS setting. The action space is
A={Wv×Wo1×Wo2}, whereWvconsists of
8 action-verbs corresponding to the entity relations
tracked and additional native TextWorld actions
liketake (see Sec. A.1 for details). Wo1,Wo2are
(identical) sets of potential arguments correspond-
ing to the active entities which can be interacted
with in the game (single and double argument ac-
tions allowed). As this basic agent is not condi-
tioned on previous actions, we further concatenate
the last four commands taken to the current obser-
vation. For the same reason, we also append the
full quest instructions at every timestep’s observa-
tion. All illegal actions are pruned at each state to
reduce search space size.
We train the agent on 100 games per level and
test on 10 games. Evaluation is measured by avg.
normalized reward per game:1
|K|/summationtextT
t=1ri, where
Kis the true action sequence, Tis the episode
Figure 4: Preliminary evaluation results for a basic
LSTM-DQN text-RL agent on synthetic quests. Dot-
ted line shows average generated quest lengths.
length (set to 50) and ri= 1for each action in K
and−1otherwise (and 0 for neutral actions like
examine ). A normalized score of 1 means the agent
performed the required actions exactly.
As can be seen in Fig. 4, the agent learns to suc-
cessfully perform the required actions only for the
easiest levels. Examining longer games the agent
did not complete, we note that the lack of condi-
tioning on previous states is a serious limitation.
Equipping agents with better sequence encoding
(e.g., attention), recurrent memory, and utilizing
state information is expected to signiﬁcantly im-
prove performance. Furthermore, due to technical
limitations of the current implementation, some
actions cannot be reversed. This adds to the difﬁ-
culty of the task, and will be addressed in future
versions. Finally, learning good initial policies for
semantic parsers is known to be a hard problem
with RL alone, and related approaches commonly
use hybrid RL/supervised training methods (Liang
et al., 2017; Jiang et al., 2012).
6 Discussion
Our approach faces tough challenges. However, we
are encouraged by the signiﬁcant recent advances
towards these challenges in related areas, and plan
to leverage this progress for our framework.
Programming semantics and rewards for
instruction-following agents is known to be no-
toriously difﬁcult (Winograd, 1972) as language
and environments grow increasingly complex. Re-
search on learned instruction-conditional re-
ward models (Bahdanau et al., 2018) is a promis-
ing approach towards reducing the amount of “en-
vironment engineering” required.Another critical open question in our framework
is whether the surface generator will be able to
generate surfaces representative enough to allow
for generalization to real examples. Current NLG
systems are increasingly capable of structured text
generation (Marcheggiani and Perez-Beltrachini,
2018), and though they produce relatively short
surfaces, we believe that coupling them with the
generated action graphs is a promising approach to
scaling up to longer sequences while maintaining
coherence. Such systems can use sentence-level
semantic parses as training data, meaning they can
leverage existing weakly-supervised shallow pars-
ing techniques. Encouraging for our modelling
paradigm, recent work (Peng et al., 2018) extend-
ing the Dyna-Q (DQ) framework (Sutton, 1990)
demonstrates a real-world application of structured
NLG with a simulated RL training environment.
Given sufﬁcient text generation capabilities, one
may question the added utility of the game en-
vironment (as opposed to learning a direct map-
pingX→K). Recent research suggests that
for stronger generalization, data alone may not
be enough, and symbolic reasoning capabilities
are necessary (Khashabi et al., 2018; Yi et al.,
2018). Given the compositional complexity and
difﬁculty of the language involved, we believe they
will prove necessary in our setting as well.
7 Conclusions
There is a growing need for combining neuro-
symbolic reasoning with advanced language repre-
sentation methods. In the case of procedural text
understanding, key obstacles are suitable training
environments, as well as the lack of fully annotated
action graphs. Motivated by this, we proposed
TEXT2QUEST , an approach intended to enhance
learning by turning raw text inputs into a struc-
tured text-based game environment, as well as aug-
menting data with synthetic fully annotated action
graphs. To encourage further research in this direc-
tion, we publicly release TEXTLABS, an instance
ofTEXT2QUEST for the materials synthesis task.
We implemented prototype modules for basic game
generation and solving. Future work will focus on
designing learning agents to solve the games, as
well as improving text generation capabilities. We
hope that the proposed approach will lead to devel-
oping useful systems for action graph extraction as
well as other language understanding tasks.
References
The materials science procedural text corpus.
https://github.com/olivettigroup/
annotated-materials-syntheses . Ac-
cessed: 5/4/2019.
Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward
Hughes, Pushmeet Kohli, and Edward Grefen-
stette. 2018. Learning to follow language instruc-
tions with adversarial reward induction. CoRR ,
abs/1806.01946.
Trapit Bansal, Arvind Neelakantan, and Andrew Mc-
Callum. 2017. Relnet: End-to-end modeling of enti-
ties & relations. CoRR , abs/1706.07179.
Antoine Bosselut, Omer Levy, Ari Holtzman, Corin
Ennis, Dieter Fox, and Yejin Choi. 2017. Simulat-
ing action dynamics with neural process networks.
CoRR , abs/1711.05313.
Marc-Alexandre C ˆot´e,´Akos K ´ad´ar, Xingdi Yuan, Ben
Kybartas, Tavian Barnes, Emery Fine, James Moore,
Matthew Hausknecht, Layla El Asri, Mahmoud
Adada, Wendy Tay, and Adam Trischler. 2018.
Textworld: A learning environment for text-based
games. CoRR , abs/1806.11532.
Rajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan,
Adam Trischler, and Andrew McCallum. 2018.
Building dynamic knowledge graphs from text
using machine reading comprehension. CoRR ,
abs/1810.05682.
Wenfeng Feng, Hankz Hankui Zhuo, and Subbarao
Kambhampati. 2018. Extracting action sequences
from texts based on deep reinforcement learning.
CoRR , abs/1803.02632.
Jessica B Hamrick. 2019. Analogues of mental sim-
ulation and imagination in deep learning. Current
Opinion in Behavioral Sciences .
Jiarong Jiang, Adam R. Teichert, Hal Daum ´e, and Ja-
son Eisner. 2012. Learned prioritization for trading
off accuracy and speed. In NIPS .
Daniel D. Johnson. 2017. Learning graphical state tran-
sitions. In ICLR 2017 .
Daniel Khashabi, Tushar Khot, Ashutosh Sabharwal,
and Dan Roth. 2018. Question answering as global
reasoning over semantic abstractions. In AAAI .
Edward Kim, Zach Jensen, Alexander van Grootel,
Kevin Huang, Matthew Staib, Sheshera Mysore,
Haw-Shiuan Chang, Emma Strubell, Andrew Mc-
Callum, Stefanie Jegelka, et al. 2018. Inorganic ma-
terials synthesis planning with literature-trained neu-
ral networks. arXiv preprint arXiv:1901.00032 .
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
Zettlemoyer. 2017. Zero-shot relation extraction via
reading comprehension. CoRR , abs/1706.04115.Chen Liang, Jonathan Berant, Quoc Le, Kenneth D.
Forbus, and Ni Lao. 2017. Neural symbolic ma-
chines: Learning semantic parsers on freebase with
weak supervision. pages 23–33.
Zhengdong Lu, Haotian Cui, Xianggen Liu, Yukun
Yan, and Daqi Zheng. 2018. Object-oriented neu-
ral programming (oonp) for document understand-
ing. In ACL.
Li Lucy and Jon Gauthier. 2017. Are distributional
representations ready for the real world? evaluat-
ing word vectors for grounded perceptual meaning.
CoRR , abs/1705.11168.
Adam H. Marblestone, Gregory Wayne, and Konrad P.
K¨ording. 2016. Toward an integration of deep learn-
ing and neuroscience. In Front. Comput. Neurosci.
Diego Marcheggiani and Laura Perez-Beltrachini.
2018. Deep graph convolutional encoders for
structured data to text generation. CoRR ,
abs/1810.09995.
Sheshera Mysore, Edward Kim, Emma Strubell,
Ao Liu, Haw-Shiuan Chang, Srikrishna Kompella,
Kevin Huang, Andrew McCallum, and Elsa Olivetti.
2017. Automatically extracting action graphs from
materials science synthesis procedures. CoRR ,
abs/1711.06872.
Karthik Narasimhan. 2017. Grounding natural lan-
guage with autonomous interaction . Ph.D. thesis,
Massachusetts Institute of Technology, Cambridge,
USA.
Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu,
Kam-Fai Wong, and Shang-Yu Su. 2018. Deep
Dyna-Q: Integrating Planning for Task-Completion
Dialogue Policy Learning.
Scott Reed and Nando de Freitas. Neural Programmer-
Interpreters. pages 1–13.
Richard S. Sutton. 1990. Integrated Architectures for
Learning, Planning, and Reacting Based on Approx-
imating Dynamic Programming. In Machine Learn-
ing Proceedings 1990 .
Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive Psychology , 3(1):1 – 191.
Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Tor-
ralba, Pushmeet Kohli, and Joshua B. Tenenbaum.
2018. Neural-Symbolic VQA: Disentangling Rea-
soning from Vision and Language Understanding.
(NeurIPS).
Tom Zahavy, Matan Haroush, Nadav Merlis,
Daniel Jaymin Mankowitz, and Shie Mannor.
2018. Learn what not to learn: Action elimination
with deep reinforcement learning. In NeurIPS .
A Appendices
A.1 Entity & Relation Types
We have claimed that converting an annotation
schema to a game for TEXTLABS was relatively
straightforward. In this section, we provide de-
tails of the mapping between the Synthesis Project
annotation schema of (denoted with “SP” in the
tables) to the TEXTLABS implementation (denoted
“TL”). A mapping between the central entity types
is presented in Figure 5, as well as the TEXTLABS
actions and representative corresponding relations
in the schema. All current TEXTLABS entities and
actions are shown here, though not all of the orig-
inal entities and relations are listed. For the full
mapping, refer to the project source repository.
A.2 Synthetic Action-Graphs
Figure 6 displays sample representative generated
quests for the various difﬁculty levels evaluated in
Sec. 5, demonstrating the controllable complexity.
As can be seen by comparison with the real text in
Fig. 7 (which is only one sentence), these graphs
correspond to short real-world surfaces, where even
the hardest could by covered by a 2-3 sentence-long
procedure.
A.3 Action-Graphs from Real Annotated
Graphs
We now provide further details on how the origi-
nal Synthesis Project (SP) annotated graphs can be
converted to a TEXTLABS action graph K. There
are some minor differences between the formats,
primarily in the handling of the SP “next-operation”
relation. Rather than use a “next-operation” re-
lation, we currently opt to explicitly model in-
puts/outputs to operations, as can be seen in Fig.
7. This is a natural abstraction away from the sur-
face text enabled by the grounded environment,
and helps in tracking which materials participated
in each operation, which is useful information for
later analysis. Also, as noted, we currently use a
simpliﬁed mapping (for example, many descriptor
annotations such “Amount-Unit”, “Property-Misc”,
etc. are chunked together as generic descriptors).
In Fig. 7 we show Kboth in action graph and ac-
tion sequence form to demonstrate the equivalence.
Also, we note that the “next-operation” annotations
in MSP are currently just placeholders and not the
true labels. For the purpose of demonstration, in
Fig. 7 we manually add the correct annotation to
our example (center and bottom).
Entity Type (SP) Entity Type (TL) Notes
Material Material
Number Descriptor
Operation Operation
Amount-Unit Descriptor
Condition-Unit Operation-Descriptor
Material-Descriptor Material-Descriptor
Condition-Misc Operation-Descriptor
Synthesis-Apparatus Synthesis-Apparatus
Nonrecipe-Material Null Currently ignored, not part of synthesis
Brand Descriptor
Apparatus-Descriptor Synthesis-Apparatus-Descriptor
- Mixture Internal entity, represents a mixture
Relation Type (SP) Action (TL)
Participant-Material input-assign
Apparatus-of locate
Recipe-Target obtain
Descriptor-of link-descriptor
- run-op Internal, used for simulating actions
- take/drop/examine Native TextWorld actions on entities
Figure 5: Central entity/relation types from the Synthesis Project schema (“SP”), and the corresponding T EXT-
LABS version (“TL”).
Figure 6: Sample representative generated quests for various difﬁculty levels (listed in parentheses by each graph).
Each edge corresponds to an action in the text-based game.
Figure 7: Comparisons of the equivalent action graph representations. Top: Action graph section from Synthesis
Project (MSP). Center : T EXTLABS, showing same section with Kin graph form. Dashed borders indicate
operation result entities which may be implicit in the text. Bottom : TEXTLABS with sameKas list of actions
from initial state s0.
Chapter 4
Process-Level Representation of
Scientific Protocols with
Interactive Annotation
Ronen Tamari, Fan Bai, Alan Ritter, Gabriel Stanovsky
Published in the European Chapter of the Association for Compu-
tational Linguistics (EACL), 2021
40
Process-Level Representation of Scientiﬁc Protocols
with Interactive Annotation
Ronen Tamari:˚Fan Bai;Alan Ritter;Gabriel Stanovsky:‹
:The Hebrew University of Jerusalem
;Georgia Institute of Technology
‹Allen Institute for Artiﬁcial Intelligence
{ronent,gabis}@cs.huji.ac.il
{fan.bai,alan.ritter}@cc.gatech.edu
Abstract
We develop Process Execution Graphs (PEG),
a document-level representation of real-world
wet lab biochemistry protocols, addressing
challenges such as cross-sentence relations,
long-range coreference, grounding, and
implicit arguments. We manually annotate
PEGs in a corpus of complex lab protocols
with a novel interactive textual simulator
that keeps track of entity traits and semantic
constraints during annotation. We use this data
to develop graph-prediction models, ﬁnding
them to be good at entity identiﬁcation and
local relation extraction, while our corpus
facilitates further exploration of challenging
long-range relations.1
1 Introduction
There is a drive in recent years towards
automating wet lab environments, where
menial benchwork procedures such as pipetting,
centrifuging, or incubation are software-controlled,
and either executed by fully automatic lab
equipment (Lee and Miles, 2018), or with a
human-in-the-loop (Keller et al., 2019). These
environments allow reliable and precise experiment
reproducbility while relieving researchers from
tedious and laborious work which is prone
to human error (Bates et al., 2017; Prabhu
and Urban, 2017). To achieve this, several
programmatic formalisms are developed to
describe an experiment as an executable program.
For example, Autoprotocol (Lee and Miles, 2018)
deﬁnes a mix predicate taking three arguments:
mode ,speed , and duration .
˚Work begun on an internship at the Allen Institute for
Artiﬁcial Intelligence.
1Our annotated corpus, simulator, annotation interface,
interaction data, and models are available for use by the
research community at https://textlabs.github.
io/.
Figure 1: We develop a scaffold (center) between
sentence-level lab procedure representations (top) and
low-level, lab-speciﬁc instructions (bottom). The
Process Execution Graph (PEG) captures document-
level relations between procedures (orange rounded
nodes) and their arguments (blue rectangular nodes).
A promising direction to leverage automatic
wet-lab environments is a conversion from natural
language protocols, written in expressive free-form
language, to low-level instructions, ensuring a non-
ambiguous, repeatable description of experiments.
In this work, we focus on a crucial ﬁrst step
towards such conversion – the extraction and
representation of the relations conveyed by the
protocol in a formal graph structure, termed
Process Execution Graphs (PEG), exempliﬁed in
Figure 1. PEGs capture both concrete, exact
quantities (“ 30 minutes ”), as well as vague
instructions (“swirl gently ”). A researcher can then
port the PEG (either manually or automatically)
to their speciﬁc lab equipment, e.g., specifying
what constitutes a gentle swirl setting and adding
missing arguments, such as the temperature of the
Figure 2: Example interaction with our simulator,
showing predicate grounding (“chill” is a temp_type
operation) input assignment (“vial” is an argument of
“chill”), validation (warning for a missing argument) and
auto-complete driven by state-tracking, where only legal
instructions in a given state are presented.
incubation in Figure 1.
Formally, PEGs are directed, acyclic labeled
graphs, capturing how objects in the lab (e.g.,
cells,tubes ) are manipulated by lab operations
(e.g., mixing ,incubating ), and in what
order. Importantly, PEGs capture relations which
may span across multiple sentences and implicit
arguments. For example, the PEG in Figure 1
explicitly captures the relation between culture
tubes , mentioned in the ﬁrst sentence, and swirl
andincubate which appear in later sentences.
To annotate long and complex lab protocols,
we develop a text-based game annotation
interface simulating objects and actions in a lab
environment (see example in Figure 2). Our
annotators are given wet-lab protocols written
in natural language taken from biochemistry
publications, and are asked to repeat their steps by
issuing textual commands to the simulator. The
commands are deterministically converted to our
PEG representation. This interface takes much of
the burden off annotators by keeping track of object
traits and commonsense constraints. For example,
when the annotator issues a transfer command
for a container, the simulator moves all its contents
as well. We ﬁnd that in-house annotators were
able to effectively use this interface on complex
protocols, achieving good agreement.
Finally, we use this data to explore several
models, building upon recent advances in graph
prediction algorithms (Luan et al., 2019; Wadden
et al., 2019). We thoroughly analyze model
performance and ﬁnd that our data introduces
interesting new challenges, such as complex co-
reference resolution and long-range, cross-sentence
relation identiﬁcation.In conclusion, we make the following
contributions:
•We formalize a PEG representation for
free-form, natural language lab protocols,
providing a semantic scaffold between
free-form scientiﬁc literature and low-level
instruments instruction.
•We develop a novel annotation interface for
procedural text annotation using text-based
games, and show that it is intuitive enough for
wet-lab protocol annotation by non-experts.
•We release X-WLP, a challenging corpus of
279 PEGs representing document-level lab
protocols. This size is on par with similar
corpora of procedural text (Dalvi et al., 2018;
Mysore et al., 2019; Vaucher et al., 2020).
•We develop two graph parsers: a pipeline
model which chains predictions for graph sub-
components, and a joint-model of mention
and relation detectors.
2 Background and Motivation
Several formalisms for programmatic lab controller
interfaces were developed in recent years (Yachie
and Natsume, 2017; Lee and Miles, 2018). For
instance, Autoprotocol deﬁnes 35 lab commands,
including spin ,incubate , and mix.2While
these deﬁne wet-lab experiments in a precise and
unambiguous manner, they do not readily replace
their natural language description in scientiﬁc
publications, much like a model implementation in
python does not replace its high-level description
in ML papers. Similarly to ML model descriptions,
lab protocols are often not speciﬁed enough to
support direct conversion to low-level programs.
For example, the protocol in Figure 1 does not
specify the swirling (mixing) speed or its duration.
Our process execution graph (PEG) captures
the predicate-argument structure of the protocol,
allowing it to be more lenient than a programming
language (for example, capturing that gently
modiﬁes swirl ). Better suited to represent
underspeciﬁed natural language, PEGs can serve
as a convenient scaffold to support downstream
tasks such as text-to-code assistants (Mehr et al.,
2020). For example, by asking researchers to ﬁll in
missing required arguments for swirl .
To annotate PEGs, we leverage the sentence-
level annotations of Kulkarni et al. (2018) (WLP
henceforth). WLP, exempliﬁed at the top of
2https://autoprotocol.org/specification
Figure 1, collected sentence-level structures using
the BRAT annotation tool (Stenetorp et al., 2012).
For example, capturing that cells,culture tubes
are arguments for add. However, WLP does
not capture cross-sentence implicit relations such
thatculture tubes are an argument for incubate .
These are abundant in lab protocols, require
tracking entities across many sentences, and are not
easy to annotate using BRAT (see discussion in §4).
We vastly extend upon WLP annotations, aiming to
capture the full set of expressed protocol relations,
using a novel text-based games annotation interface
which lends itself to procedural text annotation.
3 Task Deﬁnition: Process Execution
Graphs
Intuitively, we extend the WLP
annotations (Kulkarni et al., 2018) from the
sentence level to entire documents, aiming
to capture allof the relations in the protocol.
Formally, our representation is a directed,
labeled, acyclic graph structure, dubbed a Process
Execution Graph (PEG), exempliﬁed in Figures 1
and 3, and formally deﬁned below.
Nodes PEG nodes are triggered by explicit text
spans in the protocol, e.g., “swirl", or “ice”. Nodes
consist of two types: (1) predicates , marked in
orange: denoting lab operations, such as add or
incubate ; and (2) arguments , marked in blue:
representing physical lab objects (e.g., culture
tubes ,cells), exact quantities ( 30 minutes ), or
abstract instructions (e.g., gently ).
Operation type Frequent example spans Count Pct.
Transfer add, transfer, place 1301 33.2
Temperature
Treatmentincubate, store, thaw 503 12.8
General Initiate, run, do not vortex 469 11.9
Mix mix, vortex, inverting 346 8.8
Spin spin, centrifuge, pellet 282 7.2
Create prepare, make, set up 178 4.5
Destroy discard, decant, pour off 170 4.3
Remove remove, elute, extract 168 4.3
Measure count, weigh, measure 149 3.8
Wash wash, rinse, clean 146 3.7
Time wait, sit, leave 114 2.9
Seal cover, seal, cap 68 1.7
Convert change, transform, changes 21 0.5
Table 1: Details of PEG predicate types, along with
example frequent trigger spans and relative frequency
in X-WLP.Argument type Frequent example spans Count Pct.
Reagent supernatant, dna, sample 3362 32.6
Measurement 1.5 mL, 595nm, 1pmol 1924 18.6
Setting overnight, room temperature 1622 15.7
Location tube, ice, plates 1373 13.3
Modiﬁer gently, carefully, clean 1070 10.3
Device forceps, pipette tip 590 5.7
Method dilutions, pipetting 271 2.6
Seal lid, cap, aluminum foil 97 0.9
Table 2: Details of PEG argument types, along with
example frequent trigger spans and relative frequency
in X-WLP.
Node grounding The PEG formulation above is
motivated as a scaffold towards fully-executable lab
programs employed in automatic lab environments.
To achieve this, we introduce an ontology for
each of the node types, based on the Autoprotocol
speciﬁcation (Lee and Miles, 2018), as indicated
below each text span in Figures 1 and 3. For
example, swirl corresponds to an Autoprotocol
mix operation, a culture tube is of type location ,
and 30 minutes is a setting . See Tables 1,
2 for details of predicate and argument types
respectively, their frequencies in our data and
example spans.
Edges Following PropBank notation (Kingsbury
and Palmer, 2003), PEGs consist of three types
of edges derived from the Autoprotocol ontology,
and denoted by their labels: (1) core-roles (e.g.,
“ARG0”, “ARG1”): indicating predicate-speciﬁc
roles, aligning with Autoprotocol’s ontology. For
example, ARG0 ofmix assigns the element to be
mixed; (2) non-core roles (e.g., “setting”, “site”,
or “co-ref”): indicate predicate-agnostic relations.
For example, the siteargument always marks the
location in which a predicate is taking place; and
(3) temporal edges, labeled with a special “succ”
label: deﬁne a temporal transitive ordering between
predicates. In Figure 1, add occurs before swirl ,
which occurs before incubate . See Table 3 for
predicate-speciﬁc core-role semantics, and Table 6
for non-cores roles types and frequencies of all
roles in X-WLP. See Appendix A.3 for the rules
deﬁning what relations can hold between various
entity types.
Relation to Autoprotocol As shown at the
bottom of Figure 1, a PEG is readily convertible to
Autoprotocol or similar laboratory interfaces once
it is fully instantiated, thanks to edge labels and
node grounding to an ontology. For example, a
Operation Role Semantics Required
SpinARG0 centrifuged to
produce solid phase
ARG1 and/or liquid
phase ARG2ARG0
Convert ARG0 converted to ARG1 ARG0, ARG1
Seal ARG0 sealed with ARG1 ARG0
Create ARG* are created ARG0
General - ARG0
Destroy ARG* discarded ARG0
Measure ARG* to be measured ARG0
Mix ARG* are mixed ARG0
Remove ARG0 removed from ARG1 ARG0
Temperature
TreatmentARG* to be heated/cooled ARG0
Time Wait after operation on ARG0 ARG0
TransferARG* are sources,
transferred to "site"ARG0, site
Wash ARG0 washed with ARG1 ARG0
Table 3: Details of core role semantics for all operation
types. The “Required” column speciﬁes which roles
must be ﬁlled for a given operation. ARG* is short for
tARG0 ,ARG1 ,ARG2u.
researcher can specify what gently means in terms
of mixing speed for their particular lab instruments.
Reentrancies and cross-sentence relations
While the PEG does not form directed cycles,3it
does form non-directed cycles (or reentrancies ) –
where there exists nodes u, vsuch that there are
two different paths from utov. This occurs when
an object participates in two or more temporally-
dependent operations. For example, see culture
tubes , which participates in all operations in
Figure 1. In addition, edges pu, vqmay be
triggered either by within-sentence relations , when
bothuandvare triggered by spans in the same
sentence, or by cross-sentence relations , when u
andvare triggered by spans in different sentences.
In the following section we will show that both
reentrancies and cross-sentence relations, which
are not captured by previous annotations, are
abundant in our annotations.
4 Data Collection: The X-WLP Corpus
In this section, we describe in detail the creation
of our annotated corpus: X-WLP. The protocols in
X-WLP are a subset (44.8%) of those annotated in
the WLP corpus. These were chosen because they
are covered well by Autoprotocol’s ontology (for
details on ontology coverage, see §A.1).
In total, we collected 3,708 sentences (54.1K
3This happens because the temporal relations deﬁne a partial
ordering imposed by the linearity of the execution.X-WLP (ours) MSPTC CSP ProPara
# words 54k 56k 45k 29k
# words / sent. 14.6 26 25.8 9
# sentences 3,708 2,113 1,764 3,300
# sentences / docs. 13.29 9 N/A 6.8
# docs. 279 230 N/A 488
Table 4: Statistics of our annotated corpus (X-WLP).
X-WLP annotates complex documents, constituting
more than 13 sentences on average. X-WLP overall
size is on par with other recent procedural corpora,
including ProPara (Dalvi et al., 2018), material
science (MSPTC; Mysore et al. (2019)) and chemical
synthesis procedures (CSP; Vaucher et al. (2020)). CSP
is comprised of annotated sentences (document level
information is not provided).
tokens) in 279 wet lab protocols annotated with
our graph representation. As can be seen in
Table 4, X-WLP annotates long examples, often
spanning dozens of sentences, and its size is
comparable (e.g., in terms of annotated words) to
the ProPara corpus (Dalvi et al., 2018) and other
related procedural datasets.
4.1 WLP as a Starting Point
Despite WLP’s focus on sentence-level relations
(see top of Figure 1), it is a valuable starting
point for a document-level representation. We
pre-populate our PEG representations with WLP’s
gold object mentions (e.g., cells,30 minutes ),
operation mentions ( swirl andincubate ), and
within-sentence relations (e.g., between gently and
swirl ). We ask annotators to enrich them with
type grounding for operations and arguments, as
well as cross-sentence relations, as deﬁned in §3.
From these annotations we obtain process-level
representations as presented in Figures 1 and 3.
4.2 Process-Level Annotation Interface:
Text-Based Simulator
Annotating cross-sentence relations and grounding
without a dedicated user interface is an arduous
and error-prone prospect. Consider as an example
theligation mixture mention in Figure 3. This
mention is a metonym for vial (5 sentences
earlier), after mixing in the ligase . This kind of
metonymic co-reference is known to be difﬁcult for
annotation (Jurafsky and Martin, 2009), and indeed,
such complicated annotation has been a factor
in the omission of cross-sentence information in
similar domains (Mysore et al., 2019). A simulator
can provide a natural way to account for it by
Figure 3: A full process gold PEG annotation from X-WLP for a real-world wet lab protocol whose text is presented
in the lower right corner (protocol 512), exemplifying several common properties: (1) complex, technical language,
in relatively short sentences; (2) a chain of temporally-dependent, cross-sentence operations; (3) a common object
that is being acted upon through side effects throughout the process ( vial); and (4) vialis mostly omitted in the text
after being introduced in the ﬁrst sentence, despite participating in all following sentences. In the last sentence it
appears with a metonymic expression ( ligation mixture ).
representing the relevant temporal and contextual
information: after sentence 4, vial contains the
ligation buffer mixed with other entities.
To overcome these challenges and achieve high-
quality annotations for this complex task, we
develop a simulator annotation interface, building
upon the TextWorld framework (Côté et al.,
2018). This approach uses text-based games as the
underlying simulator environment, which we adapt
to the biochemistry domain. The human annotator
interacts with the text-based interface to simulate
the raw wet lab protocol (Figure 2): setting the
types of operations (the ﬁrst interaction sets the
span “chill" as a temperature operation) and
assigning their inputs (the last line assigns vial
as an input to chill ), while the simulator tracks
entity states and ensures the correct number and
type of arguments, based on the Autoprotocol
ontology. For example, the second interaction
in Figure 2 indicates a missing argument for the
chill operation (the argument to be chilled).
Finally, tracking temporal dependency (“succ”
edges) is also managed entirely by the simulator by
tracking the order in which the annotator issues the
different operations.
Further assistance is provided to annotators in
the form of an auto-complete tool (last interaction
in Figure 2), visualization of current PEG and a
simple heuristic “linter” (Johnson, 1977) which
ﬂags errors such as ignored entities by producing
a score based on the number of connected
components in the output PEG.See the project web page for the complete
annotation guidelines, visualizations of annotated
protocols, and demonstration videos of the
annotation process.
4.3 Data Analysis
Four in-house CS undergraduate students with
interest in NLP used our simulator to annotate the
protocols of X-WLP, where 44 of the protocols
were annotated by two different annotators to
estimate agreement.
Inter-annotator agreement. We turn
to the literature on abstract meaning
representation (AMR; Banarescu et al., 2013)
for established graph agreement metrics, which
we adapt to our setting. Similarly to our PEG
representation, the AMR formalism has predicate
and argument nodes (lab operations and entities in
our notation) and directed labeled edges which can
form undirected cycles through reentrancies (nodes
with multiple incoming edges).4In Table 5 we
report a graph Smatch score (Cai and Knight,
2013) widely used to quantify AMR’s graph
structure agreement, as well as ﬁner grained graph
agreement metrics, adapted from Damonte et al.
(2017). Smatch values are comparable to those
obtained for AMR, where reported gold agreement
4Unfortunately, we cannot follow this analogy to train
AMR models on our graphs, since, to the best of our
knowledge, they are currently limited to single sentences,
notwithstanding a promising recent initial exploration into
multi-sentence AMR annotation (O’Gorman et al., 2018).
Agreement Metric F1
Smatch 84.99
Argument identiﬁcation 89.72
Predicate identiﬁcation 86.68
Core roles 80.52
Re-entrancies 73.12
Table 5: X-WLP inter-annotator agreement metrics.
Smatch (Cai and Knight, 2013) quantiﬁes overall graph
structure. Following metrics provide a ﬁner-grained
break down (Damonte et al., 2017).
Relation # Intra. # Inter. Total # Re-entrancy
Core
‚ARG0 2962 952 3914 1645
‚ARG1 560 127 687 3
‚ARG2 84 123 207 77
Total (core) 3606 1202 4808 1725
Non-Core
‚site 1306 325 1631 360
‚setting 3499 2 3501 -
‚usage 1114 24 1138 -
‚co-ref 129 1575 1704 -
‚located-at 199 72 271 -
‚measure 2936 18 2954 -
‚modiﬁer 1861 2 1863 -
‚part-of 72 65 137 -
Total (non-core) 11116 2083 13199 360
Temporal 1218 788 2006 -
Grand Total 15940 (80%) 4073 (20%) 20013 2085
Table 6: Breakdown of PEG relation types by frequency
in X-WLP, showing counts of inter/intra-sentence
relations. Re-entrancies are possible only for core
and “site” arguments, and may be either inter or intra-
sentence.
varies between 0.69´0.89(Cai and Knight, 2013),
while our task deals with longer, paragraph length
representations. Reentrancies are the hardest
for annotators to agree on, probably since they
involve longer-range, typically cross-sentence
relations. On the other hand, local decisions such
as argument and predicate identiﬁcation achieve
higher agreement, and also beneﬁt greatly from the
annotations of WLP.
Information gain from process-level annotation.
Analysis of the relations in X-WLP, presented
in Table 6, reveals that a signiﬁcant proportion
of arguments in PEGs are re-entrancies (32.4%)
or cross-sentence (50.3%).5Figure 3 shows a
representative example, with the vialparticipating
in multiple re-entrancies and long-range relations,
5For these calculations we consider only argument relations
that can in principle occur as re-entrancies: “ARG*” and
“site”, see relation ontology in Appendix A.3 for details.
Cross-sentence calculation includes co-reference closure
information.Dataset Avg. #args/op #Ops. w/o core arg. #Ops. Pct.
WLP 1.87 3297 17485 18.9
X-WLP 3.01 0 3915 0.0
Table 7: Comparison of average arguments per
operation and percentage of semantically under-
speciﬁed operations (missing core arguments) in WLP
and X-WLP.
triggered by each sentence in the protocol. These
relations are crucial to correctly model the
protocols at the process level, and are inherently
missed by sentence-level formalisms, showing the
value of our annotations.
To shed light on the additional process-level
information captured by our approach relative to
WLP, in Table 7 we compare the average number
of arguments per operation node as well as the
amount of operation nodes with no core arguments.
For example, see the swirl instruction at the
top of Figure 1: in WLP, this predicate has
no core role argument and is thus semantically
under-deﬁned. X-WLP correctly captures the core
role of culture tubes . By deﬁnition, our use of
input validation by the simulator prevents semantic
under-speciﬁcation, which is likely a signiﬁcant
factor in the higher counts for cross-sentence
relations and overall average arguments in X-WLP.
Annotation cost. The time to annotate an
average document of 13.29 sentences was
approximately 53 minutes (roughly 4 minutes per
sentence), not including annotator training. Our
annotator pay was 13 USD / hour. The overall
annotation budget for X-WLP was roughly 3,200
USD.
5 Models
We present two approaches for PEG prediction.
First, in §5.1 we design models for separate graph
sub-component prediction, which are chained to
form a pipeline PEG prediction model. Second, in
§5.2 we present a model which directly predicts the
entire PEG using a span-graph prediction approach.
5.1 Pipeline Model (P IPELINE )
A full PEG representation as deﬁned in §3 can be
obtained by chaining the following models which
predict its sub-components. In all of these, we use
SciBERT (Beltagy et al., 2019) which was trained
on scientiﬁc texts similar to our domain.
Mention identiﬁcation. Given a scientiﬁc
protocol written in natural language, we begin
by identifying all experiment-involved text spans
mentioning lab operations (predicates) or entities
and their traits (arguments), which are the building
blocks for PEGs. We model this problem of
mention identiﬁcation as a sequence tagging
problem. Speciﬁcally, we transfer span-level
mention labels, which are annotated in the WLP
corpus into token-level labels using the BIO
tagging scheme, then ﬁne-tune the SciBERT model
for token classiﬁcation.
Predicate grounding. Next, we ground
predicate nodes into the operation ontology types
discussed in §3. See Table 1 in the Appendix for
the complete list. Predicted mentions are marked
using special start and end tokens ( [E-start]
and[E-end] ), then fed as input to SciBERT. The
contextual embedding of [E-start] is input to
a linear softmax layer to predict the ﬁne-grained
operation type.
Operation argument role labeling. Once the
operation type is identiﬁed, we predict its
semantic arguments and their roles. Given
an operation and an argument mention, four
special tokens are used to specify the positions
of their spans (Baldini Soares et al., 2019). Type
information is also encoded into the tokens, for
example, when the types of the operator and its
argument are mix-op andreagent respectively,
four special tokens [E1-mix-op-start] ,
[E1-mix-op-end] ,[E2-rg-start] and
[E2-rg-end] are used to denote the spans
of the mention pair. After feeding the input
into SciBERT, the contextualized embeddings of
[E1-op-mix-start] and[E2-rg-start]
are concatenated as input to a linear layer that
is used to predict the entity’s argument role.
Arguments of an operation can be selected from
anywhere in the protocol, leading to many cross-
sentence operation-argument link candidates. To
accommodate cross-sentence argument roles, we
use the entire document as input to SciBERT for
each mention pair. However, SciBERT is limited
to processing sequences of at most 512 tokens.
To address this limitation, longer documents are
truncated in a way that preserves surrounding
context, when encoding mention pairs.6Only 8
6Given an input document, which has more than 512 words,
withnwords between two mentions, we truncate the contextof the 279 protocols in our dataset contain more
than 512 tokens.
Temporal ordering. Finally, we model order of
operations using the succ relation (see Figure 3).
These are predicted using a similar approach as
argument role labeling, where special tokens are
used to encode operation spans.
5.2 Jointly-Trained Model (M ULTI -TASK )
To explore the beneﬁts of jointly modeling
mentions and relations, we experiment with
a graph-based multi-task framework based on
DYGIE++ model (Wadden et al., 2019). Candidate
mention spans are encoded using SciBERT, and a
graph is constructed based on predicted X-WLP
relations and argument roles. A message-passing
neural network is then used to predict mention
spans while propagating information about related
spans in the graph (Dai et al., 2016; Gilmer et al.,
2017; Jin et al., 2018).
This approach requires computing hidden state
representations for all Opn4qpairs of spans in
an input text, which for long sequences, will
exhaust GPU memory. While Wadden et al. (2019)
considered primarily within-sentence relations,
our model must consider relations across the
entire protocol, which makes this a problem of
practical concern. To address this, we encode a
sliding window of wadjacent sentences when the
full protocol does not ﬁt into memory, allowing
smaller windows for the start and end of the
protocol, and concatenate sentences within each
window as inputs to the model. As a result, each
sentence is involved in wwindows leading to
repeated, possibly contradicting predictions for
both mentions and relations. To handle this,
we output predictions agreed upon by at least k
windows, where kis a hyperparameter tuned on a
development set.
6 Experiments
In §5, we presented a pipelined approach to PEG
prediction based on SciBERT and a message-
passing neural network that jointly learns span
and relation representations. Next, we describe
the details of our experiments and present
empirical results demonstrating that X-WLP
supports training models that can predict PEGs
from natural language instructions.
to keep at most p512´nq{2words for each side.
Data Split System F1
originalKulkarni et al. (2018) 78.0
Wadden et al. (2019) 79.7
PIPELINE 78.3
X-WLP-eval P IPELINE 74.7
Table 8: Mention identiﬁcation test set F 1scores for
models on the WLP dataset. Top: WLP dataset with the
original train/dev/test split. Bottom: excluding X-WLP
protocols from the WLP training data, and using them
for evaluation.
Data. X-WLP is our main dataset including 279
fully annotated protocols. Statistics of X-WLP
are presented in Table 4. Additionally, we have
344 protocols from the original WLP dataset. We
use this auxiliary data only for training mention
taggers in the pipeline model, and use X-WLP for
all other tasks. For argument role labeling and
temporal ordering, negative instances are generated
by enumerating all possible mention pairs whose
types appear at least once in the gold data. We use
5-fold cross validation; 2 folds (112 protocols) are
used for development, and the other 3 folds (167
protocols) are used to report ﬁnal results.
Model setup. ThePIPELINE framework employs
a separate model for each task, by default using
the propagated predictions from previous tasks as
input. In addition, we evaluate the model for each
task with gold input denoted as PIPELINE (gold) .
Finally, the MULTI -TASK framework learns all
tasks together and we decompose its performance
into the component subtasks.
Implementation details. We use the uncased
version of SciBERT7for all our models due to the
importance of in-domain pre-training. The models
under the PIPELINE system are implemented using
Huggingface Transformers (Wolf et al., 2020), and
we use AdamW with the learning rate 2ˆ10´5
for SciBERT ﬁnetuing. For the MULTI -TASK
framework, we set the widow size wto 5, the
maximum value that enables the model to ﬁt in
GPU memory. For all other hyperparameters,
we follow the settings of the WLP experiments
in (Wadden et al., 2019).
6.1 Results
The results of the two models on the different
subtasks are presented in Tables 8- 11. We identify
three main observations based on these results.
7https://github.com/allenai/scibertSystem P R F1
MULTI -TASK 76.0 69.0 72.3
PIPELINE 71.8 76.3 74.0
‚w/ gold mentions 79.0 80.2 79.6
Table 9: Predicate grounding test set results.
Task M ULTI -TASK PIPELINE # gold
Core
‚All roles 57.9 53.7 2839
‚All roles (gold mentions) - 76.5 2839
‚ARG0 61.0 57.1 2313
‚ARG1 36.1 32.9 412
‚ARG2 69.7 61.4 114
Non-Core
‚All roles 55.7 48.8 4826
‚All roles (gold mentions) - 78.1 4826
‚site 58.7 55.4 962
‚setting 77.4 74.7 974
‚usage 35.6 33.0 296
‚co-ref 39.8 36.7 1014
‚measure 63.3 56.6 804
‚modiﬁer 51.0 41.8 519
‚located-at 9.7 13.3 179
‚part-of 0.5 10.8 78
Temporal Ordering 61.8 57.3 2176
Temp. Ord. (gold mentions) - 76.3 2176
Table 10: Operation argument role labeling (core and
non-core roles, decomposed by relation) and temporal
ordering test set F 1performance.
Split M ULTI -TASK PIPELINE # gold
Intra-sentence 63.4 58.2 2160
Inter-sentence 32.5 39.1 679
Table 11: Operation argument role labeling (core
roles) test set F 1, decomposed based on whether
the operation and the argument are triggered within
the same sentence (intra-sentence) versus different
sentences (inter-sentence).
First, PIPELINE outperforms MULTI -TASK on
the operation classiﬁcation task in Table 9, as it
uses all protocols from WLP as additional training
data to improve mention tagging.
Second, MULTI -TASK performs better than the
PIPELINE approach on most relation classiﬁcation
tasks in Table 10, but is worse than PIPELINE when
PIPELINE uses gold mentions, demonstrating that
jointly modeling mentions and relations helps in
mitigating error propagation.
Third, cross-sentence relations are challenging
for both models, as shown in Table 11. This
explains the low performance of co-ref , which
is comprised of 92.4% cross-sentence relations.
In addition, there are a couple of interesting
points to note. In Table 8, the performance of
PIPELINE on the X-WLP subset is lower than its
performance on the WLP test set, likely because
there are fewer protocols in the training set. For
the relation-decomposed performance in Table 10,
we can see that some of the relations like “ARG2”
can be correctly predicted by MULTI -TASK using
only a few gold labels while some more widely
used relations are harder to learn, such as “ARG0”
and “site”; indeed, “ARG2” is only used in the
spin operation (see Table 3), while the other roles
participate in more diverse contexts.
7 Related Work
Natural Language Processing (NLP) for scientiﬁc
procedural text is a rapidly growing ﬁeld. To-
date, most approaches have focused on text-
mining applications (Isayev, 2019) and typically
annotate only shallow, sentence-level semantic
structures (e.g., Fig. 1, top). Examples include
WLP (Kulkarni et al., 2018) and materials science
procedures (Mysore et al., 2019; Kuniyoshi et al.,
2020). Recent interest in automation of lab
procedures has also led to sentence-level annotation
of procedural texts with action sequences designed
to facilitate execution (Vaucher et al., 2020).
However, as noted in recent concurrent
work (Mehr et al., 2020), neither sentence-level
semantic structures nor action sequences are
sufﬁcient for the goal of converting text to
a machine-executable synthesis procedure; for
this purpose, a more structured, process-level
semantic representation is required. In particular,
executable representations require a structured
declaration of the locations and states of the
different materials throughout a process, details
not represented by sentence-level annotations.
Our simulator can naturally represent such
information by maintaining a stateful model of
the process. Simulation ﬁdelity can be controlled
by implementing the execution semantics of
operations to the level of detail required.
Mehr et al. (2020) have similarly proposed a
process-level executable representation, but use
an NLP pipeline consisting primarily of rules and
simple pattern matching, relying on a human-in-
the-loop for corrections; linking our approach with
their framework is a promising future direction.
Structurally, PEGs are similar to abstract
meaning representation (AMR; Banarescu et al.
2013), allowing us to use agreement and
performance metrics developed for AMR. In
contrast with the sentence-level AMR, a major
challenge in this work is annotating and predictingprocedure-level representations.8
Another line of research focuses on procedural
text understanding for more general domains:
simple scientiﬁc processes (Dalvi et al., 2018),
open domain procedural texts (Tandon et al., 2020),
and cooking recipes (Kiddon et al., 2015; Bosselut
et al., 2018). These works represent process-level
information and entity state changes, but typically
feature shorter processes, simpler language and an
open ontology, compared with our domain-speciﬁc
terminology and grounded ontology.
Our framework also provides a link to text-based
game approaches to procedural text understanding.
Tamari et al. (2019) modelled scientiﬁc procedures
with text-based games but used only synthetic data.
Our simulator enables leveraging recent advances
on text-based games agents (e.g., (Adhikari et al.,
2020)) towards natural language understanding.
8 Conclusion
We developed a novel meaning representation and
simulation-based annotation interface, enabling
the collection of process-level annotations of
experimental procedures, as well as two parsers
(pipeline and joint modelling) trained on this
data. Our dataset and experiments present
several directions for future work, including the
modelling of challenging long range dependencies,
application of text-based games for procedural
text understanding, and extending simulation-based
annotation to new domains.
Acknowledgments
We would like to thank Peter Clark, Noah
Smith, Yoav Goldberg, Dafna Shahaf, and Reut
Tsarfaty for many fruitful discussions and helpful
comments, as well as the X-WLP annotators:
Pranay Methuku, Rider Osentoski, Noah Zhang
and Michael Zhan. This work was partially
supported by an Allen Institute for AI Research
Gift to Gabriel Stanovsky. This material is
based upon work supported by the NSF (IIS-
1845670) and the Defense Advanced Research
Projects Agency (DARPA) under Contract No.
HR001119C0108. The views, opinions, and/or
ﬁndings expressed are those of the author(s) and
should not be interpreted as representing the ofﬁcial
views or policies of the Department of Defense or
the U.S. Government.
8In addition, in contrast with AMR, PEG nodes are directly
mapped to the trigger spans in the document.
References
Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté,
Mikuláš Zelinka, Marc-Antoine Rondeau, Romain
Laroche, Pascal Poupart, Jian Tang, Adam Trischler,
and Will Hamilton. 2020. Learning dynamic belief
graphs to generalize on text-based games. Advances
in Neural Information Processing Systems , 33.
Livio Baldini Soares, Nicholas FitzGerald, Jeffrey
Ling, and Tom Kwiatkowski. 2019. Matching
the blanks: Distributional similarity for relation
learning. In Proceedings of the 57th Annual Meeting
of the Association for Computational Linguistics ,
pages 2895–2905, Florence, Italy. Association for
Computational Linguistics.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Grifﬁtt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In LAW@ACL .
Maxwell Bates, Aaron J Berliner, Joe Lachoff, Paul R
Jaschke, and Eli S Groban. 2017. Wet lab accelerator:
a web-based application democratizing laboratory
automation for synthetic biology. ACS synthetic
biology , 6(1):167–171.
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT:
A pretrained language model for scientiﬁc text. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) , pages 3615–3620,
Hong Kong, China. Association for Computational
Linguistics.
Antoine Bosselut, Corin Ennis, Omer Levy, Ari
Holtzman, Dieter Fox, and Yejin Choi. 2018.
Simulating action dynamics with neural process
networks. In International Conference on Learning
Representations .
Shu Cai and Kevin Knight. 2013. Smatch: an
evaluation metric for semantic feature structures.
InProceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers) , pages 748–752, Soﬁa, Bulgaria.
Association for Computational Linguistics.
Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben
Kybartas, Tavian Barnes, Emery Fine, James Moore,
Matthew Hausknecht, Layla El Asri, Mahmoud
Adada, et al. 2018. Textworld: A learning
environment for text-based games. In Workshop on
Computer Games , pages 41–75. Springer.
Hanjun Dai, Bo Dai, and Le Song. 2016. Discriminative
embeddings of latent variable models for structured
data. In International conference on machine
learning , pages 2702–2711.
Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau
Yih, and Peter Clark. 2018. Tracking state changes in
procedural text: a challenge dataset and models forprocess paragraph comprehension. In Proceedings of
the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long
Papers) , pages 1595–1604, New Orleans, Louisiana.
Association for Computational Linguistics.
Marco Damonte, Shay B. Cohen, and Giorgio
Satta. 2017. An incremental parser for Abstract
Meaning Representation. In Proceedings of the
15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
1, Long Papers , pages 536–546, Valencia, Spain.
Association for Computational Linguistics.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley,
Oriol Vinyals, and George E Dahl. 2017. Neural
message passing for quantum chemistry. In
Proceedings of the 34th International Conference
on Machine Learning-Volume 70 . JMLR. org.
Olexandr Isayev. 2019. Text mining facilitates materials
discovery. Nature , 571(7763):42–43.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola.
2018. Junction tree variational autoencoder for
molecular graph generation. In International
Conference on Machine Learning .
Stephen C Johnson. 1977. Lint, a C program checker .
Citeseer.
Daniel Jurafsky and James H. Martin. 2009. Speech and
Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics,
and Speech Recognition , second edition. Pearson
Prentice Hall.
Ben Keller, Justin Vrana, Abraham Miller, Garrett
Newman, and Eric Klavins. 2019. Aquarium: The
Laboratory Operating System version 2.6.0.
Chloé Kiddon, Ganesa Thandavam Ponnuraj, Luke
Zettlemoyer, and Yejin Choi. 2015. Mise en place:
Unsupervised interpretation of instructional recipes.
InProceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing , pages 982–
992, Lisbon, Portugal. Association for Computational
Linguistics.
Paul Kingsbury and Martha Palmer. 2003. Propbank:
the next level of treebank.
Chaitanya Kulkarni, Wei Xu, Alan Ritter, and
Raghu Machiraju. 2018. An annotated corpus
for machine reading of instructions in wet lab
protocols. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers) , pages 97–
106, New Orleans, Louisiana. Association for
Computational Linguistics.
Fusataka Kuniyoshi, Kohei Makino, Jun Ozawa, and
Makoto Miwa. 2020. Annotating and extracting
synthesis process of all-solid-state batteries from
scientiﬁc literature. In Proceedings of the 12th
Language Resources and Evaluation Conference ,
pages 1941–1950, Marseille, France. European
Language Resources Association.
Peter L Lee and Benjamin N Miles. 2018. Autoprotocol
driven robotic cloud lab enables systematic machine
learning approaches to designing, optimizing, and
discovering novel biological synthesis pathways. In
SIMB Annual Meeting 2018 . SIMB.
Yi Luan, Dave Wadden, Luheng He, Amy Shah, Mari
Ostendorf, and Hannaneh Hajishirzi. 2019. A
general framework for information extraction using
dynamic span graphs. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short
Papers) , pages 3036–3046, Minneapolis, Minnesota.
Association for Computational Linguistics.
S. Hessam M. Mehr, Matthew Craven, Artem I.
Leonov, Graham Keenan, and Leroy Cronin. 2020.
A universal system for digitization and automatic
execution of the chemical synthesis literature.
Science , 370(6512):101–108.
Ben Miles and Peter L. Lee. 2018. Achieving
reproducibility and closed-loop automation in
biological experimentation with an iot-enabled lab
of the future. SLAS TECHNOLOGY: Translating
Life Sciences Innovation , 23(5):432–439. PMID:
30045649.
Sheshera Mysore, Zachary Jensen, Edward Kim, Kevin
Huang, Haw-Shiuan Chang, Emma Strubell, Jeffrey
Flanigan, Andrew McCallum, and Elsa Olivetti.
2019. The materials science procedural text corpus:
Annotating materials synthesis procedures with
shallow semantic structures. In Proceedings of the
13th Linguistic Annotation Workshop , pages 56–64.
Tim O’Gorman, Michael Regan, Kira Grifﬁtt, Ulf
Hermjakob, Kevin Knight, and Martha Palmer.
2018. AMR beyond the sentence: the multi-
sentence AMR corpus. In Proceedings of the
27th International Conference on Computational
Linguistics , pages 3693–3702, Santa Fe, New
Mexico, USA. Association for Computational
Linguistics.
Gurpur Rakesh D Prabhu and Pawel L Urban. 2017.
The dawn of unmanned analytical laboratories. TrAC
Trends in Analytical Chemistry , 88:41–52.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi ´c,
Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsujii.
2012. Brat: A web-based tool for nlp-assisted text
annotation. In Proceedings of the Demonstrations
at the 13th Conference of the European Chapter
of the Association for Computational Linguistics ,
EACL ’12, page 102–107, USA. Association for
Computational Linguistics.Ronen Tamari, Hiroyuki Shindo, Dafna Shahaf, and
Yuji Matsumoto. 2019. Playing by the book: An
interactive game approach for action graph extraction
from text. In Proceedings of the Workshop on
Extracting Structured Knowledge from Scientiﬁc
Publications , pages 62–71, Minneapolis, Minnesota.
Association for Computational Linguistics.
Niket Tandon, Keisuke Sakaguchi, Bhavana Dalvi,
Dheeraj Rajagopal, Peter Clark, Michal Guerquin,
Kyle Richardson, and Eduard Hovy. 2020. A dataset
for tracking entities in open domain procedural
text. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP) , pages 6408–6417, Online. Association for
Computational Linguistics.
Alain C. Vaucher, Federico Zipoli, Joppe Geluykens,
Vishnu H. Nair, Philippe Schwaller, and Teodoro
Laino. 2020. Automated extraction of chemical
synthesis actions from experimental procedures.
Nature Communications , 11(1):1–11.
David Wadden, Ulme Wennberg, Yi Luan, and
Hannaneh Hajishirzi. 2019. Entity, relation,
and event extraction with contextualized span
representations. In Proceedings of the 2019
Conference on Empirical Methods in Natural
Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 5784–5789, Hong Kong,
China. Association for Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le
Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander M. Rush. 2020. Transformers:
State-of-the-art natural language processing. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online. Association
for Computational Linguistics.
Nozomu Yachie and Tohru Natsume. 2017. Robotic
crowd biology with maholo labdroids. Nature
Biotechnology , 35(4):310–312.
A Annotation Schema
In the following subsections, we provide further
details of the annotation schema used. Section
§A.1 describes how the ontology was constructed
based on Autoprotocol, and §A.2 provides details
on ontology coverage for the X-WLP protocols
which were chosen for annotation. Section §A.3
details the rules deﬁning valid PEG edges, or what
relations can hold between various entity types.
The annotation guidelines given to annotators are
available on the project web page.
A.1 Ontology Construction
Operation nodes correspond to “action” entities
in WLP. In X-WLP, to facilitate conversion to
executable instructions, we further add a ﬁne-grain
operation type; for each operation, annotators were
required to select the closest operation type, or a
general type if none applied.
To deﬁne our operation type ontology, we
consulted the Autoprotocol (Miles and Lee,
2018) open source standard used for executable
biology lab protocols. Autoprotocol deﬁnes
35 different operation types,9from which we
grouped relevant types into higher level clusters;
X-WLP operation types are broadly aligned with
Autoprotocol operation types, but are more general
in scope, to not limit applicability to any one
platform. For example, we use a more general
measure operation type rather than the speciﬁc
types of measurement operations in Autoprotocol
(spectrophotometry ,measure-volume ,
etc.).
Table 12 maps between X-WLP operation
types and their equivalents in Autoprotocol, if
one exists. The X-WLP operation types do
not perfectly overlap with Autoprotocol as the
former is written for humans, while the latter
is designed for the more constrained domain of
robot execution. Accordingly, some operations not
currently supported in Autoprotocol were added,
likewash . See Table 1 for example mention spans
for each X-WLP operation type.
The set of supported operations was chosen to
maximize coverage over the types of operations
found in the sentence-level annotations of WLP
(see §A.2 below for details).
9Based on https://github.com/autoprotocol/
autoprotocol-python/blob/master/
autoprotocol/instruction.py as of January
2021.A.2 Ontology Coverage
To identify candidate protocols for annotation
which were well covered by the ontology, we
created a mapping between ontology instruction
types and the 100 most frequent text-spans of
WLP action entities (constituting 74% of all
action spans in WLP). WLP action text spans that
didn’t correspond to any ontology instruction were
mapped to a general label; action text spans that
could be mapped to the ontology we call ontology-
covered actions. For annotation in X-WLP, we
then selected WLP protocols estimated to have a
high percentage of ontology-covered actions (based
on the mapping above). This simple method was
found to be effective in practice, as measured by the
actual ontology coverage of X-WLP annotations,
summarized in Fig. 4.
For each annotated protocol, we calculated the
percentage of known (not general ) operations.
Fig. 4 plots, for each coverage percentile ( y-axis),
the percentage ( x-axis) of X-WLP protocols with
at least ypercent known operations. From the plot
we can see for example that half of the protocols in
X-WLP have >90% ontology coverage, and 90%
of the protocols have >70% ontology coverage.
X-WLP Operation Autoprotocol Instructions
Spin Spin
Convert N/A
Seal Seal, Cover
Create Oligosynthesize, Provision
General N/A
Destroy N/A
MeasureAbsorbance, Fluorescence,
Luminescence, IlluminaSeq,
SangerSeq, MeasureConcentration,
MeasureMass, MeasureV olume,
CountCells, Spectrophotometry,
FlowCytometry, FlowAnalyze,
ImagePlate
Mix Agitate
Remove Unseal, Uncover
Temperature TreatmentThermocycle, Incubate,
FlashFreeze
TransferAcousticTransfer,
MagneticTransfer,
Dispense, Provision,
LiquidHandle, Autopick
Wash N/A
Time N/A
Table 12: Mapping between X-WLP operation types
and corresponding Autoprotocol instructions (if any
exist). Autoprotocol operations tend to be more speciﬁc
as they are intended for machine execution. X-WLP
protocols are written for humans, so operation types are
deﬁned at a higher level of abstraction.
Figure 4: Plot displaying for each coverage percentile
(y-axis), the percentage ( x-axis) of X-WLP protocols
with at least ypercent known (ontology-covered)
operations.
A.3 Syntax governing PEG edges
Formally, edges are represented by triplets of the
formps, r, tqwhere sandtare argument nodes
andris a core or non-core role. Dependent on a
particular role r, certain restrictions may apply to
the ﬁne-grained type of sandt, as described below.
A.3.1 Core Roles
Core roles, displayed in Table 3, represent
operation speciﬁc roles, for example “ARG1” for
theseal operation is a sealentity representing the
seal of the “ARG0” argument. For core roles, the
following restrictions hold:
•Source nodes s are restricted
to any of the object types sP
treagent ,device ,seal,locationurepresenting
physical objects. The only exception to this
rule is that “ARG1” for the seal operation
must be a sealentity.
•Target node tis a predicate of one of the types
in Table 1.
•ris a core argument relation, rP
tARG0 ,ARG1 ,ARG2uor ARG* for short.
•Certain roles may be required for a valid
predicate t, for example the transfer
operation requires at minimum both source
and target arguments to be speciﬁed by the
ARG0 and “site” roles, respectively.Role Source Types Target Types
co-ref Object Object
measure Measurement Object
setting Setting Object
modiﬁer ModiﬁerObject, Operation,
Measurement
usage Method, Object Operation
located-at Object Object
part-of Object Object
Table 13: Details of non-core roles and restrictions
on source and target node types. Object is short for
the set of entity types representing physical objects:
treagent ,device ,seal,locationu.
A.3.2 Non-core Roles
Non-core roles (e.g., “setting”, “site”, or “co-ref”)
indicate predicate-agnostic labels. For example, the
siteargument always marks the location in which
a predicate is taking place. Non-core roles are
displayed in Table 13, and role-speciﬁc restrictions
onsandtare listed under “Source Types” and
“Target Types”, respectively.
Chapter 5
Dyna-bAbI: unlocking bAbI’s
potential with dynamic synthetic
benchmarking
Ronen Tamari, Kyle Richardson, Aviad Sar-Shalom, Noam
Kahlon, Nelson Liu, Reut Tsarfaty, Dafna Shahaf
Published in the Joint Conference on Lexical and Computational
Semantics (*SEM), 2022
54
Dyna-bAbI: unlocking bAbI’s potential with dynamic synthetic
benchmarking
Ronen Tamari†∗Kyle Richardson⋆Noam Kahlon†Aviad Sar-Shalom△
Nelson F. Liu⋄Reut Tsarfaty⋆‡Dafna Shahaf†
†The Hebrew University of Jerusalem⋆Allen Institute for AI
‡Bar-Ilan University△Tel-Aviv University⋄Stanford University
{ronent,dshahaf}@cs.huji.ac.il ,{reutt,kyler}@allenai.org
Abstract
While neural language models often perform
surprisingly well on natural language
understanding (NLU) tasks, their strengths
and limitations remain poorly understood.
Controlled synthetic tasks are thus an
increasingly important resource for diagnosing
model behavior. In this work we focus on
story understanding, a core competency
for NLU systems. However, the main
synthetic resource for story understanding,
the bAbI benchmark, lacks such a systematic
mechanism for controllable task generation.
We develop Dyna-bAbI, a dynamic framework
providing fine-grained control over task
generation in bAbI. We demonstrate our ideas
by constructing three new tasks requiring
compositional generalization, an important
evaluation setting absent from the original
benchmark. We tested both special-purpose
models developed for bAbI as well as
state-of-the-art pre-trained methods, and found
that while both approaches solve the original
tasks (>99% accuracy), neither approach
succeeded in the compositional generalization
setting, indicating the limitations of the
original training data. We explored ways to
augment the original data, and found that
though diversifying training data was far
more useful than simply increasing dataset
size, it was still insufficient for driving robust
compositional generalization (with <70%
accuracy for complex compositions). Our
results underscore the importance of highly
controllable task generators for creating robust
NLU systems through a virtuous cycle of
model and data development.1
1 Introduction
Considerable progress has been made recently
in natural language understanding (NLU), driven
largely by advances in model pre-training (Devlin
∗Work begun during an internship at the Allen Institute.
1Data and code available at https://
dyna-babi-project.github.io/ .
Figure 1: (a) Low task configurability leads to
static datasets, benchmark saturation & unreliable
model development. (b) We propose a dynamic
benchmarking approach; developing models and tasks
in a tight feedback loop using (c) Dyna-bAbI task
generator. Dyna-bAbI provides fine-grained control
over task structure, composition and difficulty, yielding
challenging new test sets exposing limitations of state-
of-the-art models.
et al., 2019; Raffel et al., 2020) and the
development of large-scale NLU benchmarks
across a wide range of tasks (Wang et al., 2018,
2019; Liang et al., 2020). Such successes, however,
have coincided with the discovery of various
shortcomings in existing human curated datasets,
largely related to annotation artifacts (Gururangan
et al., 2018), or systematic biases that create
shortcuts that can inflate model performance and
harm generalization.
In order to overcome these issues, two
avenues of research have recently gained traction:
1) development of dynamic benchmarks (Potts
et al., 2021; Kiela et al., 2021) where, in contrast
to conventional static benchmarks, evaluation
and data collection are conducted interactively
with humans and models in a rapidly evolving
feedback loop and; 2) renewed interest in synthetic
benchmarks (Lake and Baroni, 2018; Sinha et al.,
2019; Clark et al., 2020; Ruis et al., 2020) that
allow for absolute control over the data creation
process in order to help understand the strengths
and weaknesses of existing models on targeted
tasks and language phenomena.
Story understanding is a particularly important
domain for research on dynamic and synthetic
benchmarks; it is a core competency for NLU
systems (McClelland et al., 2020; Dunietz et al.,
2020), but the scale and annotation detail required
make human data collection prohibitively costly.
However, the main synthetic resource for story
understanding remains the bAbI task suite (Weston
et al., 2016), which is saturated by models
reaching near-perfect performance (Liu et al.,
2021), and further limited by exploitable biases
in the data (Kaushik and Lipton, 2018). Despite
its creators’ initial intentions, bAbI has largely
remained a static benchmark limited to a small
subset of the tasks potentially possible to generate
within the bAbI “micro-world”. Accordingly, two
natural questions arise: (Q1) is near-perfect model
performance on the original bAbI tasks a reliable
indicator of story understanding competence? ;
(Q2) are there still interesting challenges to
discover inside the broader bAbI task space that
help identify weaknesses in current models and
drive modelling innovation?
To answer these questions, we employ a
dynamic synthetic benchmarking approach on
bAbI, combining the benefits of the agile approach
of recent dynamic benchmarks with the scale
and control provided by synthetic datasets. As
illustrated in Figure 1, in dynamic synthetic
benchmarks the data generator itself is designed
for agile development, enabling experimentation
with increasingly complex tasks and a wider
range of linguistic phenomena.2Constructing
2While our framework does not enable automatic collectionchallenging tasks is a challenge in and of itself,
requiring precise control over the reasoning
patterns underlying each question. To meet these
requirements, we developed a new task generator
for bAbI called Dyna-bAbI3.
Using Dyna-bAbI, we first devise new splits that
systematically test compositional generalization
across tasks; as shown in Fig. 1c, we test models
on novel combinations (right side, line 10) of
concepts seen at training, like co-reference and
object tracking (left). We find that training on the
original bAbI tasks (hereafter: bAbI 1.0) is not
sufficient for models to attain good compositional
generalization. Though general purpose pre-trained
models far outperform special-purpose (non-pre-
trained) architectures developed for bAbI, they still
suffer a 20-50% drop in accuracy compared to
the non-pre-trained models which suffer a 50-80%
drop. Both types attain near perfect performance
on the original tasks, suggesting that bAbI 1.0 is
not challenging enough to differentiate between the
two classes of models (Q1) .
We next investigate how different enhancements
of training data affect compositional generalization:
(a) injecting more questions into bAbI 1.0, and
(b) generating new, more diverse training samples.
Compared to question injection, we find that
diverse training data better facilitates compositional
generalization, as well as being more data
efficient. However, neither approach drives reliable
compositional generalization; a representative
state-of-the-art (SOTA) model, T5 (Raffel et al.,
2020), demonstrates a lack of robustness to
novel combinations and also exhibits knowledge
inconsistency, for example, by correctly answering
certain types of questions but systematically
failing to answer equivalent paraphrases. These
results suggest that there remain many important
challenges within the broader bAbI task space (Q2)
which can be discovered through more careful
control of task generation.
To sanity-check the quality of our new tests
compared with bAbI 1.0, we employ the notion
ofconcurrence proposed by Liu et al. (2021);
of new data based on model errors as in other dynamic
benchmarks, we still chose the term “dynamic” to
highlight their important common function: data generation
frameworks that enable easily “moving the goalposts” in
meaningful directions (in our case, for probing models’
systematic generalization capacities).
3Implemented in Python for improved accessibility compared
with the original Lua implementation ( https://github.
com/facebookarchive/bAbI-tasks ).
concurrence is a measure of correlation between
models’ performance on a synthetic task and their
performance on an existing, non-synthetic NLU
benchmark. We find high concurrence between our
new challenge tasks and the widely used SQuAD
dataset (Rajpurkar et al., 2016), in contrast to bAbI
1.0, which achieved low concurrence.
Giving the continued interest in using bAbI 1.0
to evaluate new modelling approaches (Banino
et al., 2020, 2021; Schlag et al., 2021), our new
challenge splits and the Dyna-bAbI task generator
contribute to more reliably guiding future efforts.
While we focused on bAbI, our results apply more
generally, telling a cautionary tale about the limits
of static synthetic datasets, and motivating the
development of controllable task generators for
dynamic synthetic benchmarking.
2 Related Work
Our work brings together two promising areas of
current research: dynamic benchmarking such as
Dynabench (Kiela et al., 2021) that address many
existing issues with static benchmarks (Bowman
and Dahl, 2021), and synthetic benchmarking,
which is widely used for high-precision and data-
intensive problems such as relational and logical
reasoning (Sinha et al., 2019; Clark et al., 2020;
Betz et al., 2021; Richardson and Sabharwal, 2022),
robot planning (Banerjee et al., 2020), instruction
following and language grounding (Long et al.,
2016; Lake and Baroni, 2018) among many others
(Richardson et al., 2020; Khot et al., 2021). Most
approaches to synthetic benchmarking focus on
model development on a static benchmark, and
are not designed to facilitate agile and highly
controlled task space exploration, which is our
focus here.
The recent gSCAN dataset (Ruis et al., 2020) and
later extensions (Qiu et al., 2021; Wu et al., 2021)
can be seen as an example of a synthetic benchmark
“going dynamic”. Our work differs in terms of
target domain (story understanding as opposed to
multi-modal language grounding), and we further
focus attention on a more general research direction
of intentional, a-priori design of NLU benchmarks
for agile development. In this regard, our work
can be seen as part of a trend towards data-centric
research efforts in response to prevailing model-
centric research, which generally focuses heavily
on architectural design and novelty (Kaushik and
Lipton, 2018), at the expense of work on the dataside (Sambasivan et al., 2021; Rogers, 2021).
We address the domain of story understanding
as a particularly core (and data-intensive) capacity
underlying language use (McClelland et al., 2020),
thought to require constructing and manipulating
situation models of entities and their relations as
they unfold throughout discourse (Zwaan, 2016;
Tamari et al., 2020). Procedural text datasets (Dalvi
et al., 2018; Tandon et al., 2020) are closely related
in that they provide detailed annotation of entities
and state changes, and have mostly focused on
relatively small and static benchmarks using human
collected data. Overall, recent works identify a
lack of benchmarks which systematically probe the
situation models constructed by systems processing
discourse-level texts (Sugawara et al., 2021).
The bAbI benchmark (Weston et al., 2016)
is seen as highly relevant in terms of objective
(targeting situation modelling) (Dunietz et al.,
2020), but has been viewed critically due
to its constrained nature and exploitable
artifacts (Kaushik and Lipton, 2018). Our
work focuses on improving the evaluation in bAbI
through compositional generalization, widely
used across NLP to more rigorously probe model
robustness (Finegan-Dollak et al., 2018; Keysers
et al., 2020; Gontier et al., 2020; Yanaka et al.,
2021), but to our knowledge still not applied to
story understanding or bAbI.
3 Synthetic Dynamic Benchmarking on
bAbI
3.1 Dyna-bAbI
What makes a synthetic benchmark dynamic ? We
think of a dynamic synthetic benchmark as a
highly controllable task generator, enabling rapid
exploration of interesting areas of a task space.
The original bAbI 1.0 simulator code does not
readily facilitate such exploration; each of the
bAbI 1.0 tasks is generated by a hard-coded script
which does not enable parametric manipulation
of interesting generation aspects such as question
difficulty or compositionality.
Accordingly, we developed Dyna-bAbI, a
Python-based version of the original simulator.
Dyna-bAbI facilitates control of task generation
through a configuration file, effectively abstracting
away much of the underlying implementation
complexity. The configuration file allows users
to specify high-level task parameters such as the
set of target concepts, passage length, and filtering
conditions to mine for harder/rarer examples. We
also modularized the code to facilitate adding new
questions and other concepts more easily.
In this next sections we describe the underlying
structure of the bAbI 1.0 tasks, and how we
combine them using Dyna-bAbI to create more
complex compositional generalization tasks.
3.2 bAbI task structure
A task in bAbI 1.0 is a set of train, validation
and test splits. Each split is a set of instances,
where an instance is a tuple ( p, q, a )=(passage,
question, answer ). Passages are generated using
a micro-world simulator by sampling a valid
sequence of world events from an event set Eand
generating a linguistic description of them. By
default, linguistic descriptions are generated by a
simple sentence-level mapping from an event to a
natural language sentence. For example, the event
move(john,park) could be translated to “John
moved to the park.”
Some tasks also incorporate more complex
linguistic mappings between events and
sentences, such as co-reference: the
event sequence ( move(john,park) ,
move(john,kitchen) ) could be mapped to
“John moved to the park. Then he went to the
kitchen.” We denote the set of possible linguistic
mappings by L.
Finally, a valid question-answer pair ( q,a) over p
is sampled from question set Q. In bAbI, each
split is generated using some particular subset
of all possible events, linguistic constructs and
questions (§3.3); for a given split we can then
define its concept set ,C=E ∪ L ∪ Q . Instances
also include a set of supporting facts ( f), or the
relevant lines from which acan be derived (see
Fig. 1). The support composition ( fc) is the set of
events and linguistic constructs contained in f(see
examples in §4.2.1), and is useful for characterizing
compositionality performance (§3.4).
3.3 Original bAbI 1.0 tasks
Our focus here is on a particular subset of 12 bAbI
1.0 tasks evaluating aspects of story understanding.
Table 1 summarizes them, detailing E,L,Qfor
each task. For L, we list only complex constructs
beyond the default event-sentence mapping (which
is present in every task). See appendix A.1 for
additional details on task construction. Not all
of the story understanding tasks are considered.
For example, tasks 14 and 20 address timeTaskEvents
(E)Linguistic
Constructs
(L)Questions
(Q)Avg. sents. &
supp. facts
per story
1 MOVE - where-P 6, 1
2MOVE,
POSS- where-O 15.52, 2
3MOVE,
POSS- where-was-O 51.9, 3
5MOVE,
GIVE,
POSS- give-qs 20.1, 1
6 MOVE - yes-no 6.27, 1
7MOVE,
GIVE,
POSS- counting 8.67, 2.33
8MOVE,
POSS- list 8.75, 1.94
9 MOVE NEGATE yes-no 6, 1
10 MOVE INDEF yes-no 6, 1
11 MOVE CO-REF where-P 6, 2
12 MOVE CONJ. where-P 6, 1
13 MOVECONJ.,
CO-REFwhere-P 6, 2
Table 1: Subset of 12 bAbI 1.0 tasks considered
here. Each task is characterized by the possible events,
linguistic constructs and questions that can occur in
instances. POSS (possession) is short for GRAB and
DROP events. Statistics based on training sets. A large
space of task configurations remains unexplored.
reasoning and agent motivations, and we leave their
integration for future work.
3.4 Compositional generalization on bAbI
As can be seen in Table 1, many possible task
configurations are not covered by the original
benchmark; which directions should be explored?
We focus on out-of-distribution (OOD) robustness,
which is increasingly seen as a vital evaluation
criteria across AI/NLP research (Shanahan
et al., 2020; Hendrycks et al., 2020). We target
compositional generalization , a particularly
important class of OOD problems (Lake et al.,
2017; Lake and Baroni, 2018). Compositional
generalization refers to the ability to systematically
generalize to test inputs containing novel
combinations of more basic elements seen at
training time (Partee et al., 1995). For example,
a model that has learned basic object tracking
and co-reference separately (tasks 2 and 11,
see Fig. 1c) could be expected to solve tasks
requiring a mixture of both object tracking and
co-reference (Fig. 1c, line 10 question on right
side). Compositional tasks are absent from bAbI
1.0 which features only IID test sets (independent,
identically distributed).4
Compositional task generation. To create
compositional generalization tasks in practice, we
create training (and validation) splits composed of
Msub-tasks with concept sets/braceleftbig
Ci
train/bracerightbigM
i=1, and a
test set Ctestsuch that Ctest̸=Ci
train∀i, butCtest=/uniontextM
i=1Ci
train. In other words, each training sub-
task can be thought of focusing on a particular
subset of test concepts, so models are exposed to
all test concepts at training time, but not to all
combinations of them (Yanaka et al., 2021).
Task difficulty. We hypothesize that support
composition ( fc) and supporting fact set size
(|f|) are main factors underlying a particular
instance’s difficulty, and especially novel
support compositions not seen at training time.
Additionally, the difference between train and test
splits results in potentially harder distractors, as
test-time distractors appear in novel contexts.
Our notions of concepts and support composition
resemble atoms and compounds in DBCA, a
related study on compositionality (Keysers et al.,
2020). While DBCA enables automatic creation
of compositional train and test splits, we opt
here for a more human-interpretable representation
that allows more precise manual control of the
combinations of concepts a model is exposed to
at train and test time.
Quality comparison vs. bAbI 1.0 tasks.
Intuitively, good synthetic datasets help drive the
development of better modelling approaches. Our
new compositional tasks might be harder than
bAbI 1.0, but how do we know whether they are
a more useful target? To provide a preliminary
answer to this question, we adopt the notion of
concurrence as a quality measure (Liu et al., 2021).
Two benchmarks are said to have high concurrence
when they rank a set of modelling approaches
similarly. Concurrence offers a way to formalize
the intuition above, as high concurrence between a
synthetic and natural language benchmark suggests
that the synthetic benchmark could have driven
similar innovations. We follow the setup of Liu
et al. (2021) using SQuAD for the natural language
benchmark.5Notably, bAbI 1.0 achieved very
low concurrence with SQuAD; for example, pre-
4Weston et al. (2016) noted that transfer learning was an
important goal out of the original work’s scope.
5Liu et al. (2021) consider a set of 20 modelling approaches
used on SQuAD, including 10 pre-trained and 10 non-pre-
trained methods.Split TypeAvg.
lengthSizeAvg. supp.
fact set size
concat(T2) Train 10.76 18,000 2
concat(T7) Train 13.5 63,000 1.68
inject(T7) Train 23.25 190,158 1.42
diverse(T7) Train 20 17,000 2.17
concat(T12) Train 10.8 108,000 1.42
inject(T12) Train 15.97 368,831 1.28
diverse(T12) Train 20 24,772 2.45
mix(T2) Test 13.25 1,000 2.05
mix(T7) Test 20 3,000 2.50
mix(T12) Test 20 6,000 3.70
Table 2: Splits used for our experiments. All except the
original data ( concat ) are created with Dyna-bAbI.
training consistently yields large gains on SQuAD,
but on bAbI 1.0, both pre-trained and non-pre-
trained models achieve perfect performance on
many tasks. The low concurrence thus suggests
that bAbI 1.0 may be an unreliable benchmark for
model development, and highlights the importance
of improving its quality.
4 Experiments
With the controllable task generation afforded by
Dyna-bAbI, we can now create datasets probing
deeper story understanding capabilities of models.
We present two main experiments targeting the
following questions:
•Exp. 1: (q1.a) What role does model
architecture play in the capacity for
compositional generalization? (q1.b) What is
the concurrence of our compositional tasks
with real datasets, compared with bAbI 1.0?
•Exp. 2: (q2) How do training data
quantity and diversity affect compositional
generalization?
Data
For our experiments we created 4 kinds of
splits over three subsets of bAbI 1.0 tasks,
summarized in Table 2. We denote a subset
of tasks T, and consider T2={2,11},
T7={1,2,3,5,11,12,13}, and T12=
{1,2,3,5, ...,13}.
•concat splits are simply concatenations of the
official data for the tasks T. We considered
the larger version where each task consists of
9,000/1,000 training/development examples;
e.g., concat( T2)consists of 18,000 training
examples and 2,000 development examples.
•inject splits enrich the concat data as follows:
for each question in the original data, we
supplement it with all possible additional
questions of the specified types. In this work,
the supplement question types were where-P
andwhere-O (to provide location information
of objects and agents).
•diverse splits use rejection sampling to
generate more diverse samples, such that
the number of supporting facts per question
is roughly uniform across all sub-task
instances for a given question type. Without
rejection sampling, most generated questions
would be trivial (e.g., 1-2 supporting facts).
Compositionality is retained by holding out
certain combinations. In particular, at training
time, complex linguistic constructs (e.g., co-
reference) are only seen with MOVE events.
•mix are test splits generated using rejection
sampling like diverse , and consist of instances
which may feature elements from any of
the considered tasks. As a result, questions
inmix splits require novel/more complex
reasoning patterns compared to those seen
during training.
See appendix A.1 for examples and extended
details on task generation.
4.1 Exp. 1: Can training on bAbI 1.0
facilitate compositional generalization?
For this experiment, we compared models on T2
andT7, since they allow for a direct conversion to
an extractive QA format,6enabling us to use the
same concurrence framework of Liu et al. (2021).
Models. We considered 3 classes of models:
•Non-pre-trained specialized architectures for
bAbI 1.0 including EntNet (Henaff et al.,
2017) and STM (Le et al., 2020), the latter
being current SOTA on bAbI 1.07.
•Non-pretrained general-purpose QA methods,
such as BiDAF (Seo et al., 2017).
•General purpose pre-trained approaches
including RoBERTa (Liu et al., 2020) and T5
(base) (Raffel et al., 2020).
The last two categories are comprised of the
20 models evaluated in Liu et al. (2021), with
the addition of T5 to the last group. For
implementation details, see appendix A.2.
6Tasks 6-10 require generative QA, for answering yes-no ,
count andlistquestions.
7As of March 10, 2022.Results & Analysis
Experiment results are summarized in Table 3.
All models perform well in IID settings, but
performance drops considerably in OOD settings
Architecture alone is not a significant
compositionality driver (q1.a). The large
OOD performance gap between pre-trained and
non-pre-trained models indicates that pre-training
plays a much greater role than specialized
architectures for QA performance, adding to
similar findings in other NLP domains (Hendrycks
et al., 2020). These results raise questions about
special purpose relational reasoning architectures
that continue to be developed today: the poor OOD
performance suggests that such models may not
be fulfilling their intended design. Either way,
these results underscore the importance of rigorous
evaluation to verify that modelling motivations are
borne out in practice (Aina et al., 2019).
Compositionality increases concurrence (q1.b).
As can be seen in the Fig. 2 plots8, increasing
compositionality is correlated with increased
concurrence. In contrast to the original bAbI 1.0
tasks which exhibited virtually no correlation with
SQuAD, our compositional task mix(T7)exhibits
high concurrence of r= 0.92, τ= 0.78(Pearson
and Kendall correlation functions, resp.). These
results are comparable to other natural language as
well as purpose-built synthetic datasets considered
in Liu et al. (2021), which feature r, τin the
ranges [0.87,0.99]and[0.77,0.94], respectively.
Our results thus extend the findings of Liu et al.
(2021); they demonstrated the existence of high
concurrence synthetic benchmarks, we additionally
suggest a guiding principle for how to create them
(incorporate compositionality evaluation).
4.2 Exp. 2: enriching bAbI 1.0 training data
The results above suggest that the bAbI data in
their current form may not be rich enough to drive
compositional generalization.9In this experiment
we probe this question, enriching the training data
to better understand its impact on compositional
generalization. In particular, we investigate
two approaches to enriching the training data
while maintaining the compositionality evaluation,
corresponding to the inject and diverse splits.
8See appendix A.4 for full numeric results.
9An alternate hypothesis is that certain patterns may be too
hard for models to learn; we confirm this is not the case by
using the inoculation methodology of Liu et al. (2019), see
details in Appendix A.3.
Name Train Test Evaluation accuracy SQuAD Concurrence
EntNet STM BiDAF Roberta T5 ρ τ
2-task IID concat(T2) concat(T2) 98.95 99.85 100 100 99.85 [-0.35,0.08] [-0.35,-0.19]
2-task OOD concat(T2) mix(T2) 72.0 67.6 97.2 98.7 98.1 0.48 0.51
7-task IID concat(T7) concat(T7) 96.8 99.4 99.98 99.98 99.8 [-0.4,0.08] [-0.35,0.03]
7-task OOD concat(T7) mix(T7) 22.2 26.7 30.5 57.7 49.57 0.92 0.78
12-task IID concat(T12) concat(T12) 96.19 99.34 – – 99.54 – –
12-task OOD concat(T12) mix(T12) 31.97 35.65 – – 67.4 – –
Table 3: Experiment 1. OOD evaluation exposes large differences between pre-trained and non-pre-trained models,
and also achieves high concurrence with the SQuAD benchmark. We report [min,max] concurrence for bAbI 1.0.
70 80 90
SQuAD EM80859095100bAbI task 2 EMr = 0.08
 = -0.19
 
70 80 90
SQuAD EM304050607080mix(T7) EMr = 0.92
 = 0.78
 
non-pretrained models
pretrained models
Figure 2: SQuAD concurrence plots for bAbI
1.0 task 2 (left; reproduced from Liu et al. (2021)
with permission) and mix(T7)(right). bAbI task
2 has the highest concurrence of all T7tasks, yet
exhibits virtually no correlation with SQuAD. mix(T7)
exhibits high concurrence, highlighting the relevance of
compositional evaluation.
Notably, Exp. 2 can be seen as a first iteration
of the dynamic benchmarking loop depicted in
Fig. 1: based on the error analysis of Exp. 1,
we leverage Dyna-bAbI for targeted creation of
new tasks, which allow us to systematically test
our hypotheses.
In this experiment we focus on pre-trained
models, as they significantly out-performed non-
pre-trained methods. We use T5 as a representative
since its generative abilities make it straightforward
to apply also to T12(unlike the extractive methods
which were applicable only to T7).
Injecting supplementary questions. One
hypothesis for the poor performance of models on
themixsplits could be that the original bAbI tasks
do not provide enough supervision for models to
learn the basic event semantics. For example, tasks
5 and 7 are the only bAbI 1.0 tasks featuring the
GIVE event, and neither includes any questions
about the location of participants. However, test-
time compositional questions may require models
to infer that the participants in a GIVE eventTrain TestEvaluation accuracy /
# supporting facts
1 2 3+ Total
inject(T7) concat(T7) 99.83 100 93.35 99.05
inject(T7) mix(T7) 89.82 80.55 64.16 71.57
diverse(T7) concat(T7) 99.58 100 78.36 96.94
diverse(T7) mix(T7) 100 98.44 93.84 95.8
inject(T12) concat(T12) 99.94 99.97 91.91 99.35
inject(T12) mix(T12) 92.45 85.29 67.67 72.2
diverse(T12) concat(T12) 99.75 98.73 76.81 97.73
diverse(T12) mix(T12) 99.01 96.29 81.24 84.82
Table 4: Enriching the training data. Injecting
knowledge to the original bAbI tasks doesn’t
substantially improve compositionality. Sampling more
structurally diverse instances yields more significant
improvements, though is still limited, especially for
more complex compositions.
share the same location (e.g., line 10 question in
Fig. 1c). Error analysis shows that such implicit
inferences are indeed challenging for models
trained on the concat splits (see details in appendix
A.5). Perhaps the inject splits supplementing
the original tasks with relevant information will
improve compositionality performance? Table 4
displays the result of this experiment; performance
onmixis improved only marginally, despite a 3-
fold increase in training data (Table 2).
Sampling structurally diverse training data. As
shown in Table 2, though inject splits significantly
increase dataset size, their diversity remains
low: most questions require only one or two
supporting facts. Therefore, we next enrich training
data through sampling more structurally diverse
samples. This method is known to improve data
efficiency for both compositional generalization
as well as IID settings (Oren et al., 2021). As
can be seen in Table 4, training on the diverse
splits yields a more significant improvement;
similar to the findings of Oren et al. (2021),
sampling more diverse training data leads to greater
Accuracy on where-O
questions over all
instances with f_c=
{GIVE,MOVE}, n<=2Figure 3: Error analysis on mix(T12)for T5 trained on diverse( T12)data. The sub-plots break down performance
on questions requiring {≤2,3,≥4}supporting facts. For each sub-plot, the left side of each row corresponds to
a particular support composition ( fc), and the right side displays accuracy over inputs sharing fc, across various
question types. Performance on fcseen at training time (blue frames) is generally high, but overall generalization is
not systematic, as evidenced by high variance across different fc, especially for higher complexity ( n= 3, n≥4)
and more novel compositions.
generalization as well as much improved data
efficiency.10However, as the error analysis of the
next section shows, performance on compositional
generalization is still fundamentally limited.
4.2.1 Discussion and error analysis
Figure 3 breaks down the performance of T5
onmix(T12)after training on diverse( T12). The
heatmaps plot performance across various support
compositions ( fc) occurring in the test data, sub-
divided by the number of required supporting
facts nper question. Performance on support
compositions seen at training time (blue frames)
is generally high, indicating the importance of
training pattern diversity for better generalization.
The plots indicate that T5 shows some ability to
generalize to new support compositions, especially
for lower n. Furthermore, certain question types
appear to be more learned more robustly; for
listand count questions, performance remains
relatively high even for larger nand across novel
fc. We hypothesize that such questions may be
easier as simple counting rules suffice to reach
an answer, and these are “close to the surface”;
unlike other events that may implicitly convey
10The relatively low performance of diverse trained models in
the “3+” column for concat splits is predominantly due to
length discrepancies at train and test time: concat contains
some very long stories which are challenging for the model
trained on the uniform length and shorter diverse stories.
Figure 4: Example mix(T12)instance demonstrating
the question phrasing sensitivity failure mode in T5: the
model correctly answers the question in where-P form
(line 22), and incorrectly in yes-no form (line 21).
information, in our stories, changes of possession
are always explicit in the text.
In general however, the plots indicate that T5 is
far from robust compositional generalization:
Performance deteriorates with increased
complexity. Performance is near perfect for simple
compositions ( n≤2) but deteriorates significantly
for more complex cases ( n≥3).
Question phrasing sensitivity. The discrepancy
between the relatively high performance on
where-P questions compared with very low
performance on yes-no questions suggests that
models are learning highly question-dependent
story representations. E.g., if a model answers
ycorrectly to some “Where is p?” question, we
would expect it to answer “yes” correctly for the
same question in yes-no format, “Is paty?”. Figure
4 shows a characteristic example: T5 answers
correctly in the where-P format, but incorrectly
answers “maybe” for the yes-no format, likely
thrown off by the distractor indefinite phrase in
sentence 3.
We present further empirical support for
question phrasing sensitivity in appendix A.6.
These results suggest models may be learning
shortcuts that work well for the story/question pairs
seen at training time, but not more robust rules
that also generalize to novel test instances. Such
highly question-dependent story representation
stands in contrast to more human-like narrative
comprehension, which is thought to involve the
construction of situation models , or structured
representations of entities and their relations as
depicted by the text. Situation models are less
dependent on a-priori knowledge of a question
(or its phrasing), and are often generated on-line
during the course of comprehension (Graesser et al.,
1994).
Performance below chance for certain question
types. The heatmaps expose a particularly
challenging class of yes-no questions involving
disjunctions over indefinites (center and right plots,
bottom right); accuracy for such questions is close
to zero. See appendix A.7 for an example instance.
5 Future work & conclusions
Our work opens up multiple new directions
for future research. Our new tool, Dyna-bAbI
is readily extendable for systematic probing of
more diverse linguistic phenomena. A beneficial
first step could include integration of additional
bAbI tasks. That said, our experience suggests
that the design of truly scalable synthetic and
dynamic benchmarks poses significant theoretical
and engineering challenges, warranting deeper
research on their own right.
Our results raise new questions about the
viability of learning robust situation models using
standard question-answering training, and ourdatasets present new challenges for future efforts.
Additionally, Dyna-bAbI can naturally
complement parallel work probing the the situation
representations constructed by neural language
models (Li et al., 2021) by facilitating tailored data
generation for specific questions, thus broadening
and deepening the scope of possible research.
In conclusion, we introduced Dyna-bAbI, a
new framework for highly controllable bAbI task
generation. We used it to create compositional
generalization datasets providing new modelling
challenges for state-of-the-art neural language
models. More broadly, our results underscore
the importance in development of benchmarks
themselves, beyond only the models solving them.
Broader Impact
While large, neural language models are
increasingly seen as foundations for a wide array
of NLP tasks, we still lack a clear understanding
of their capabilities and failure modes. Our work
joins many recent efforts using carefully controlled
synthetic tasks to more rigorously evaluate models’
language comprehension abilities.
While our choice of a synthetic language
benchmark allows more precise control over
evaluation, the synthetic nature of the data is an
obvious limitation. Similar to the original bAbI
benchmark, our tasks are not a substitute for
real natural language datasets, but should rather
complement them. Even if a method works well
on our data, it should be shown to perform well
on real data as well. Rather, our tasks are better
thought of as comprehension “unit-tests”, where
poor performance on our tasks serves as a warning
sign suggesting the model may exhibit limited
systematicity and robustness on more difficult,
naturalistic inputs.
Acknowledgements
We thank the Aristo team at the Allen Institute
for AI for valuable support and feedback.
Ronen Tamari was supported by the Center for
Interdisciplinary Data-science Research at HUJI.
This work was supported by the European Research
Council (ERC) under the European Union’s
Horizon 2020 research and innovation programme
(grant no. 852686, SIAM, Shahaf). Part of
this research is also supported by the European
Research Council, ERC-StG grant no. 677352
(Tsarfaty), which we gratefully acknowledge.
References
Laura Aina, Carina Silberer, Ionut-Teodor Sorodoc,
Matthijs Westera, and Gemma Boleda. 2019. What
do entity-centric models learn? insights from entity
linking in multi-party dialogue. In Proceedings
of the 2019 Conference of the North American
Chapter of the Association for Computational
Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers) , pages 3772–
3783, Minneapolis, Minnesota. Association for
Computational Linguistics.
Pratyay Banerjee, Chitta Baral, Man Luo, Arindam
Mitra, Kuntal Pal, Tran C. Son, and Neeraj Varshney.
2020. Can transformers reason about effects
of actions? Computing Research Repository ,
arXiv:2012.09938.
Andrea Banino, Adrià Puigdomènech Badia, Raphael
Köster, Martin J. Chadwick, Vinicius Zambaldi,
Demis Hassabis, Caswell Barry, Matthew Botvinick,
Dharshan Kumaran, and Charles Blundell. 2020.
Memo: A deep network for flexible combination
of episodic memories. In International Conference
on Learning Representations .
Andrea Banino, Jan Balaguer, and Charles Blundell.
2021. Pondernet: Learning to ponder. In 8th
ICML Workshop on Automated Machine Learning
(AutoML) .
Gregor Betz, Christian V oigt, and Kyle Richardson.
2021. Critical thinking for language models.
Proceedings of IWCS .
Lukas Biewald. 2020. Experiment tracking with
weights and biases. Software available from
wandb.com.
Samuel R. Bowman and George Dahl. 2021. What
will it take to fix benchmarking in natural language
understanding? In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 4843–4855, Online.
Association for Computational Linguistics.
Peter Clark, Oyvind Tafjord, and Kyle Richardson.
2020. Transformers as soft reasoners over language.
InProceedings of the Twenty-Ninth International
Joint Conference on Artificial Intelligence, IJCAI-20 ,
pages 3882–3890. International Joint Conferences on
Artificial Intelligence Organization. Main track.
Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau
Yih, and Peter Clark. 2018. Tracking state changes in
procedural text: a challenge dataset and models for
process paragraph comprehension. In Proceedings of
the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long
Papers) , pages 1595–1604, New Orleans, Louisiana.
Association for Computational Linguistics.Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training
of deep bidirectional transformers for language
understanding. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short
Papers) , pages 4171–4186, Minneapolis, Minnesota.
Association for Computational Linguistics.
Jesse Dunietz, Greg Burnham, Akash Bharadwaj, Owen
Rambow, Jennifer Chu-Carroll, and Dave Ferrucci.
2020. To test machine comprehension, start by
defining comprehension. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 7839–7859, Online. Association
for Computational Linguistics.
William Falcon et al. 2019. Pytorch lightning. GitHub.
Note: https://github.com/PyTorchLightning/pytorch-
lightning , 3.
Catherine Finegan-Dollak, Jonathan K. Kummerfeld,
Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui
Zhang, and Dragomir Radev. 2018. Improving text-
to-SQL evaluation methodology. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 351–360, Melbourne, Australia. Association
for Computational Linguistics.
Nicolas Gontier, Koustuv Sinha, Siva Reddy, and
Christopher Pal. 2020. Measuring systematic
generalization in neural proof generation with
transformers. In Advances in Neural Information
Processing Systems 33 . Curran Associates, Inc.
Arthur C. Graesser, Murray Singer, and Tom Trabasso.
1994. Constructing Inferences During Narrative Text
Comprehension. Psychological Review , 101(3):371–
395.
Suchin Gururangan, Swabha Swayamdipta, Omer Levy,
Roy Schwartz, Samuel Bowman, and Noah A.
Smith. 2018. Annotation artifacts in natural
language inference data. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers) ,
pages 107–112, New Orleans, Louisiana. Association
for Computational Linguistics.
Mikael Henaff, Jason Weston, Arthur Szlam, Antoine
Bordes, and Yann LeCun. 2017. Tracking the world
state with recurrent entity networks. 5th International
Conference on Learning Representations, ICLR 2017
; Conference date: 24-04-2017 Through 26-04-2017.
Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam
Dziedzic, Rishabh Krishnan, and Dawn Song.
2020. Pretrained transformers improve out-of-
distribution robustness. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 2744–2751, Online. Association
for Computational Linguistics.
Divyansh Kaushik and Zachary C. Lipton. 2018. How
much reading does reading comprehension require?
a critical investigation of popular benchmarks. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing , pages
5010–5015, Brussels, Belgium. Association for
Computational Linguistics.
Daniel Keysers, Nathanael Schärli, Nathan Scales,
Hylke Buisman, Daniel Furrer, Sergii Kashubin,
Nikola Momchev, Danila Sinopalnikov, Lukasz
Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao
Wang, Marc van Zee, and Olivier Bousquet.
2020. Measuring compositional generalization:
A comprehensive method on realistic data.
In International Conference on Learning
Representations .
Tushar Khot, Kyle Richardson, Daniel Khashabi,
and Ashish Sabharwal. 2021. Learning to Solve
Complex Tasks by Talking to Agents. arXiv preprint
arXiv:2110.08542 .
Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh
Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie
Vidgen, Grusha Prasad, Amanpreet Singh, Pratik
Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel,
Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit
Bansal, Christopher Potts, and Adina Williams. 2021.
Dynabench: Rethinking benchmarking in nlp.
Diederik P. Kingma and Jimmy Ba. 2017. Adam: A
method for stochastic optimization.
Brenden Lake and Marco Baroni. 2018. Generalization
without systematicity: On the compositional skills
of sequence-to-sequence recurrent networks. In
International conference on machine learning , pages
2873–2882. PMLR.
Brenden M Lake, Tomer D Ullman, Joshua B
Tenenbaum, and Samuel J Gershman. 2017. Building
machines that learn and think like people. Behavioral
and brain sciences , 40.
Hung Le, Truyen Tran, and Svetha Venkatesh. 2020.
Self-attentive associative memory. In International
Conference on Machine Learning , pages 5682–5691.
PMLR.
Belinda Z. Li, Maxwell Nye, and Jacob Andreas.
2021. Implicit representations of meaning in
neural language models. In Proceedings of
the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 1813–1827, Online.
Association for Computational Linguistics.
Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei
Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin
Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang,
Rahul Agrawal, Edward Cui, Sining Wei, Taroon
Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu,
Shuguang Liu, Fan Yang, Daniel Campos, RanganMajumder, and Ming Zhou. 2020. XGLUE: A
new benchmark datasetfor cross-lingual pre-training,
understanding and generation. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 6008–6018,
Online. Association for Computational Linguistics.
Nelson F. Liu, Tony Lee, Robin Jia, and Percy
Liang. 2021. Can small and synthetic benchmarks
drive modeling innovation? a retrospective
study of question answering modeling approaches.
Computing Research Repository , arXiv:2102.01065.
Nelson F. Liu, Roy Schwartz, and Noah A. Smith. 2019.
Inoculation by fine-tuning: A method for analyzing
challenge datasets. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short
Papers) , pages 2171–2179, Minneapolis, Minnesota.
Association for Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2020.
Ro{bert}a: A robustly optimized {bert} pretraining
approach.
Reginald Long, Panupong Pasupat, and Percy Liang.
2016. Simpler context-dependent logical forms
via model projections. In Proceedings of the
54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 1456–1465, Berlin, Germany. Association for
Computational Linguistics.
James L. McClelland, Felix Hill, Maja Rudolph, Jason
Baldridge, and Hinrich Schütze. 2020. Placing
language in an integrated understanding system: Next
steps toward human-level performance in neural
language models. Proceedings of the National
Academy of Sciences , arXiv:1707(Xx):201910416.
Inbar Oren, Jonathan Herzig, and Jonathan Berant.
2021. Finding needles in a haystack: Sampling
structurally-diverse training sets from synthetic data
for compositional generalization.
Barbara Partee et al. 1995. Lexical semantics and
compositionality. An invitation to cognitive science:
Language , 1:311–360.
Christopher Potts, Zhengxuan Wu, Atticus Geiger,
and Douwe Kiela. 2021. DynaSent: A dynamic
benchmark for sentiment analysis. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 2388–2404, Online.
Association for Computational Linguistics.
Linlu Qiu, Hexiang Hu, Bowen Zhang, Peter Shaw, and
Fei Sha. 2021. Systematic generalization on gscan:
What is nearly solved and what is next? Computing
Research Repository , arXiv:2109.12243.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research ,
21(140):1–67.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions
for machine comprehension of text. In Proceedings
of the 2016 Conference on Empirical Methods
in Natural Language Processing , pages 2383–
2392, Austin, Texas. Association for Computational
Linguistics.
Kyle Richardson, Hai Hu, Lawrence Moss, and Ashish
Sabharwal. 2020. Probing natural language inference
models through semantic fragments. In Proceedings
of AAAI .
Kyle Richardson and Ashish Sabharwal. 2022. Pushing
the limits of rule reasoning in transformers through
natural language satisfiability. Proceedings of AAAI .
Anna Rogers. 2021. Changing the world by changing
the data. In Proceedings of the 59th Annual Meeting
of the Association for Computational Linguistics
and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long
Papers) , pages 2182–2194, Online. Association for
Computational Linguistics.
Laura Ruis, Jacob Andreas, Marco Baroni, Diane
Bouchacourt, and Brenden M Lake. 2020. A
benchmark for systematic generalization in grounded
language understanding. In Advances in Neural
Information Processing Systems , volume 33, pages
19861–19872. Curran Associates, Inc.
Nithya Sambasivan, Shivani Kapania, Hannah Highfill,
Diana Akrong, Praveen Paritosh, and Lora M Aroyo.
2021. “everyone wants to do the model work, not
the data work”: Data cascades in high-stakes ai. In
Proceedings of the 2021 CHI Conference on Human
Factors in Computing Systems , CHI ’21, New York,
NY , USA. Association for Computing Machinery.
Imanol Schlag, Tsendsuren Munkhdalai, and Jürgen
Schmidhuber. 2021. Learning associative inference
using fast weight memory. In International
Conference on Learning Representations .
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In International
Conference on Learning Representations .
Murray Shanahan, Matthew Crosby, Benjamin Beyret,
and Lucy Cheke. 2020. Artificial intelligence and
the common sense of animals. Trends in Cognitive
Sciences , 24(11):862–872.
Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle
Pineau, and William L. Hamilton. 2019. CLUTRR:
A diagnostic benchmark for inductive reasoning from
text. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processing
and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP) ,
pages 4506–4515, Hong Kong, China. Association
for Computational Linguistics.
Saku Sugawara, Pontus Stenetorp, and Akiko
Aizawa. 2021. Benchmarking machine reading
comprehension: A psychological perspective. In
Proceedings of the 16th Conference of the European
Chapter of the Association for Computational
Linguistics: Main Volume , pages 1592–1612, Online.
Association for Computational Linguistics.
Ronen Tamari, Chen Shani, Tom Hope, Miriam
R L Petruck, Omri Abend, and Dafna Shahaf.
2020. Language (re)modelling: Towards embodied
language understanding. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 6268–6281, Online. Association
for Computational Linguistics.
Niket Tandon, Keisuke Sakaguchi, Bhavana Dalvi,
Dheeraj Rajagopal, Peter Clark, Michal Guerquin,
Kyle Richardson, and Eduard Hovy. 2020. A dataset
for tracking entities in open domain procedural
text. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP) , pages 6408–6417, Online. Association for
Computational Linguistics.
Alex Wang, Yada Pruksachatkun, Nikita Nangia,
Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel Bowman. 2019. Superglue: A
stickier benchmark for general-purpose language
understanding systems. In Advances in Neural
Information Processing Systems , volume 32. Curran
Associates, Inc.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. GLUE:
A multi-task benchmark and analysis platform for
natural language understanding. In Proceedings
of the 2018 EMNLP Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP ,
pages 353–355, Brussels, Belgium. Association for
Computational Linguistics.
Jason Weston, Antoine Bordes, Sumit Chopra, and
Tomás Mikolov. 2016. Towards ai-complete
question answering: A set of prerequisite toy
tasks. In 4th International Conference on Learning
Representations, ICLR 2016, San Juan, Puerto Rico,
May 2-4, 2016, Conference Track Proceedings .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,
Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven
Le Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander Rush. 2020. Transformers:
State-of-the-art natural language processing. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online. Association
for Computational Linguistics.
Zhengxuan Wu, Elisa Kreiss, Desmond C. Ong, and
Christopher Potts. 2021. ReaSCAN: Compositional
reasoning in language grounding. NeurIPS 2021
Datasets and Benchmarks Track .
Hitomi Yanaka, Koji Mineshima, and Kentaro Inui.
2021. SyGNS: A systematic generalization testbed
based on natural language semantics. In Findings of
the Association for Computational Linguistics: ACL-
IJCNLP 2021 , pages 103–119, Online. Association
for Computational Linguistics.
Rolf A. Zwaan. 2016. Situation models, mental
simulations, and abstract concepts in discourse
comprehension. Psychonomic Bulletin and Review ,
23(4):1028–1034.
A Appendix
A.1 Extended task construction details
This section provides further details of the training
and test splits used for our experiments.
Table 5 enumerates the basic “building blocks”,
or concepts underlying the tasks, as presented in
§3.2.
Tables 6 and 7 detail the concept sets for each of
the sub-tasks comprising the training and test sets,
for the T2,T7andT12groups of tasks.
As can be seen from the tables, the main sources
of compositionality are:
•Following the bAbI 1.0 task structure, at
training time, all of the more complex
linguistic constructs are seen only with
MOVE events (and none of the other event
types).
•Similarly, at training time, yes-no questions
are always seen only with MOVE events (and
none of the other event types), and with the
INDEF or NEGATE linguistic constructs (but
not others, such as COREF).
•where-was-O questions are never seen in
stories with GIVE events.
Language templates. For our new generated tasks
we use the same language templates as used in the
original bAbI 1.0 benchmark (e.g., the same entity
names, verb synonyms). The only modification
to the language generation engine was that we
completely omit the use of “there”; in the original
benchmark, “there” could be used in confusing
contexts, as shown in Fig. 5.
A.1.1 Example instances
Figure 6 shows examples from each of the 4
types of splits used in our experiments. The
concat instance is from the original bAbI 1.0 task
5. The inject data contains the same passages
asconcat , but adds supplementary questions on
agent and object locations. diverse instances
1 Mary journeyed to the bathroom.
2 Sandra went to the garden.
3 Daniel went back to the garden.
4 Daniel went to the office.
5 Sandra grabbed the milk there.
6 Sandra put down the milk there.
7 Where is the milk? garden 6 2
Figure 5: Example from original bAbI 1.0 benchmark
with confusing usage of “there”. In Dyna-bAbI we do
not include “there”, to avoid this confusion.contain more diverse support compositions ( fc),
but certain combinations are held out. In particular,
diverse instances only feature non-default linguistic
mappings with MOVE events, never with POSS
(GRAB or DROP) or GIVE. In the mixinstances,
all combinations of support compositions are
possible, as shown in the example which features
possession (POSS) events along with co-reference.
A.1.2 Long instances in the bAbI 1.0 tasks
For the T5 experiments, we used a slightly
modified version of the bAbI 1.0 tasks, where
we trimmed all training and validation examples
that didn’t fit into the 512-token input window.
This resulted in trimming 1,585 training instances
and 175 validation instances from T7andT12
(common to both sets). These data points are
not consequential as our analysis focuses on the
effects of compositionality and not story length;
all instances in diverse andmixare substantially
shorter than the 512-token maximum input window
size.
A.2 Implementation details
T5. We use the publicly available HuggingFace
pre-trained T5-base implementation (Wolf et al.,
2020) which has 220M parameters. We similarly
use the HuggingFace tokenization pipeline. We
fine-tune T5 for 12 epochs on our bAbI data, using
the Adam optimizer (Kingma and Ba, 2017), an
initial learning rate of 5∗10−5and training batch
size of 8.
STM. We used the official STM implementation11,
with the only change being a batch size of 32
instead of 128, due to technical constraints.
EntNet. We re-implemented the model in PyTorch,
similarly using a batch-size of 32. Following the
official Lua reference implementation12, we used
20 memory units each with dimension 100. We
used the SGD optimizer.
For both the EntNet and STM, we trained models
for 200 epochs, and took the best of 10 tries,
following Henaff et al. (2017).
For the 20-model concurrence benchmark, refer
to Liu et al. (2021) for model details, as we used
the same experimental setup.
11https://github.com/thaihungle/SAM
12https://github.com/facebookarchive/
MemNN/tree/master/EntNet-babi
Events Template Example Notes
MOVE P {moved} to the L. John traveled to the park.
GRAB P {grabbed} the O. Mary picked up the apple.
DROP P {dropped} the O. Daniel dropped the milk.
GIVE P1 {gave} P2 the O. John handed Mary the apple.
Linguistic
Constructs
COREFP (MOVE|GRAB|DROP)
Following that, {he}
(MOVE|GRAB|DROP).John went to the garden.
Following that, he moved to the storeCo-reference
CONJ P1 and P2 {moved} to the L1. Jeff and Fred went to the cinema. Conjunction
COMPOUNDP1 and P2 {moved} to the L1.
Then they {moved} to the L2.Jeff and Fred went to the cinema.
Then they traveled to the school.Compound co-reference
NEGATE P is not at the L. Julie is not in the park. Negation
INDEF P is either at the L1 or the L2. John is either in the park or the school. Indefinite expression
Questions
where-P Where is P? Where is John?
where-O Where is the O? Where is the football?
where-was-O Where was the O before the L? Where was the football before the hallway?
yes-no Is P at the L? Is John at the park?
list What is P carrying? What is John carrying?
counting How many objects is P carrying? How many objects is John carrying?
give-qsWho gave the O to P2?
Who gave the O?
Who received the O?
Who did P1 give the P2 to?
What did P1 give to P2?Who gave the football to John?
...Constitutes multiple
question types over
GIVE events.
Table 5: Details of the events, linguistic constructs and questions constituting the bAbI tasks covered in this work.
Words in {brackets} are drawn from a small set of synonyms.
Figure 6: Example instances from each of the 4 types of splits used in our experiments.
Events Linguistic Constructs Questions
Sub-task Type Move Grab Drop Give Co-reference Conjunction Compound co-ref. where-P where-O where-was-O give
1 Train /enc-33 /enc-33
2 Train /enc-33 /enc-33 /enc-33 I/D/enc-33
3 Train /enc-33 /enc-33 /enc-33 I I /enc-33
5 Train /enc-33 /enc-33 /enc-33 /enc-33 I/D I/D /enc-33
11 Train /enc-33 /enc-33 /enc-33
12 Train /enc-33 /enc-33 /enc-33
13 Train /enc-33 /enc-33 /enc-33
mix(T2) Test /enc-33 /enc-33 /enc-33 /enc-33 /enc-33
mix(T7) Test /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33
Table 6: Concept sets for the T2andT7sub-set of the original bAbI tasks, and the new tasks generated with
Dyna-bAbI. Train sub-task numbering follows the original bAbI numbering. The inject anddiverse tasks inherit the
same concept set from the original tasks, and additionally “I”, “D” denote question types included only in the inject
ordiverse tasks, respectively. “I/D” denotes question types included in both.
Events Linguistic Constructs Questions
Task Type Move Grab Drop Give Co-reference Conjunction Compound co-ref. Negation Indefinite where-P where-O where-was-O yes-no counting list give
1 Train /enc-33 /enc-33
2 Train /enc-33 /enc-33 /enc-33 I/D/enc-33
3 Train /enc-33 /enc-33 /enc-33 I I /enc-33
5 Train /enc-33 /enc-33 /enc-33 /enc-33 I/D I/D /enc-33
6 Train /enc-33 I/D /enc-33
7 Train /enc-33 /enc-33 /enc-33 /enc-33 I I /enc-33
8 Train /enc-33 /enc-33 /enc-33 I I /enc-33
9 Train /enc-33 /enc-33 I/D /enc-33
10 Train /enc-33 /enc-33 I/D /enc-33
11 Train /enc-33 /enc-33 /enc-33
12 Train /enc-33 /enc-33 /enc-33
13 Train /enc-33 /enc-33 /enc-33
mix(T12) Test /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33
Table 7: Concept sets for the T12sub-set of the original bAbI tasks, and the new tasks generated with Dyna-bAbI.
Train sub-task numbering follows the original bAbI numbering. The inject anddiverse tasks inherit the same
concept set from the original tasks, and additionally “I”, “D” denote question types included only in the inject or
diverse tasks, respectively. “I/D” denotes question types included in both.
For the T5 experiments, we used the
PyTorch Lightning (Falcon et al., 2019) trainer
implementation, and Weights & Biases (Biewald,
2020) for experiment tracking and artifacts
management.
We used standard hyper-parameter settings for
all models, with slight changes in the case of
memory issues as described above.
Experimental infrastructure details. Our
experiments were performed using an RTX-8000
GPU, with a total computational budget of roughly
1,000 GPU hours.
A.3 Inoculation experiment results
To rule out the hypothesis that certain patterns may
be too hard for models to learn, we follow the
inoculation methodology presented in Liu et al.
(2019): after training on the original tasks, we fine-
tune the T5 on small amounts of OOD data (disjoint
from the test data), and evaluate performance as a
function of “inoculation dose”. As can be seen in
Fig. 7, we find that performance quickly (with only
500 additional inoculation samples per question
type) reaches over 90% accuracy on both the
mix(T7)andmix(T12)challenge sets. These results
support the hypothesis that the training data is not
rich enough, indicating clearly that the model is
capable of quickly learning to solve the challenge
tasks, given exposure to training samples with
similar enough patterns.
A.4 Concurrence experiments
Table 8 presents the full results for the concurrence
experiments of §4.1. SQuAD and bAbI task 2
results are reproduced from Liu et al. (2021), see
there also for implementation details of the models
used.
A.5 Extended error analysis: GIVE events
We analyze the performance of models on the
mix(T7)split after being trained on concat( T7), and
in particular we focus on GIVE events. As noted in
§4.2, compositions involving GIVE are intuitively
challenging as they entail multiple inferences
which are not explicit in the text: the actors share
the same location, and the possession of the object
being given is transferred from the giver to the
recipient. The only task in concat( T7)featuring
GIVE events is task 5, which never asks about the
locations of actors or objects, but only about the
participant roles in the event (e.g., who was the
giver or recipient; see Fig. 1 example from task 5).Model Evaluation accuracy
SQuAD mix(T2) mix(T7) babi task 2
rasor 64.86 88.20 35.03 100.00
bidaf 67.39 97.20 30.50 100.00
documentreader 69.66 90.20 40.70 100.00
documentreader
(no_features)69.21 82.50 37.17 100.00
bidafplusplus 69.49 99.50 44.20 80.70
mnemonicreader 73.02 98.20 39.63 100.00
mnemonicreader
(no_features)72.67 97.50 38.20 100.00
qanet 72.41 67.70 - 100.00
fusionnet 72.90 99.50 39.73 100.00
fusionnet
(no_features)72.24 88.10 37.80 100.00
bert 81.46 95.50 47.63 100.00
bert_large 84.17 98.30 59.10 100.00
bert_large_wwm 87.33 98.70 67.63 99.90
albert 81.86 98.20 56.70 100.00
albert_xxlarge 89.07 99.80 80.00 100.00
roberta 83.37 98.70 57.70 100.00
roberta_large 86.96 99.80 64.07 100.00
electra 85.88 98.70 53.47 100.00
spanbert 86.20 98.40 55.70 99.50
spanbert_large 88.74 98.60 62.27 95.40
Table 8: Full results of concurrence experiments
presented in §4.1.
Figure 7: Inoculation experiment results.
Num.
supporting factsNum.
samplesEvaluation accuracy
BiDAF RoBERTa T5
1 334 53.3 93.4 86.8
2 (w/o GIVE) 734 51.50 82.3 71.8
2 (with GIVE) 99 3.03 7.07 5.05
3 (w/o GIVE) 1365 24.6 47.2 44.3
3 (with GIVE) 468 4.27 7.05 15.2
Table 9: Breakdown of model performance on mix(T7)
for questions including (or not) GIVE events in the
supporting fact set. The poor performance on questions
including GIVE indicates that training on the bAbI
1.0 data does not facilitate generalization to novel
compositions of GIVE.
where-P (→)
yes-no (↓)correct incorrect
correct 209 4
incorrect 145 88
Table 10: Confusion matrix displaying question
phrasing sensitivities in T5. We pose a question in two
formats: (1) yes-no : “Is XatL? yes” vs (2) where-P :
“Where is X?L”. We find performance is considerably
higher for questions posed in the where-P format,
indicating the model isn’t learning the equivalence of
both forms.
To measure this intuition empirically, we analyze
a subset of 567 questions including GIVE events
in the supporting facts set. As shown in Table 9,
performance for all models on questions including
GIVE is extremely low, far below performance for
questions without it. Qualitative analysis indicates
many failure cases follow the pattern shown in the
right-side example of Fig. 1c, question on line
10: the location of an entity (e.g., Daniel) must be
inferred via the known (co-)location of a second
participant in the GIVE event (e.g., Jeff). These
results strengthen the hypothesis that standard QA
training on the original bAbI data does not drive
strong event comprehension in models.
A.6 Extended error analysis: question
phrasing sensitivity
This section presents further empirical analysis
of the question phrasing sensitivities discussed in
§4.2.1, relating to the performance of the T5 model
trained on the diverse( T12)data and evaluated on
the challenge set mix(T12).
We collected all yes-no questions from mix(T12)
for which the answer was “yes”, yielding 446
questions in total. For each such (question, answer)
pair, of the form (“Is person at the location ?”,
“yes”), we created an equivalent pair in the format
of a where-P question, (“Where is person ?”,
location ). Figure 4 shows a characteristic
example. Ideally, we would expect a model to
be agnostic to equivalent phrasings of a question.
However, as displayed in Table 10, we find that T5
is considerably more accurate for questions posed
in the where-P format, likely due to exposure to a
larger variety of such questions at training time.1 Bill grabbed the milk.
2 Bill put down the milk.
3 John is either in the bedroom or the kitchen.
4 Fred journeyed to the kitchen.
5 John grabbed the football.
6 Following that he put down the football.
7 Bill picked up the milk.
8 Following that he went to the bedroom.
9 Bill is in the office.
10 Bill is in the cinema.
11 Bill passed the milk to Julie.
12 Julie handed the milk to Bill.
13 Jeff is not in the school.
14 John took the football.
15 Fred and Jeff moved to the school.
16 Afterwards they journeyed to the bathroom.
17 Bill handed the milk to Julie.
18 John dropped the football.
19 Daniel is either in the school or the
bedroom.
20 Daniel took the football.
21 Is John in the bedroom? yes 3 18 19 20
Figure 8: Double disjunction example from mix(T12).
A.7 Extended error analysis: double
disjunctions
As the shown in the §4.2.1 error analysis, a
particularly difficult class of questions are double
disjunctions over indefinite expressions. Figure 8
displays a typical example from mix(T12), where
the locations of two actors are given in indefinite
form (sentences 3 and 19), and are also known to
be co-located, since they share the location of the
object “football”, as inferred from sentences 18 and
20. Hence it is possible to infer their location as the
intersection of the two indefinite expressions (here
“bedroom”). Rather than answering “yes” to the
question “Is John in the bedroom?”, T5 invariably
answers “maybe” for such cases. This pattern
is likely due to the fact that in the training data
“maybe” is a typical answer for yes-no questions
about actors mentioned by indefinite expressions
(task 10 in bAbI 1.0).
B Datasheet for datasets
Motivation
For what purpose was the dataset created?
Was there a specific task in mind? Was there
a specific gap that needed to be filled? Please
provide a description.
Few synthetic resources for probing NLP
models’ performance on discourse-level narrative
understanding texts. Existing resources lack
customizability (control over data created +
amenable to extension).
Who created this dataset (e.g., which
team, research group) and on behalf of
which entity (e.g., company, institution,
organization)?
Joint team of researchers at Hebrew University
of Jerusalem (Israel) and the Allen Institute for
Artifical Intelligence.
Who funded the creation of the dataset? If
there is an associated grant, please provide
the name of the grantor and the grant name
and number.
Work was supported by the Center for
Interdisciplinary Data-science Research (CIDR)
at HUJI. This work was also supported by the
European Research Council (ERC) under the
European Union’s Horizon 2020 research and
innovation programme (grant no. 852686, SIAM)
and NSF-BSF grant no. 2017741 (Shahaf). Part of
this research is also supported by the European
Research Council, ERC-StG grant no. 677352
(Tsarfaty).
Any other comments?
Composition
What do the instances that comprise the
dataset represent (e.g., documents, photos,
people, countries)? Are there multiple types
of instances (e.g., movies, users, and ratings;
people and interactions between them; nodes
and edges)? Please provide a description.
Instances represent variable length stories.
How many instances are there in total (of
each type, if appropriate)?
Any size dataset can be created (programmatic
generation).
Does the dataset contain all possible
instances or is it a sample (not necessarilyrandom) of instances from a larger set?
If the dataset is a sample, then what is the
larger set? Is the sample representative of the
larger set (e.g., geographic coverage)? If so,
please describe how this representativeness
was validated/verified. If it is not representative
of the larger set, please describe why not
(e.g., to cover a more diverse range of
instances, because instances were withheld
or unavailable).
Used rejection sampling for some datasets to cover
more diverse instances.
What data does each instance consist
of? “Raw” data (e.g., unprocessed text or
images) or features? In either case, please
provide a description.
Simple textual stories generated using templates
(“John went to the kitchen. He grabbed the apple.”).
Is there a label or target associated with
each instance? If so, please provide a
description.
Each instance is accompanied by a (question,
answer) pair, both in natural language.
Is any information missing from individual
instances? If so, please provide a description,
explaining why this information is missing (e.g.,
because it was unavailable). This does not
include intentionally removed information, but
might include, e.g., redacted text.
N/A
Are there recommended data splits (e.g.,
training, development/validation, testing)?
If so, please provide a description of these
splits, explaining the rationale behind them.
The data is organized in splits, which are explained
in section 4 of the paper.
Are there any errors, sources of noise, or
redundancies in the dataset? If so, please
provide a description.
Template based language generation may result in
somewhat unnatural texts.
Is the dataset self-contained, or does it link
to or otherwise rely on external resources
(e.g., websites, tweets, other datasets)? If
it links to or relies on external resources, a)
are there guarantees that they will exist, and
remain constant, over time; b) are there official
archival versions of the complete dataset
(i.e., including the external resources as they
existed at the time the dataset was created); c)
are there any restrictions (e.g., licenses, fees)
associated with any of the external resources
that might apply to a future user? Please
provide descriptions of all external resources
and any restrictions associated with them,
as well as links or other access points, as
appropriate.
Self contained.
Does the dataset contain data that,
if viewed directly, might be offensive,
insulting, threatening, or might otherwise
cause anxiety? If so, please describe why.
No.
Does the dataset relate to people? If not,
you may skip the remaining questions in this
section.
No.
Any other comments?
Collection Process
How was the data associated with each
instance acquired? Was the data directly
observable (e.g., raw text, movie ratings),
reported by subjects (e.g., survey responses),
or indirectly inferred/derived from other
data (e.g., part-of-speech tags, model-
based guesses for age or language)? If
data was reported by subjects or indirectly
inferred/derived from other data, was the data
validated/verified? If so, please describe how.
Programmatically generated using logical rules
and templates.
If the dataset is a sample from a larger
set, what was the sampling strategy (e.g.,
deterministic, probabilistic with specific
sampling probabilities)?
Rejection sampling was used in some cases,
described in Section 4.
Does the dataset relate to people? If not,
you may skip the remaining questions in this
section.
No.
Any other comments?
Preprocessing/cleaning/labelingWas any preprocessing/cleaning/labeling
of the data done (e.g., discretization or
bucketing, tokenization, part-of-speech
tagging, SIFT feature extraction, removal of
instances, processing of missing values)?
If so, please provide a description. If not, you
may skip the remainder of the questions in this
section.
No.
Was the “raw” data saved in addition to the
preprocessed/cleaned/labeled data (e.g., to
support unanticipated future uses)? If so,
please provide a link or other access point to
the “raw” data.
N/A
Is the software used to
preprocess/clean/label the instances
available? If so, please provide a link or other
access point.
N/A
Any other comments?
Uses
Has the dataset been used for any tasks
already? If so, please provide a description.
Benchmark to guide model development for
reading comprehension and textual reasoning tasks.
Is there a repository that links to any or all
papers or systems that use the dataset?
If so, please provide a link or other access
point.
Not currently, we will use the https://
paperswithcode.com/ integration to track
results.
What (other) tasks could the dataset be
used for?
N/A
Is there anything about the composition of
the dataset or the way it was collected and
preprocessed/cleaned/labeled that might
impact future uses? For example, is there
anything that a future user might need to
know to avoid uses that could result in
unfair treatment of individuals or groups (e.g.,
stereotyping, quality of service issues) or
other undesirable harms (e.g., financial harms,
legal risks) If so, please provide a description.
Is there anything a future user could do to
mitigate these undesirable harms?
N/A
Are there tasks for which the dataset
should not be used? If so, please provide a
description.
Similar to the original bAbI benchmark, our
tasks are not a substitute for real natural language
datasets, but should rather complement them. Even
if a method works well on our data, it should be
shown to perform well on real data as well. Rather,
our tasks are better thought of as comprehension
“unit-tests”, where poor performance on our tasks
serves as a warning sign suggesting the model
may exhibit limited systematicity and robustness
on more difficult, naturalistic inputs.
Any other comments?
Distribution
Will the dataset be distributed to third
parties outside of the entity (e.g., company,
institution, organization) on behalf of which
the dataset was created? If so, please
provide a description.
N/A
How will the dataset will be distributed (e.g.,
tarball on website, API, GitHub) Does the
dataset have a digital object identifier (DOI)?
Github + Weights and Biases. No DOI currently.
When will the dataset be distributed?
Data and code-base for task generation to be
uploaded upon publication.
Will the dataset be distributed under a
copyright or other intellectual property (IP)
license, and/or under applicable terms of
use (ToU)? If so, please describe this license
and/or ToU, and provide a link or other access
point to, or otherwise reproduce, any relevant
licensing terms or ToU, as well as any fees
associated with these restrictions.
Will be available with standard MIT license.
Have any third parties imposed IP-based or
other restrictions on the data associated
with the instances? If so, please describe
these restrictions, and provide a link or other
access point to, or otherwise reproduce, anyrelevant licensing terms, as well as any fees
associated with these restrictions.
N/A
Do any export controls or other regulatory
restrictions apply to the dataset or to
individual instances? If so, please describe
these restrictions, and provide a link or other
access point to, or otherwise reproduce, any
supporting documentation.
N/A
Any other comments?
Maintenance
Who will be
supporting/hosting/maintaining the
dataset?
Corresponding author of paper.
How can the owner/curator/manager of
the dataset be contacted (e.g., email
address)?
Via email with corresponding author, and through
dedicated GitHub repository.
Is there an erratum? If so, please provide a
link or other access point.
N/A
Will the dataset be updated (e.g., to correct
labeling errors, add new instances, delete
instances)? If so, please describe how
often, by whom, and how updates will be
communicated to users (e.g., mailing list,
GitHub)?
Extensions will be maintained via GitHub.
If the dataset relates to people, are there
applicable limits on the retention of the
data associated with the instances (e.g.,
were individuals in question told that their
data would be retained for a fixed period
of time and then deleted)? If so, please
describe these limits and explain how they will
be enforced.
N/A
Will older versions of the dataset continue
to be supported/hosted/maintained? If so,
please describe how. If not, please describe
how its obsolescence will be communicated to
users.
Data versioning supported natively through
Weights and Biases.
If others want to extend/augment/build
on/contribute to the dataset, is there a
mechanism for them to do so? If so, please
provide a description. Will these contributions
be validated/verified? If so, please describe
how. If not, why not? Is there a process for
communicating/distributing these contributions
to other users? If so, please provide a
description.
The codebase can be freely extended, we will only
be responsible of course for changes to the main
branch.
Any other comments?
Chapter 6
Breakpoint Transformers for
Modeling and Tracking
Intermediate Beliefs
Kyle Richardson∗, Ronen Tamari∗, Oren Sultan, Dafna Sha-
haf, Reut Tsarfaty, Ashish Sabharwal
Published in the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), 2022
77
Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs
Kyle Richardson2†Ronen Tamari1†∗Oren Sultan1
Reut Tsarfaty2,3Dafna Shahaf1Ashish Sabharwal2
1The Hebrew University of Jerusalem2Allen Institute for AI3Bar-Ilan University
{ronent,orens,dshahaf}@cs.huji.ac.il ,{kyler,reutt,ashishs}@allenai.org
Abstract
Can we teach natural language understanding
models to track their beliefs through intermedi-
ate points in text? We propose a representation
learning framework called breakpoint modeling
that allows for learning of this type. Given any
text encoder and data marked with intermediate
states ( breakpoints ) along with corresponding
textual queries viewed as true/false proposi-
tions (i.e., the candidate beliefs of a model, con-
sisting of information changing through time)
our approach trains models in an efficient and
end-to-end fashion to build intermediate rep-
resentations that facilitate teaching and direct
querying of beliefs at arbitrary points alongside
solving other end tasks. To show the benefit
of our approach, we experiment with a diverse
set of NLU tasks including relational reasoning
on CLUTRR and narrative understanding on
bAbI. Using novel belief prediction tasks for
both tasks, we show the benefit of our main
breakpoint transformer , based on T5, over con-
ventional representation learning approaches
in terms of processing efficiency, prediction
accuracy and prediction consistency, all with
minimal to no effect on corresponding QA end-
tasks. To show the feasibility of incorporating
our belief tracker into more complex reasoning
pipelines, we also obtain SOTA performance
on the three-tiered reasoning challenge for the
TRIP benchmark (around 23-32% absolute im-
provement on Tasks 2-3).1
1 Introduction
Despite considerable progress made recently in nat-
ural language understanding (NLU), driven largely
by advances in language model pre-training (De-
vlin et al., 2019; Raffel et al., 2020) and the de-
velopment of large-scale NLU benchmarks (Wang
et al., 2018), understanding the behavior of mod-
els remains a formidable and highly consequential
∗Work begun during an internship at the Allen Institute.
†Equal contribution.
1Project code available at https://github.com/
allenai/situation_modeling .
b=[1,2,3,4,5],...
c=5, b=[1,2,3,4],...(a) Breakpoints in a formal language
4
5
(b) "Breakpoints" for natural languageVariable StatesProgram
Text
1
2
3
4The
apple is
in the
parkThe apple
is in the
kitchenMary
is at
the
parkJohn is
at the
kitchen
b=[ ?  ,  ?   ,  ?  ,  T  ]Belief Queries/V ariables
b=[ F  ,  T   ,  ?  ,  T  ]
b=[ F  ,  T   ,  F  ,  T  ]
b=[ T  ,  F   ,  T  ,  T  ]
(c) Use breakpoints for model debugging (joint model)
Question:  Where is the apple?
constraint  
checking
Does my model know : Objects that don't  move stay in their location?Feed answer  
as query  
 to belief model,  
check  
consistencySys. Answer : The apple is in the park
Post-hoc
ProbingFigure 1: Deep narrative understanding in natural lan-
guage (bottom) involves the ability to answer queries
about arbitrary intermediate points in a given story. We
liken this task to breakpoints in programming (top), or
reporting the state of a program at different stages of ex-
ecution, facilitating human inspection of model beliefs
and consistency with end-task behavior (bottom).
challenge for model safety. Such a challenge is par-
ticularly acute in tasks such as narrative understand-
ing, where one must piece together many individual
(possibly implicit) facts through time in order to
solve problems. For example, in the story in Fig-
ure 1, answering the question Where is the apple?
requires knowing how to track objects through time
(e.g., knowing the location of the John andMary
and their interaction) and how to compartmentalize
other types of knowledge across the story. In such
a setting, where models are trained to narrowly an-
swer questions, a natural question arises: do models
acquire the kind of requisite background knowledge
and world tracking abilities, and ultimately learn
representations that give rise to correct beliefs2
about intermediate states?
A chief difficulty in answering such questions is
2Similar in spirit to Kassner et al. (2021), we define a
belief as an attribution of a truth value to a proposition relative
to a context or partial information state (Landman, 2012).
E.g., a belief that John is in the kitchen istrue in the context
immediately following the event John went to the kitchen .
John went to  
the store and  
bought an apple[B]
breakpoint tokenHe then returned  
 home to the kitchenJohn is in the store  
John is in the kitchen  
John has the apple  
John has the ballJohn has the apple  
John is in the kitchen  
The apple in in kitchen  
He refers to John
queries  proposition encodingT F T U ... T T T T ...belief state belief state
proposition encoding
propositions
story input sencoder
End-task (e.g., QA)[B]
story encoding  breakpoint  encodings  
decoderFigure 2: A high-level view of our modeling approach.
For a given story and a set of textual queries correspond-
ing to intermediate points in the story ( breakpoints ),
truth assignments are assigned to queries to form belief
states based on a projection over encodings of break-
points and individual proposition encoding s using a
single task-specific encoder .
that directly inspecting the propositional attitudes
of our current models remains a formidable chal-
lenge due to the latent nature of their knowledge.
Such a complication also makes it unclear what the
right interface should be for eliciting beliefs in the
first place (e.g., how can we determine if a model
believes a proposition John is in the kitchen at an
arbitrary point in text?). In addition, for tasks such
as QA, story contexts and questions are usually en-
coded jointly (often with full attention over context
and query), which makes it difficult to tease apart
a model’s understanding of a story independent of
each question. Entangled story and question repre-
sentations can be inefficient when scaling to a large
space of questions, particularly for novel combina-
tions of questions and stories (Tamari et al., 2022).
Such entangled representations also allow models
to exploit spurious patterns in questions that in-
flate performance (Kaushik and Lipton, 2018) and
hinder interpretability.
We present a model-agnostic representation
learning framework called breakpoint modeling
that facilitates teaching models to have proposi-
tional beliefs at arbitrary points in stories (or break-
points ) using ordinary textual queries as our inter-
face language. Our general modeling approach is
illustrated in Figure 2. Given any task-specific en-
coder and data marked with the intermediate state
of interest (or breakpoints , denoted throughout as
[B]) along with a set of textual queries (i.e., the
candidate beliefs provided in training as auxiliaryintermediate supervision), models are trained in
an end-to-end fashion to learn intermediate task-
specific representations (pooled from single encod-
ings of stories) that jointly facilitate making correct
and consistent belief predictions efficiently across
a large space of queries. Making an analogy with
breakpoints in programming (see top of Figure 1),
we aim to simulate stopping execution at interme-
diate points during a story to inspect the model’s
belief state (e.g., checking that a model’s answers
for QA are consistent with their beliefs and sat-
isfy certain high-level constraints), as well as teach
the model to have certain beliefs learned through
intermediate supervision at training time.
Using a state-of-the-art pretrained model, T5
(Raffel et al., 2020), we develop and investigate
abreakpoint transformer to do belief prediction
on three categories of tasks: narrative understand-
ingon bAbI (Weston et al., 2016; Tamari et al.,
2022), relational reasoning on CLUTRR (Sinha
et al., 2019) and physical commonsense reason-
ingover human authored stories on TRIP (Storks
et al., 2021). In the former two cases, we focus
on training and evaluating models on a novel be-
lief prediction task. We report improvements over
a conventional transformer-based representation
learning approach (Reimers and Gurevych, 2019)
both in terms of prediction accuracy (4% to 8%
absolute improvement on CLUTRR dev) and belief
consistency, all with significantly improved pro-
cessing efficiency (i.e., minimal forward calls to
the full transformer) and minimal effect on end-
task performance when jointly trained with QA. In
the latter case for TRIP, we show how to integrate
our modeling approach into a more complex trans-
former pipeline and report state-of-the-art results
on the three-tiered reasoning task (with 23-32% ab-
solute improvement on two component tasks) over
existing task-specific architectures.
Taken together, our results show the viability
of building an end-to-end trainable belief track-
ing mechanism and integrating it within exist-
ing transformer-based reasoning systems. To our
knowledge, our work is among the first to look at at
general-purpose sentence representation learning
for intermediate states in text as a way to facilitate
complex situation reasoning.
2 Related Work
Our work brings together two recent areas that aim
to understand model behavior (broadly model prob-
Task Example Stories Breakpoint Propositions
Relational
Reasoning
(CLUTRR)John is the brother of Susan [B]1Susan’s mother
is Janice [B]2, ...P1: { ‘Susan is the sister of John’ true , ‘Susan is the sister-in-law of Janice‘
false ,‘Janice is the mother of John‘ unk }
P2: { ‘Janice is the mother of John‘ true , ‘John is the father of Janice‘ false , ...}
Story
Understanding
(bAbI)John moved to the kitchen [B]1He picked up
an apple [B]2John then gave the apple to Mary
[B]3...P1:{ ‘John has the apple‘ false , ‘John is in the kitchen‘ true ,...}
P2: { ‘John has the apple‘ true ,‘John is in the kitchen‘...}...
P3: { ‘John has the apple‘ false , ‘Mary has the apple‘ true }
Commonsense
(TRIP)Tom dropped his radio ...carpet. [B]1The radio
broke .. [B]2Tom turned on the radio ... [B]3...P2: { ‘radio is in pieces‘ true , ‘radio is powered‘ false }, ...
P3: { ‘radio was powered‘ true }
Figure 3: Three tasks rendered as stories with special breakpoint tokens [B]j(for convenience, marked with an
index j). Each intermediate breakpoint is aligned to a set of propositions Pjmarked with truth conditions (i.e.,
true,false , unknown) corresponding to the truth value of each proposition at that breakpoint.
ing): probing of the type that includes finding neu-
ral correlates of high-level behavioral phenomena,
modular structure in networks (Tenney et al., 2019;
Hewitt and Manning, 2019) on the one hand, as
well as diagnostic testing, which aims to understand
model competence through controlled input-output
testing (Lake and Baroni, 2018; Richardson et al.,
2020), or post-hoc consistency analysis (Kassner
et al., 2021). Our work is more closely related
to Li et al. (2021), who show that partial world
state information can be decoded from NLMs even
without explicit supervision. In that work, state
information is roughly localized to entity mentions,
but varies across different datasets. Differently
from such probing work, our breakpoint models are
trained in a supervised manner to localize particu-
lar propositional information at particular locations
(similar to Geiger et al. (2021)).
Our breakpoint model closely relates to late-
interaction encoder architectures that tease apart
the encoding of problems and solutions. This in-
cludes the sentence transformer from Reimers and
Gurevych (2019), which we compare against in
our experiments, as well as read once transformers
(Lin et al., 2021), colBERT (Khattab and Zaharia,
2020) and others. Given that the types of narrative
tasks we focus on require modeling many inter-
mediate points, we follow this work in putting an
emphasis on representation and encoding efficiency.
In contrast to this, and other related work on sen-
tence representation learning (Gao et al., 2021; Ni
et al., 2022), we uniquely focus on learning repre-
sentations of intermediate states in text for complex
situational reasoning.
We are also inspired by the situation modeling
literature in cognitive science (Golden and Rumel-
hart, 1993; Frank et al., 2003; Venhuizen et al.,
2019), and proposals for their integration with NLP
research (Tamari et al., 2020). These works also
studied neural models of narrative comprehension
in carefully controlled micro-worlds, but typicallyfocused on relatively short sentence-level inputs.
Our work also relates to efforts on building inter-
pretable models by making the underlying reason-
ing processes transparent, either through explicit
decomposition (Andreas et al., 2016; Khot et al.,
2021; Bostrom et al., 2022) or generation of ratio-
nales (Camburu et al., 2018; Wiegreffe and Maraso-
vic, 2021) and other reasoning structures (Tafjord
et al., 2021; Dalvi et al., 2021; Gontier et al., 2020).
In contrast, we focus on belief representations that
are ultimately faithful (Jacovi and Goldberg, 2020)
to end-tasks by training knowledge directly into a
model’s task-specific representations.
3 Breakpoint Modeling
The goal of breakpoint modeling is to capture the
intermediate states and beliefs of models at arbi-
trary positions in text. Our models take stories as
inputs, or pieces of text containing one or more in-
termediate positions ( breakpoints ), as well as sets
of text propositions that align to certain intermedi-
ate points (see Figure 3). Such propositions play
the role of auxiliary supervision if provided at train-
ing time or as queries to the model for performing
probing; when coupled with predictions they con-
stitute the beliefs of the model.
While breakpoint models can technically take
different forms, their basic function is to assign
encodings to intermediate states in text and their
corresponding propositions (§ 3.1) and to make pre-
dictions about the truth/falsity of each proposition
(§ 3.2). Learning (§ 3.3) reduces to the problem of
teaching a model to have a correct and consistent
set of beliefs for each target task given a set of rep-
resentative intermediate propositions and beliefs
provided at training time (§ 3.4).
3.1 Breakpoint and Proposition Encoding
As illustrated in Figure 3, stories are texts
consisting of ntokens within which there can
exist m≥1arbitrarily selected intermedi-
  
 
PropositionsBelief StatesSingle Read Breakpoint Model
T...F F...U T...T
  
  
 T...F T...FMulti-pass Late Interaction
 
 ...T...FFigure 4: What is the best way to model intermediate
states and beliefs with existing encoder models? An il-
lustration of two late-interaction architectures we inves-
tigate ( Single Read model, our main model described
in § 3 and a Multi-pass Late Interaction model)
ate points or breakpoints . For convenience,
we will render a story sin the following way:
s:=w1,b1. . . w ·,b1[B]. . . w ·,bj. . .[B]. . . w n,bm[B]
where [B]is a special token used to explicitly
mark position of each breakpoint bj. Intuitively, a
breakpoint token represents all of the information
in the story relevant to building an accurate belief
state at the corresponding (intermediate) point
in the text. Associated with each bjis a set of
text propositions Pj={p1, p2, ..., p t}. Truth
assignments to these text propositions constitute
the candidate beliefs at breakpoint bj(in the sense
of Footnote 2).
At the core of any breakpoint model are two
encoders, enc story,enc prop, that are used to gener-
ate a representation or embedding for each break-
point in the story and each proposition, respec-
tively. Representations of breakpoints b∈Rdare
pooled from a single encoding of an input story
s:cs←enc story(s)∈R|s|×dand representations
for propositions cprop∈Rdare obtained in a sim-
ilar fashion using enc prop. While the choice of
the encoder and the details of how pooling is done
can vary (see details in §5.1), in all of our models
breakpoint representations bare obtained by taking
projections of the hidden states of the [B]tokens
fromcs. We also investigate models that assume a
siamese architecture (Reimers and Gurevych, 2019)
where enc storyandenc pare the same encoder.
An important property of breakpoint models
is that all breakpoints representations bjare ob-
tained from a single read encoding of each target
story. We later compare this against a much less
efficient approach that requires multiple forward
passes through the story to obtain intermediate en-
codings (i.e., the multi-pass approach shown in
Figure 4). Our model therefore stays within the
spirit of a late-interaction architecture (Khattab
and Zaharia, 2020) by using separate encodings ofbreakpoints and propositions, which allows us to
scale to large sets of propositional queries.
3.2 Proposition Scoring and Semantics
Given a breakpoint encoding band an aligned
proposition encoding cprop, aproposition scorer
makes a prediction about a proposition at that
breakpoint. As mentioned, our aim is to pre-
dict the truth value of a proposition at an inter-
mediate state, which we take to be the model’s
belief in that proposition. Our scorer takes
the form of a classifier that maps a breakpoint
encoding and proposition encoding to the dis-
crete space {true,false,unknown }, following
Li et al. (2021) and the annotation scheme from
NLI (Dagan et al., 2005; Bowman et al., 2015).
To make clear that the interpretation of each
proposition is tied to a specific breakpoint, we will
use the symbolic notation from Li et al. (2019) and
introduce three binary logical predicates E,C,and
U. For each bjandp∈Pj, these predicates cap-
ture whether pisentailed by, is contradicted by,
or has an unknown relation to the information in
the text at breakpoint bj, respectively. For instance,
E(bj, p)is true if the text proposition pis entailed
by the story at breakpoint bj.
3.3 Learning
Suppose we have a dataset Dconsisting of n
stories {s(i)}n
i=1along with the following addi-
tional information. For each story s(i), we have
mbreakpoints B(i).3For each such breakpoint
bj, we have tlabeled text propositions4P(i)
j,
where each proposition pk∈P(i)
jis labeled with
y(i)
j,k∈ {true,false,unknown }indicating pk’s
truth value at breakpoint bj. Using the above pred-
icate logic notation, we can equivalently think of
having, for each pk∈P(i)
j, exactly one predicate
Y(i)
j,k∈ {E,C,U}annotated in D, with the se-
mantics that Y(i)
j,k(bj, pk)is True (and the other two
predicates for bjandpkare False).
The goal here is to learn a model that assigns
truth values to all text propositions across all
breakpoints—equivalently, truth values for all three
logical predicates—in a way that maximally aligns
withD. Semantically, this can be expressed as
3In general, mdepends on i. However, for simplicity of
exposition, we use mhere instead of m(i).
4Again, tin general depends on both iandj, but we use t
instead of t(i)
jhere for simplicity.
satisfying the logical formula (Li et al., 2019):
/logicalanddisplay
s(i)∈D/logicalanddisplay
bj∈B(i)/logicalanddisplay
pk∈P(i)
jY(i)
j,k(bj, pk) (1)
with the added constraint that for each story s(i)
and all j, k, exactly one of E(bj, pk),C(bj, pk),
andU(bj, pk)is True.
Using Pr[y(i)
j,k]to denote the model’s probability
corresponding to the predicate Y(i)
j,k(bj, pk), this
formula can be translated into the following loss
using the product translation from Li et al. (2019):
Lprop=n/summationdisplay
i=1m/summationdisplay
j=1t/summationdisplay
k=1−log Pr[ y(i)
j,k] (2)
which yields the common cross-entropy loss that
we use in our experiments.
3.4 Proposition Sampling
Propositions in breakpoint models have a dual role:
when given at training time, they provide interme-
diate supervision for training models across dif-
ferent situation states. When given at inference
time they allow for post-hoc probing of a model’s
beliefs. As shown in the Figure 3, propositions,
in virtue of being ordinary text, can express many
different types of information and thus provide an
unbounded source of semantic supervision (Hanjie
et al., 2022), e.g., for expressing fluents , or condi-
tions that change through time in a story (e.g., John
is in the kitchen , or event pre/post-conditions (e.g.,
The radio was powered via English tense).
For training models to have beliefs, a necessary
first step is to devise a sampling policy for gener-
ating these intermediate annotations. While such
a strategy needs to be tailored to each target task,
we experiment with a combination of extracting
propositions from existing task annotations (Fig-
ure 5) and generating propositions based on a set
ofdomain constraints using the semantics of each
target domain (details in the next section).
4 Proposition Prediction Tasks
We focus on three categories of tasks: text-based
relational reasoning ,story understanding and
commonsense reasoning , each considered in turn.
In the former two cases, we devise new proposi-
tion and belief prediction tasks that involve training
on intermediate belief state annotations. We also
include out-of-distribution (o.o.d) generalizationQiana is Lisa’s mother
Qiana is Derick’s wife Derrick is Lisa’s father
Story Lisa is Jerry’s granddaughter [B]Derrick is Lisa’s
father [B]Qiana is Derrick’s wife [B]
Atomic Belief
Annotations :
The basic facts
that should be
predictedE(1,‘Jerry is the grandfather of Lisa‘)
E(1,‘Derick is the father of Lisa‘),
C(1,‘Lisa ..father of Derick‘),
E(3,‘Qiana..wife of Derick‘),
E(3,Qiana is the mother of Lisa)
Knowledge :
Constraints to
satisfy(Implies(And dsdE(2,‘Derick...father of Lisa‘)
dsdE(3,‘Qiana is the wife of Derick‘))
E(3,‘Qiana is the mother of Lisa‘))
Figure 5: How are intermediate propositions collected?
An illustration of constructing intermediate proposi-
tions from CLUTRR proof trees (above) in Gontier
et al. (2020). BOTTOM: An example ground constraint,
which we use for analyzing consistency.
tests beyond standard i.i.d (independent and iden-
tically distributed) evaluation. In the latter case,
we recast an existing task in terms of breakpoint
models to show the versatility of our approach in a
more complex multi-task setting.
4.1 Relational Reasoning
CLUTRR Sinha et al. (2019) focuses on QA over
synthetic stories about family relations as shown
in Figure 3, and has more recently been extended
to focus on proof generation (Gontier et al., 2020).
As illustrated in Figure 5, we use the proof anno-
tations in the latter work to generate intermediate
propositions that track the time-course of family
relations as they emerge at each new sentence.
Relying on the clean subset of CLUTRR stories
Sinha et al. (2019) and proof annotations, break-
points are added after each sentence. Propositional
renderings of the explicit story facts, as well as
intermediate propositions revealed in the proof an-
notations, were then added to each correspond-
ing breakpoint in the story and serve as the base
proposition set. From these base propositions, ad-
ditional propositions, including negative and un-
known propositions, were added using the follow-
ing general constraints: monotonicity , that beliefs,
once established to be true /false, cannot change;
themutually exclusivity of certain relations (e.g.,
X is the grandfather of Y is mutually exclusive with
X is the grandmother of Y );inverse relations be-
tween certain relations ( e.g., thatXfemis a sister
ofYfemmeans that Yis a sister of X), and that all
non-deductively valid propositions are unknown
(i.e., with label U).5Such ground propositions con-
5We note that all such constraints remain faithful to the
semantics of the original tasks, such as CLUTRR.
straints are included in the breakpoint annotations
as symbolic expressions (see again Figure 5) to al-
low for measuring model consistency at inference
time (later in Figure 6. See details in § A.3.
o.o.d evaluation. Stories in CLUTRR are charac-
terized by their length k(number of events) and
generalization testing is usually performed to mea-
sure generalization. Our main datasets (later seen
in Table 1) consists of 13k training stories drawn
from stories from k=2,..,5. We tune our mod-
els and evaluate on a mixture of in-domain and
generalization stories of lengths k=2,..8 each con-
taining around 1.5k stories (containing (avg) 10
propositions per breakpoint and 15 constraints per
story). While these splits deviate from standard
uses of CLUTRR, we also compare against stan-
dard splits (i.e., training on k= 2,3and testing on
k′= 2, ..10) to look at the ability of training joint
belief prediction and QA models on the original
QA task .
4.2 Story Understanding
We experiment with the bAbI QA benchmark (We-
ston et al., 2016), which contains questions over
stories about agents in controlled micro-worlds (see
Figure 3). As with CLUTRR, the synthetic nature
of domain makes it possible to automatically ex-
tract proposition annotations that express object
location (e.g., PersonX/ObjectX’ is in Y ),object
possession (PersonX has ObjectY ), abstractions
ofevent post-conditions (e.g., PersonX took some-
thing for the event PersonX grabbed the ball ) and
pronoun references (e.g., He refers to John ). We
use the Dyna-bAbI task generator (Tamari et al.,
2022) to generate initial base propositions and, sim-
ilar with CLUTRR, heuristically add more proposi-
tions using domain constraints (see § A.2 for more
details).6We use propositional versions of the 7-
task set introduced in Tamari et al. (2022). We
specifically use the long-form version of this set,
where stories all contain 20 events/breakpoints, and
train on 500 examples per task (totaling 3.5k+1.4k
training/evaluation stories, with an average of 10
propositions per breakpoint and 123 constraints per
story).
o.o.d evaluation. In addition to training and test-
ing on this set, we also look at joint training on
proposition prediction and the original QA task.
For evaluation we also consider a more challeng-
6In contrast to CLUTRR and TRIP, bAbI does not have
explicit unknown proposition annotations, hence propositions
either have label EorC.inghardQA generalization task from Tamari et al.
(2022), where the test set features compositions
of concepts seen at training time. Appendix A.2
contains example inputs and further task details.
4.3 Physical Commonsense Reasoning
We apply our approach to the recently introduced
Tiered Reasoning for Intuitive Physics (TRIP)
dataset (Storks et al., 2021). TRIP features a story
plausibility end task, similar in scope to our propo-
sition task, as well as a multi-tiered evaluation of
models’ reasoning process. Given a pair of highly
similar human-authored short stories about every-
day activities, models must jointly identify (1) the
implausible story ( task1 ) (2) a pair of conflicting
sentences in the implausible story ( task2 ) (3) the
underlying physical states in those sentences caus-
ing the conflict ( task 3 ). While task3 takes the
form of a breakpoint modeling task, where physi-
cal states are rendered as textual propositions, we
model the first two tasks as text2text tasks using
multi-task breakpoint models (details in the ap-
pendix and in § 5.1). We use the original splits,
consisting of 675 plausible stories and 1472 implau-
sible stories. While we focus on the multi-tiered
evaluation, we devised a small filtered dev set (644
stories) for later model analysis (Table 5).
5 Modeling Details and Metrics
Here we detail our main breakpoint transformer
(§5.1) following the framework in § 3 and all met-
rics used in our experiments (§ 5.2).
5.1 Modeling
Encoder We experimented with the T5 model (Raf-
fel et al., 2020) using the implementation from
Wolf et al. (2020). T5’s bi-directional encoder
was used for both our story encoder enc storyand
proposition encoder enc prop. While any compa-
rable encoder would suffice, we chose T5 due its
common use in NLU and ability to perform gen-
eration, which we used to implement other com-
ponents in the multi-task models discussed below.
For efficiency reasons, we experimented with a
combination of the smaller T5-base model (with
220M parameters) for datasets with long stories
and many propositions (TRIP, bAbI) and T5-large
(with 770M parameters) for CLUTRR.
Breakpoint and Proposition Embeddings For
each story, individual breakpoint representations
are first pooled from the [B]token hidden states in
the story encodings cs(see again Figure 4). Fol-
lowing Ni et al. (2022), a linear projection and L2
normalization is applied to each representation to
construct initial breakpoint embeddings. To allow
for information transfer between different break-
points, we then apply an additional self-attention
layer ( sit-self ) over these resulting representations
to obtain a self-attention breakpoint representation
(see Fan et al. (2020) for a similar idea), which gets
concatenated with the initial representation to cre-
ate the final breakpoint embedding. Operationally,
the self-attention layer takes the form of a standard
transformer block (Vaswani et al., 2017) with a
single attention head.
One subtlety in using a standard bi-directional
encoder such as T5 is that each breakpoint token
can look at future parts of the story. While the
content of a breakpoint is often determined by the
preceding sentence, in some cases it is important to
have information about the future to obtain an accu-
rate representation. For example, for the story John
has the apple. [B] 1He then moved to the kitchen
[B] 2, knowing that John can’t be in the kitchen at
[B] 1(apre-condition ofmove events) requires look-
ing into the future. To limit the amount of future
information in part of our breakpoint representa-
tions, however, future masking is applied in the
breakpoint self-attention layer described above.
To obtain a proposition embedding, we use the
same T5 encoder over each text proposition pre-
fixed with a special token, then take the hidden
state of the target proposition. A final proposition
representation is then similarly obtained using the
same linear projection and normalization layers.
Proposition Classifier As in Li et al. (2021),
we use a bilinear layer for proposition
classification ( score (·)). Using the nota-
tion from § 3.3, probabilities ˆ y(bj,p) =
⟨Pr[E(bj, p)],Pr[C(bj, p)],Pr[U(bj, p)]⟩for the
3 truth values of a proposition pare computed
in the following way using the final breakpoint
representation bjand proposition encoding cp:
score (bj, p) =bT
j·M·cp+a
ˆ y(bj, p) =softmax (score (bj, p)).
Learning In addition to optimizing for the objec-
tive described in § 3.3 ( Lprop), we also experiment
with multi-task models trained to do generation
(Lgen) and QA ( Lqa), both of which are formu-
lated as text2text tasks and optimized using stan-
dard cross-entropy-based training. In the formercase, we investigate two analogues to the unsu-
pervised denoising objectives from (Raffel et al.,
2020), which aim to increase the amount of local in-
formation contained in breakpoint representations.
The first is an event generation task that in-
volves generating randomly chosen events from
their right-most breakpoint encodings (e.g., gen-
erating the text Susan’s mother is Janice from the
encoding of [B]2in Figure 3). The second, which is
inspired by Gontier et al. (2022), generates textual
abstractions either of random events from break-
points (in the case of TRIP, e.g., generating the
abstracted text PERSON dropped his OBJ... from
[B] 1in Figure 3) or random pairs of events in a
story (e.g., generating the text A person received
an apple from the an encoding averaged from the
two breakpoints [B] 2and[B] 3in Figure 3) (see
additional details in § B.2).
Taken together, our full multi-task model’s loss
is:L=λ1Lprop+λ2Lqa+λ3Lgenwhere λ{1,2,3}
are task weights manually tuned during training.
We used ADAM as our optimizer (Kingma and
Ba, 2014). Standardly, hyper-parameter tuning and
model selection was performed via a random search
search in the style of Devlin et al. (2019) on held-
out dev sets (see details in § B.1). Unless stated
otherwise, we report the average of three random
restarts for all models and their standard deviations.
Baselines We compare against two standard sen-
tence representation learning approaches based on
transformers and LSTMs. For the former we use
the sentence transformer approach (Reimers and
Gurevych, 2019) applied to our task, and for the lat-
ter we use a model close to Conneau et al. (2017).
The set up is standard: stories and propositions
are encoded separately using a single encoder and
collected via mean (transformer) and max (BIL-
STM) pooling then aggregated via concatenation
(in the style of InferSent Conneau et al. (2017))
and fed into a softmax classifier to make a belief
prediction. Importantly, these baselines models are
much less efficient compared with our single read
breakpoint model, in that they require making mul-
tiple ( multi-pass late interaction ) forward passes
through stories to create intermediate representa-
tions as illustrated in Figure 4. For the transformer
models, with use the same T5 encoder as in the
breakpoint models throughout all experiments.7
7As an additional check, we trained T5-based proposition-
only baseline, similar to the partial-input baselines in NLI
(Poliak et al., 2018), that make truth predictions from propo-
sitions alone to check for spurious patterns. These always
Given that our breakpoint models take full story
texts as input, to make the baselines fully compa-
rable, we similarly feed in the full story on each
read with a similar special token ( #) to mark the
target intermediate point (e.g., In the story John
went to the store. He bought an apple we feed the
textJohn went to the store. # He bought an apple
when modeling the first breakpoint).
Joint Modeling For CLUTRR and bAbI, we also
compare our multi-task breakpoint model trained
for QA against T5 and Bart (Lewis et al., 2020),
both fine-tuned solely for QA.
5.2 Metrics
For proposition prediction tasks we measure over-
allproposition accuracy (%). Similarly for QA
experiments, we follow other work in measuring
exact match EM accuracy (%) against a model’s
generated output. For some of our analysis on
CLUTRR (Figure 5), we measure the consistency
of belief prediction using the global consistency
metric ρfrom Li et al. (2019), which measures the
fraction of stories containing one or more constraint
violation using the constraint annotations described
in § 4. For example, using the constraint on the
bottom Figure 5, we first have the model make
predictions about the constituent propositions (1.
Derick is the father of Lisa , 2.Qiana is the wife
of Derick . 3. etc..) and see if those predictions
symbolically satisfy the constraint.
For TRIP, we follow exactly the 3-tiered evalua-
tion of Storks et al. (2021). We calculate: Plausibil-
ity (task 1): % of instances where the implausible
story was correctly identified. Consistency (task
2):% of correctly identified implausible stories
where the conflicting sentences were correctly iden-
tified. Verifiability (task 3) : % of instances with
correct plausibility/consistency predictions, where
all relevant physical states are also identified.
6 Results and Discussion
We focus on the following questions: 1. Can our
main model effectively and efficiently solve our
new belief proposition prediction tasks (introduced
in § 4) and model intermediate state? 2.Can we
effectively integrate our breakpoint model into joint
models for solving more complex tasks?
Proposition Prediction We found breakpoint mod-
els to be effective at our proposition prediction
tasks, most notably improving on the transformer
perform worse than our BILSTM baselines.Proposition Prediction
Model Dev / Test Set + (std) (Acc %)
Majority Baseline 44.60 / 41.60
BILSTM (Multi-pass) 60.36 / 58.59 (±0.24)
T5-large (Multi-pass) 81.41 / 81.94 (±0.17)
BPT-large 85.16 /85.24 (±0.34)
Question-answering, dev / test + (std), ( EMAcc %)
Model i.i.d generalization
FT-T5-base 99.00 / 99.78 (±0.19) 84.19 / 75.13 (±0.94)
FT-Bart-base 98.65 / 98.94 (±0.78) 83.21 / 70.42 (±1.23)
BPT-base 99.24 / 99.75 (±0.19) 83.61 / 74.84 (±0.89)
Table 1: TOP: Proposition prediction results on
CLUTRR on the main mix dev and test sets comparing
our breakpoint model ( BPT ) with baselines. BOTTOM:
Evaluation on standard CLUTRR QA ( k= 2,3) com-
paring our breakpoint model trained joint with QA to
fined-tuned ( FT) T5 and Bart models.
Figure 6: Effect of training data size on proposition
prediction (left) and global consistency ρ(right, lower
is better ), on CLUTRR dev (best of 3 random runs).
Multi-pass baselines for CLUTRR prediction from
81.9 to 85.2 (top of Table 1, both an over 23% im-
provement over our BILSTM baseline, suggesting
task difficulty). Based on the plots in Figure 6, we
also found our models to be more efficient learners
(e.g., achieving comparable performance to base-
lines using only 60% training data) and to exhibit
less global constraint violations in the i.i.d setting
(with a 6% reduction in constraint violations ρ),
thus leading to more consistent belief states.
Model i.i.d hard QA
Prop.% QA% QA%
Majority 65.87 – –
FT-T5-base (QA) – 97.29 (±0.14)69.09 (±0.79)
FT-Bart-base – 97.57 (±0.31)67.21 (±0.80)
BILSTM (Multi) 80.2 (±0.16) – –
T5-base (Multi) 99.1 (±0.21) – –
BPT-base 98.5 (±0.10) – –
BPT-base + QA 98.5 (±0.10) 94.9 (±0.60)70.51 (±0.29)
Table 2: bAbI proposition prediction ( Prop. % ) and
QAperformance on the main i.i.d and hardQA test
sets.
For bAbI (Table 2) all transformer-based mod-
els achieve near perfect accuracy (and significantly
outperform our BILSTM model); as such, mod-
els have near perfect consistency on the underly-
ing constraints (not shown). Given that bAbI sto-
ries are considerably longer than CLUTRR stories
(each containing 20 events/breakpoints), these re-
sults show the feasibility of modeling long contexts
with our model and representing complex state in-
formation with individual breakpoints. In contrast
to the baseline transformers, here we also see con-
siderable practical improvements in training time
efficiency due to our single read architecture, re-
sulting in a 54% reduction in training time (from
around 63 hours for multi-pass models to around
34 for ours on a single RTX A6000 GPU).
CLUTRR i.i.d vs. generalization ( gener. ) splits
i.i.d(k= 2, ..5) gener.
Baseline (best run) 94.54 ( ρ= 32.1) 61.7 ( ρ= 96.2)
BPT (best run) 95.69 ( ρ= 25.9) 69.2 (ρ= 97.6)
Table 3: Comparison between i.i.d and compositional
settings for CLUTRR.
Our model’s proposition prediction consistency
is 7.5% higher than that of the baseline, in terms
of the ρmetric reported in Table 3. As an impor-
tant caveat, however, in absolute terms, even our
breakpoint model has much lower consistency on
generalizations tasks (69.2%) than in the i.i.d. set-
ting (95.7%). We discuss this further in § 8.
Joint Training When trained jointly for both propo-
sition prediction and QA, we found minimal to no
impact on end-task performance, as shown on the
bottom of Table 1 for CLUTRR and in Table 2 for
bAbI (with a small improvement on the generaliza-
tion QA task at the cost of a mere 2% degradation
in i.i.d. QA performance). This shows the viability
of integrating our belief tracking mechanism into
existing transformer pipelines without significant
performance drops. As first motivated in Figure 1,
it also permits the development of more debuggable
systems where the results of QA can be checked
against the model’s beliefs.
Split Model Task 1 (Plaus.) Task 2 (Consist.) Task 3 (Verif.)
Dev RoB 73.6 22.4 10.6
BPT-base 81.99 (±0.91) 58.07 (±0.76) 36.44 (±0.53)
Test RoB 72.9 19.1 9.1
BPT-base 80.55 (±1.20) 53.83 (±1.65) 32.37 (±0.27)
Table 4: Results on the TRIP 3-tiered physical com-
monsense reasoning benchmark, our main breakpoint
model ( BPT ) compared against the RoBERTa-based
approach (RoB) of Storks et al. (2021).
Through our results on TRIP (Table 4), we also
see the viability of adding our belief tracking mech-
anism into more complex modeling pipelines. We
were specifically able to obtain SOTA performance
on this task and outperform the larger and highly
tailored task-specific model architecture based onRoBERTa-large used by Storks et al. (2021).
CLUTRR (mix dev)
Prop. Acc% Global Violations ρ
BPT-large (best run) 85.5 ( ∆) 36.7 ( ∆)
- brk self-attn 77.3 (-8.12) 54.3 (-17.61)
- event generation 82.1 (-3.36) 41.8 (-5.07)
- abstraction 82.1 (-3.35) 42.8 (-6.06)
BPT-base 81.8 (-3.62) 44.3 (-7.61)
TRIP (fitered dev)
BPT-base (best run) 92.8 ( ∆) –
- brk self-attn 89.43 (- 3.36) –
- event generation 89.43 (- 3.36) –
- abstraction 92.9 (+0.10) –
Table 5: Breakpoint model feature ablations.
Additional Analysis We see in Table 5 for
CLUTRR that having an additional self-attention
aggregation layer when constructing breakpoint
representations ( -brk self-attn , § 5.1) is very im-
portant for accuracy and consistency (we find sim-
ilar results for TRIP, bottom). This suggests that
further improvements might be achieved through
improved pooling and masking strategies for con-
structing breakpoint representations. We also
see the advantages of having auxiliary generation
losses ( event generation, abstraction ) for improv-
ing accuracy and performance.
7 Conclusion
Being able to track the beliefs of models remains a
formidable challenge at the forefront of model in-
terpretability. In this paper, we presented a new rep-
resentation learning framework, breakpoint model-
ing, that facilitates end-to-end learning and track-
ing of beliefs at intermediate states in narrative
text. On a diverse set of NLU tasks, we show the
benefit of our approach (based on T5) over conven-
tional learning approaches in terms of improved
belief prediction performance on new belief track-
ing tasks and processing efficiency. We also show
the feasibility of recasting existing tasks into our
framework and integrating our approach into exist-
ing transformer-based NLU pipelines, which we
believe can help to improve the interpretability of
these models as part of this larger challenge.
Acknowledgements
The authors thank the Aristo team for valuable
feedback. This work was supported by the Euro-
pean Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation pro-
gramme (grant no. 852686, SIAM, Shahaf). Part
of this research was also supported by the Euro-
pean Research Council, ERC-StG grant no. 677352
(Tsarfaty), which is gratefully acknowledged.
8 Limitations
Below we summarize the main limitations of our
current breakpoint models and the techniques pur-
sued in this study.
Compositional Generalization Despite richer
supervision over intermediate states, compositional
generalization performance remains a significant
challenge (on bAbI and CLUTRR generalization
splits, see §6) for future work, which shows that our
approach inherits many of the limitations in the gen-
eralization ability of large-scale LMs more broadly.
Following Kim et al. (2021) and others, we hypoth-
esize that the all-to-all attention employed by Trans-
formers in creating token encodings (including the
breakpoint tokens) is a factor in non-compositional
behavior; such attention is more vulnerable to over-
fitting spurious patterns. Accordingly, more ad-
vanced attention masking (Kim et al., 2021) and su-
pervision (Yin et al., 2021) approaches are promis-
ing directions to explore.
Our notion of “belief” While breakpoints pro-
vide an indication of intermediate model “beliefs”,
they are also different from beliefs in important
ways. In particular, the causal relation between
information represented in breakpoints and gen-
erated model outputs is unclear (see also Li et al.
(2021) for similar caveats in standard NLMs). For
example, models may generate outputs that are in-
consistent with their own breakpoint belief states.
Interestingly, breakpoint models may offer new
ways to address these limitations by more explicitly
representing intermediate reasoning steps; neural
logic losses (Li et al., 2019) can help enforce belief
consistency between sets of propositions (§3.3).
Task and domain limitations Finally, our exper-
iments are still limited to datasets involving rela-
tively short (TRIP) and synthetic (bAbI, CLUTRR)
inputs with limited semantics. Further work is
needed to address more natural and complex lan-
guage to ultimately develop more robust break-
point models. In contrast to standard end-to-end
QA methods, breakpoint modeling requires more
costly annotation, as training currently requires
some form of supervision on intermediate states,
beyond the final target output. Thus, developing
new methods for collecting such annotations with
minimal engineering effort remains a challenge.References
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and
Dan Klein. 2016. Learning to compose neural net-
works for question answering. In Proceedings of the
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 1545–1554, San
Diego, California. Association for Computational
Linguistics.
Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, and
Greg Durrett. 2022. Natural language deduction
through search over statement compositions. arXiv
preprint arXiv:2201.06028 .
Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
InProceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing , pages
632–642, Lisbon, Portugal. Association for Compu-
tational Linguistics.
Oana-Maria Camburu, Tim Rocktäschel, Thomas
Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-
ral language inference with natural language expla-
nations. Advances in Neural Information Processing
Systems , 31.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing , pages 670–680, Copen-
hagen, Denmark. Association for Computational Lin-
guistics.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Machine Learning Challenges Workshop ,
pages 177–190. Springer.
Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan
Xie, Hannah Smith, Leighanna Pipatanangkura, and
Peter Clark. 2021. Explaining answers with entail-
ment trees. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 7358–7370, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Angela Fan, Thibaut Lavril, Edouard Grave, Armand
Joulin, and Sainbayar Sukhbaatar. 2020. Address-
ing some limitations of transformers with feedback
memory. arXiv preprint arXiv:2002.09402 .
Stefan L. Frank, Mathieu Koppen, Leo G.M. Noord-
man, and Wietske V onk. 2003. Modeling knowledge-
based inferences in story comprehension. Cognitive
Science , 27(6):875–910.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 6894–6910, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh
Rozner, Elisa Kreiss, Thomas Icard, Noah D Good-
man, and Christopher Potts. 2021. Inducing causal
structure for interpretable neural networks. arXiv
preprint arXiv:2112.00826 .
Richard M. Golden and David E. Rumelhart. 1993. A
parallel distributed processing model of story compre-
hension and recall. Discourse Processes , 16(3):203–
237.
Nicolas Gontier, Siva Reddy, and Christopher Pal. 2022.
Does entity abstraction help generative transformers
reason? arXiv preprint arXiv:2201.01787 .
Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Chris
Pal. 2020. Measuring systematic generalization in
neural proof generation with transformers. In Ad-
vances in Neural Information Processing Systems ,
volume 33, page 22231–22242. Curran Associates,
Inc.
Andrew R Haas. 1987. The case for domain-specific
frame axioms. In The Frame Problem in Artificial
Intelligence , pages 343–348. Elsevier.
Austin W Hanjie, Ameet Deshpande, and Karthik
Narasimhan. 2022. Semantic supervision: Enabling
generalization over output spaces. arXiv preprint
arXiv:2202.13100 .
John Hewitt and Christopher D Manning. 2019. A struc-
tural probe for finding syntax in word representations.
InProceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers) , pages 4129–4138.
Alon Jacovi and Yoav Goldberg. 2020. Towards faith-
fully interpretable NLP systems: How should we
define and evaluate faithfulness? In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 4198–4205, On-
line. Association for Computational Linguistics.
Nora Kassner, Oyvind Tafjord, Hinrich Schütze, and
Peter Clark. 2021. BeliefBank: Adding memory to a
pre-trained language model for a systematic notion
of belief. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 8849–8861, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.Divyansh Kaushik and Zachary C. Lipton. 2018. How
much reading does reading comprehension require? a
critical investigation of popular benchmarks. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing , pages 5010–
5015, Brussels, Belgium. Association for Computa-
tional Linguistics.
Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-
cient and effective passage search via contextualized
late interaction over bert. In Proceedings of the 43rd
International ACM SIGIR conference on research
and development in Information Retrieval , pages 39–
48.
Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter
Clark, and Ashish Sabharwal. 2021. Text modular
networks: Learning to decompose tasks in the lan-
guage of existing models. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 1264–1279, Online.
Association for Computational Linguistics.
Juyong Kim, Pradeep Ravikumar, Joshua Ainslie, and
Santiago Ontanon. 2021. Improving compositional
generalization in classification tasks via structure an-
notations. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers) ,
pages 637–645, Online. Association for Computa-
tional Linguistics.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
Brenden Lake and Marco Baroni. 2018. Generalization
without systematicity: On the compositional skills
of sequence-to-sequence recurrent networks. In In-
ternational conference on machine learning , pages
2873–2882. PMLR.
Fred Landman. 2012. Structures for semantics , vol-
ume 45. Springer Science & Business Media.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020.
BART: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 7871–7880, Online. Association for Computa-
tional Linguistics.
Belinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021.
Implicit representations of meaning in neural lan-
guage models. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers) , pages 1813–1827, Online. Association for
Computational Linguistics.
Tao Li, Vivek Gupta, Maitrey Mehta, and Vivek Sriku-
mar. 2019. A logic-driven framework for consistency
of neural models. In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP) ,
pages 3924–3935, Hong Kong, China. Association
for Computational Linguistics.
Shih-Ting Lin, Ashish Sabharwal, and Tushar Khot.
2021. ReadOnce transformers: Reusable representa-
tions of text for transformers. In Proceedings of the
59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers) , pages 7129–7141, Online. As-
sociation for Computational Linguistics.
Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant,
Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022.
Sentence-t5: Scalable sentence encoders from pre-
trained text-to-text models. In Findings of the As-
sociation for Computational Linguistics: ACL 2022 ,
pages 1864–1874, Dublin, Ireland. Association for
Computational Linguistics.
Adam Poliak, Jason Naradowsky, Aparajita Haldar,
Rachel Rudinger, and Benjamin Van Durme. 2018.
Hypothesis only baselines in natural language infer-
ence. In Proceedings of the Seventh Joint Confer-
ence on Lexical and Computational Semantics , pages
180–191, New Orleans, Louisiana. Association for
Computational Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res. , 21(1).
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
3982–3992, Hong Kong, China. Association for Com-
putational Linguistics.
Kyle Richardson, Hai Hu, Lawrence Moss, and Ashish
Sabharwal. 2020. Probing natural language inference
models through semantic fragments. In Proceedings
of the AAAI Conference on Artificial Intelligence ,
pages 8713–8721.
Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle
Pineau, and William L. Hamilton. 2019. CLUTRR:
A diagnostic benchmark for inductive reasoning from
text. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
4506–4515, Hong Kong, China. Association for Com-
putational Linguistics.Shane Storks, Qiaozi Gao, Yichi Zhang, and Joyce Chai.
2021. Tiered reasoning for intuitive physics: Toward
verifiable commonsense language understanding. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021 , pages 4902–4918, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.
ProofWriter: Generating implications, proofs, and
abductive statements over natural language. In Find-
ings of the Association for Computational Linguis-
tics: ACL-IJCNLP 2021 , pages 3621–3634, Online.
Association for Computational Linguistics.
Ronen Tamari, Kyle Richardson, Noam Kahlon, Aviad
Sar-shalom, Nelson F. Liu, Reut Tsarfaty, and Dafna
Shahaf. 2022. Dyna-bAbI: unlocking bAbI’s po-
tential with dynamic synthetic benchmarking. In
Proceedings of the 11th Joint Conference on Lexi-
cal and Computational Semantics , pages 101–122,
Seattle, Washington. Association for Computational
Linguistics.
Ronen Tamari, Chen Shani, Tom Hope, Miriam R L
Petruck, Omri Abend, and Dafna Shahaf. 2020. Lan-
guage (re)modelling: Towards embodied language
understanding. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics , pages 6268–6281, Online. Association for
Computational Linguistics.
Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019.
BERT rediscovers the classical NLP pipeline. In
Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 4593–
4601, Florence, Italy. Association for Computational
Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems , pages 5998–6008.
Noortje J. Venhuizen, Matthew W. Crocker, and Harm
Brouwer. 2019. Expectation-based comprehension:
Modeling the interaction of world knowledge and lin-
guistic experience. Discourse Processes , 56(3):229–
255.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. GLUE:
A multi-task benchmark and analysis platform for nat-
ural language understanding. In Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP , pages
353–355, Brussels, Belgium. Association for Com-
putational Linguistics.
Jason Weston, Antoine Bordes, Sumit Chopra, and
Tomás Mikolov. 2016. Towards ai-complete question
answering: A set of prerequisite toy tasks. In 4th In-
ternational Conference on Learning Representations,
ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
Conference Track Proceedings .
Sarah Wiegreffe and Ana Marasovic. 2021. Teach me to
explain: A review of datasets for explainable natural
language processing. In Thirty-fifth Conference on
Neural Information Processing Systems Datasets and
Benchmarks Track (Round 1) .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language processing.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online. Association
for Computational Linguistics.
Pengcheng Yin, Hao Fang, Graham Neubig, Adam
Pauls, Emmanouil Antonios Platanios, Yu Su, Sam
Thomson, and Jacob Andreas. 2021. Compositional
generalization for neural semantic parsing via span-
level supervised attention. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 2810–2823, Online.
Association for Computational Linguistics.
A Dataset Details
In this section, we provide additional details about
all datasets.
A.1 TRIP
As described in §4, the TRIP benchmark con-
sists of 3 tiered tasks: (1) plausibility (2) con-
sistency (3) verifiability. To apply our model to
TRIP, we convert the first two tasks to a text2text
format: the first task involves taking two stories
(A) storyA (B) storyB $plaus as input
text and producing a text label { A,B} to identify
the implausible story; task 2 involves taking a
labeled story sentence1 1 sentence2 2..
$conflict and generating the labels identify-
ing the problematic sentences.8We convert the
third task to breakpoint format by converting state
change labels to textual propositions associated
with the corresponding timesteps. Figure 8 shows
an example instance from the TRIP development
set. Note that each task is effectively rendered as
two instances: the first instance addresses task 1
(as QA), and the second jointly addresses tasks 2
(QA) and 3 (proposition prediction).
State changes in TRIP are defined either as ef-
fects or preconditions (Storks et al., 2021) and this
information must be preserved in the conversion to
breakpoint format. Preconditions are propositions
that hold before a described event; for example,
the proposition “oven was open” should be true
before the sentence “John closed the oven.” Ef-
fect propositions are propositions that hold after
a described event; the proposition “oven is open”
should be false after “John closed the oven.” We
represent precondition and effect propositions sim-
ply by modifying the proposition tense. Given
breakpoint bt, for associated precondition proposi-
tions at time t, we use past tense (“oven was open”).
For effect propositions at time t, we use present
tense (“oven is open”).
While the TRIP data includes state information
for all time steps and entities, we follow the official
evaluation procedure9and only score the subset
of state changes defined to be relevant in the pair
of conflict sentences. At training time, we use all
available state change information for training.
8$plaus and$conflict are special tokens that prompt
the model to output an answer for tasks 1 and 2, respectively.
9https://github.com/sled-group/
Verifiable-Coherent-NLU/blob/main/
Verifiable-Coherent-NLU.ipynbFinally, while most state changes in TRIP are
attributes that can be true,false orunknown (and
thus can be directly converted to proposition form),
location attributes are formulated as k-way classifi-
cation problems. For example, an object location
attribute change is represented by 1 of 9 possible
classes (see Table 5 in Storks et al. (2021) and blue
propositions in Fig. 8). To facilitate equivalent
evaluation of k-class predictions with our break-
point model, we consider the predicted true score
for each of the possible kpropositions and take the
maximum scoring proposition to be the predicted
value.10
A.2 bAbI
A.2.1 Proposition generation
As detailed in § 4, base propositions for bAbI are
generated using the Dyna-bAbI tool (Tamari et al.,
2022). From this, new propositions are derived
from the following general constraints: location/-
possession uniqueness that dictate that objects can
only be in one location at a time and possessed by
a single agent (e.g., John cannot simultaneously be
in the kitchen and living room ),mutually exclusiv-
itybetween event types (e.g., that dropping a ball
is the opposite of picking up a ball );explanation
frame rules (Haas, 1987) that dictate that objects,
when left unchanged, maintain their location and
their possession through time (e.g., John is in the
kitchen orJohn has the apple stays true until there
is an explicit event that changes this).
A.2.2 Task details
The training data includes 500 samples per task
type, where the tasks follow the same structure as
theconcat(T7) dataset described in (Tamari et al.,
2022) (Table 6 in that work), with the only dif-
ference being the story length which was fixed to
20 sentences to match the test data. The hardQA
generalization task was generated using the same
settings as the mix(T7) evaluation set from (Tamari
et al., 2022), including the same 3 question types
with 1,000 samples for each type (also Table 6 in
Tamari et al. (2022)). Figure 7 shows example
stories from the training and hardQA test splits.
A.3 CLUTRR
We note that all of the underlying story data was
generated from scratch and relies on the publicly
available task generators from Sinha et al. (2019)
10Inspired by a similar method in Li et al. (2021).
Figure 7: Examples from the bAbI story understanding task. The train set includes 7 sub-tasks, such as co-reference
and object tracking (left). The hardQA sample (right) incorporates novel compositions of concepts seen separately
at training time. Beyond the question answering task, each example also includes proposition prediction at each
time step (not shown here, see Figure 1) for example.
and Gontier et al. (2020)11. As detailed in Gontier
et al. (2020), leakage among the proofs and propo-
sitions in stories of the same kcan be a problem.
Using some of their ideas, we avoided this by ex-
panding the inventory of names used in training
and abstracted names for parts of the training. We
verified the hardness of our data by training a no-
story proposition-only baseline an found it to have
low performance, and also manually verified all
inference rules used for generating propositions.
B Training details
B.1 Hyper-parameters
All hyper-parameter tuning for our main models
was performed via a random search in the style
of Devlin et al. (2019). Model selection was per-
formed by selecting models with the highest valida-
tion accuracy for each task (e.g., proposition accu-
racy for our proposition tasks, exact match for the
QA experiments). Unless noted otherwise, we re-
port the average of models with the optimal hyper-
parameters based on 3 random re-starts; early stop-
ping was applied throughout. All experiments were
performed on NVIDIA AX6000 GPU hardware on
a single GPU.
breakpoint models :learning rate (we experi-
mented in the range of 1e-3 to5e-6 , we gen-
erally found 5e-5 to be optimal for most experi-
11See full details at: https://github.com/
facebookresearch/clutrr and https:
//github.com/NicolasAG/SGinPGments), number of epoch (up to 35 for CLUTRR,
TRIP and 150 for bAbI), batch size (in the range
of2to16, memory permitting, we found 2to be
optimal for bAbI and TRIP experiments, and 4for
CLUTRR) and weight decay (set to 0.001 ) and
warmup steps (from 500 to1ksteps). See the
project repository for further details
joint models For multi-task training, parameters
λ{1,2,3}were hand tuned, with λ1set to 1.0 for
all proposition prediction tasks (with λ2=0.1 for
most tasks). For joint QA tasks, we found setting
λ1=1.0 andλ1=1.0 to be optimal, with
an initial warmup before turning on the proposi-
tion prediction loss (usually between 5-10epochs).
Given the high cost of training the bAbI breakpoint
QA model in Table 2, the joint QA + prop models
described on the last row start training from the
BPT-base checkpoints described in the row above.
B.2 Auxiliary Generation Losses
As detailed in § 5.1, we jointly trained our break-
point models with additional generation losses that
aim to mimic some of the unsupervised denoising
objectives used in Raffel et al. (2020). Whereas
in standard denoising you might try to generate
from a text input A dog <mask> while running
the output text <mask> barked loudly <mask> ,
from an original text A dog barked loudly while
running (with full attention over the input text),
in our case we try to generate from a story John
went to the store [B] 1He then picked up the
# Task 1 (plausibility)
{
"example id": "414-C0-a",
"question": "(A) John turned on the oven [B] John put the cake in the oven [
B] John got the ice cream out [B] John put some ice cream in a red bowl
[B] John put the red bowl in the oven [B] (B) John turned on the oven [B
] John put the cake in the oven [B] John got the ice cream out [B] John
put some ice cream in a red bowl [B] John put the rest of the ice cream
in the fridge [B] $plaus",
"answer": "B"
}
# Tasks 2 + 3 (consistency + verifiability)
{
"example id": "414-C0-b",
"question": "John turned on the oven 0 [B] John put the cake in the oven 1 [
B] John got the ice cream out 2 [B] John put some ice cream in a red
bowl 3 [B] John put the red bowl in the oven 4 [B] $conflict",
"answer": "3,4"
"proposition_lists": [
[...], # sent. idx 0
[...], # sent. idx 1
[...], # sent. idx 2
[
"red bowl is occupied",
"ice cream is put into a container",
"ice cream does not move to a new location",
"ice cream disappears",
"ice cream is picked up",
"ice cream is put down",
"ice cream is put on", "ice cream is removed",
"ice cream is taken out of a container",
"ice cream moved somewhere new",...
], # sent. idx 3
[
"red bowl is put into a container", "oven was powered",
"oven was open", "oven was turned on",...
], # sent. idx 4
],
"labels": [
[...], # sent. idx 0
[...], # sent. idx 1
[...], # sent. idx 2
["true", "true", "false", "false", "false", "false", "false", "false","
false", "false",...], # sent. idx 3
["true", "true","true", "true",...], # sent. idx 4
]
}
Figure 8: Rendering of TRIP instance in breakpoint format. Breakpoint models can operate in standard text-to-text
mode, generating output answers in response to questions, and additionally they can provide joint predictions over
propositions associated with each sentence. Propositions in blue indicate location attributes which are evaluated as
k-class predictions. See Appendix A.1 for further details on instance construction.
apple [B] 2the raw event text John went to the
store from the corresponding raw breakpoint hid-
den state for the special token [B] 1alone. In
addition to this event generation task, we also
experimented with a abstraction generation task:
given two stories in a batch and two random break-
points within those stories, e.g., John went to the
kitchen [B] 1,1...andMary went to the kitchen
[B] 2,1.., we ask the model to generate an abstract
textual description of the two events only from
the mean of the two breakpoint hidden states, i.e.,
abstraction ([B] 1,1,[B] 2,1) =A person went
to the kitchen . (This was inspired by the abstraction
generation ideas from Gontier et al. (2022)).
During training, both forms of generation were
done by randomly selecting a single breakpoint
example and abstraction pair for each story in the
batch and computing a standard loss over the gen-
erated texts and abstractions. Using symbolic an-
notations of both the CLUTRR and bAbI training
events, a deterministic algorithm was implemented
for creating abstracted texts on the fly for training.
For TRIP, where logical annotations are not avail-
able, the abstraction task was replaced by the task
of generating versions of text replaced with POS
tags (e.g., John turned off the stove would be turned
intoPER turned off the NOUN ).
Chapter 7
From Users to (Sense)Makers:
On the Pivotal Role of Stigmergic
Social Annotation in the Quest
for Collective Sensemaking
Ronen Tamari, Daniel Friedman, William Fischer, Lauren
Hebert, Dafna Shahaf
Published in the ACM Conference on Hypertext and Social Media
(HT), 2022
95
From Users to (Sense)Makers: On the Pivotal Role of Stigmergic Social
Annotation in the Quest for Collective Sensemaking
RONEN TAMARI, DAOStack, Hebrew University of Jerusalem, Israel
DANIEL A FRIEDMAN, University of California, Davis, USA
WILLIAM FISCHER and LAUREN HEBERT, Veeo, USA
DAFNA SHAHAF, Hebrew University of Jerusalem, Israel
The web has become a dominant epistemic environment, influencing people’s beliefs at a global scale. However, online epistemic
environments are increasingly polluted, impairing societies’ ability to coordinate effectively in the face of global crises. We argue
that centralized platforms are a main source of epistemic pollution, and that healthier environments require redesigning how we
collectively govern attention. Inspired by decentralization and open source software movements, we propose Open Source Attention,
a socio-technical framework for “freeing” human attention from control by platforms, through a decentralized eco-system for creating,
storing and querying stigmergic markers; the digital traces of human attention.
CCS Concepts: •Human-centered computing →Social content sharing ;Social tagging systems .
ACM Reference Format:
Ronen Tamari, Daniel A Friedman, William Fischer, Lauren Hebert, and Dafna Shahaf. 2022. From Users to (Sense)Makers: On the Pivotal
Role of Stigmergic Social Annotation in the Quest for Collective Sensemaking. In Proceedings of the 33rd ACM Conference on Hypertext and
Social Media (HT ’22), June 28-July 1, 2022, Barcelona, Spain. ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3511095.3536361
1 INTRODUCTION
The web has become a dominant epistemic environment, shaping peoples’ beliefs and knowledge on a global scale. The
web, however, is also currently a severely polluted epistemic environment [ 13], due to highly centralized and opaque
information ecologies, coupled with incentive misalignment and unprecedented information overload. A small number
of major web platforms such as Google and Facebook have gained immense control over the means to search, create,
and distribute information [ 14]. Centralization leads to opacity, in which network data as well as algorithms for content
creation, search, and distribution are effectively hidden away from public, scientific, and ethical oversight [1].
Platform incentives are fundamentally misaligned with those necessary for healthier epistemic environments [ 25]. For
example, centralization and control of data are necessary for running lucrative “attention markets”, but ultimately hinder
attempts to address information overload, and undermine both user autonomy [ 20] as well as the open information
networks necessary for healthy democracies [ 12,26]. Platforms are implicated in a host of problematic social phenomena,
including the spread of false information, behavioral changes, and societal polarization, epistemic distraction and
degradation of individual and collective sense-making capacities [ 14,24]. Impending global ecological and societal
crises lend increased urgency to addressing these problems: astute collective sense- and decision-making have perhaps
never been more needed [1, 23, 24].
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party
components of this work must be honored. For all other uses, contact the owner/author(s).
©2022 Copyright held by the owner/author(s).
Manuscript submitted to ACM
1
HT ’22, June 28-July 1, 2022, Barcelona, Spain Tamari, Friedman, Fischer, Hebert and Shahaf
Laudable recent efforts have called attention to this precarious state of affairs of collective online sensemaking [ 1,12,
14]. However, while providing invaluable insights, they have largely focused on improving platforms through regulatory
action, whether internally or externally imposed. Such platform-centric approaches are an important step towards
healthier epistemic environments, but face inherent limitations (§3), and are fraught with many impediments as they
often run counter to powerful platforms’ core business models. Perhaps more crucially, platform-centric initiatives
cannot adequately account for the fundamentally distributed [ 9], self-organizing [ 8], and stigmergic [ 16] nature of
collective intelligence.
We argue that reflecting these considerations in practice would benefit from a more radical redesign of our epistemic
environments, centered around guiding principles of agency, transparency, interoperability, decentralization, and a
collective conceptual transition to a “maker” mindset [11]; from passive users, to more active (sense)makers.
Inspired by both theoretical and practical breakthroughs of decentralization and open source software movements
contra entrenched centralized systems, we propose Open Source Attention (OSA), a conceptual framework and “call to
movement” towards decentralized, open-source, stigmergic annotation. We envision our framework as a step towards
systems for distributed governance, education, and control of collective sense-making and attention.
Hypertext and social annotation play a pivotal role in our proposed transition: in the current platform-centric ecology,
user annotations (such as likes, retweets, etc) are locked across platforms’ data siloes where they serve to optimize
platform growth. OSA aims to empower maker-centric ecologies by employing distributed content creation and storage
technology (e.g., Solid [ 19]). In this way, makers will control creation and dissemination of their annotations, which can
then be leveraged to optimize personalized human growth and learning for individuals and collectives.
2 DISTRIBUTED, STIGMERGIC FOUNDATIONS OF COLLECTIVE SENSE-MAKING
Sense-making refers to processes by which agents make sense of their environment, achieved by organizing sense
data until the environment is understood well enough to enable reasonable decisions [ 22]. Theories of extended [ 6]
and stigmergic [16] cognition highlight the integral role of environment modification in sense making; agents actively
change their environment to assist internal cognitive processes (e.g., writing to-do notes) as well as indirect stigmergic
communication with others (e.g., ant pheromone trails). Stigmergy is particularly relevant for the setting of collaboration
of large-scale groups [ 7]. In stigmergic communication, the environment acts as a kind of distributed memory; modifi-
cations left by others provide cybernetic feedback, driving both emergence of novel system-level behavior from local
Content Discovery  
ServicesContent Discovery  
Services
Makers
Decentralized  
Storage ServicesContent Discovery  
Services
PlatformAnnotation  
InterfaceContent
Discovery  
AlgorithmsContent
Discovery
Algos.
Personal Knowledge  
Management Tools (PKMs)Annotation  
InterfacesData  
StorageNew
contentDrives
Stig.
marker  
dataEnable maker
control of data 
Stig. marker dataNew
contentDrives
Data  
Storage
UsersOpen Source Attention: Maker-centric eco-system Current: platform-centric eco-system
Fig. 1. Platforms leverage control of both data and content discovery algorithms to drive growth at the expense of users (left);
Decoupling data and algorithms incentivizes content discovery services oriented towards human-centered growth (right).
2
From Users to (Sense)Makers HT ’22, June 28-July 1, 2022, Barcelona, Spain
interactions of agents, and immergence (individual interactions informed by a global state of affairs) [ 16]. Sense-making
is thus inherently co-created, through agents modifying their environment and reacting to changes made by others.
What kind of environment modifications are relevant to consider for sense-making in vast digital spaces? The
literature broadly distinguishes between two types of modifications: sematectonic stigmergy , which directly alters
the environment state (in the digital case: creating new content, such as publishing a blog post), and stigmergic
markers , which do not directly modify content, but rather serve as signalling cues (in the digital case: likes, annotations,
hyperlinking of text). Importantly, stigmergic markers play a central role in assessing epistemic quality of content, both
for humans [ 13,16] and machines [ 14], due to the sheer volume of information as well as challenges in endogenous
content interpretation. Stigmergic markers may be explicitly left by users (e.g., likes) or implicitly recorded through
their behavior (e.g., link click-through data, reading time).
3 OPEN-SOURCING STIGMERGIC MARKERS FOR HEALTHIER EPISTEMIC ENVIRONMENTS
Polluted epistemic environments are often framed as casualties of the “attention economy”; platforms selling user
data to advertisers and putting up ads in social media feeds, with the aim of “capturing” users’ attention and seducing
them to make yet another purchase. While “data” and “attention” are popular abstractions, the stigmergic perspective
is valuable in guiding practical redesign of epistemic environments. Stigmergic markers can be thought of as digital
traces of human attention, whose primacy as indicators of epistemic value makes them precious resources, whether for
extractive (e.g., ad-tech) or constructive (e.g., collective sense-making) purposes. In the following sections, we illuminate
the role of stigmergic markers in nourishing healthier epistemic environments.
3.1 From attention to intention
Healthier epistemic environments involve moving from exploitation of attention to supporting our intentions [ 24]. This
transition requires two paradigm shifts. First, a mindset shift on the human side, from passive, unwitting users consuming
“unhealthy information diets” [ 10], to active makers, who cultivate growth-oriented intentions and are mindful of the
(stigmergic) traces they leave, as well as their role as co-creators in the larger digital and physical ecology. Realistically,
humans stand no chance of making the transition in isolation; content discovery algorithms are indispensable for
navigating vast digital landscapes, but to a large degree are controlled by platforms [ 14]. Accordingly, the second
shift involves re-designing our epistemic environments to support this transition by empowering makers through
human-centric content discovery. As shown in Fig. 1, current content discovery is platform-centric: platforms enjoy a
closed feedback loop consisting of both the content discovery algorithms as well as the stigmergic marker data needed
to drive algorithmic optimization towards platform growth [ 21]. Human-centric content discovery requires supplanting
this degenerative cycle with a more symbiotic information ecology, in which makers create and control their stigmergic
markers, and thus are empowered to share their data to content discovery services oriented towards personalized
individual or collective growth. Content moderation is an important representative example [ 17]: moderation is
intractable in centralized systems, due to inherent limitations of AI capabilities as well as the scale of complex human
adjudications needed. In contrast, decentralized eco-systems enable a “marketplace of filters”, where different individuals
and organizations can create and tune content moderation systems for their own needs.
3.2 Open Source Attention: maker-centered information ecology
Analogously to open-source code and common domain knowledge [ 11], stigmergic markers can be thought of as a public
good. However, despite their unique importance, surprisingly little work has specifically targeted their decentralization
3
HT ’22, June 28-July 1, 2022, Barcelona, Spain Tamari, Friedman, Fischer, Hebert and Shahaf
(§4). Stated simply; where open source software is a movement to “free” software, similarly OSA is a movement
to “free” stigmergic markers, starting from basic hypertext primitives: emotional valence (e.g., likes), bi-directional
links, span highlighting, semantic categorization (tags, bookmarks), and textual annotation. We envision decentralized,
maker-centered ecologies, comprised of three main architectural elements (see also Fig. 1):
Annotation tools. Enable makers to easily create markers attached to any URL or content element included therein,
not just where platforms provide like buttons [ 4]. Some types of markers should themselves be mark-able, allowing for
example the option to “like” a particular annotation, or link between two annotations. Future extensions can address
implicit stigmergic markers such as read-time or click-through counts [ 15]. Apps recording these function as automatic
annotation tools, though their implicit nature requires extra caution with regard to consent and data privacy issues.
Self-sovereign storage. Makers own their markers and control their visibility (private, public, etc) to other people or
services. Identity provision is a key related service that can (but does not have to be) provided along with storage [ 21].
Content discovery services. Rather than platforms’ monolithic and opaque feeds, a decentralized ecology encourages
a market of diverse and human-centered content discovery services. For example, competing interfaces for social media
that better moderate trolls, promote thought-provoking stories, or provide customizable feed controls [17].
4 DISCUSSION
While the idea of leveraging stigmergic markers for collective sense-making has a long history [ 2], most contemporary
open-source and decentralization efforts have focused on sematectonic (content-creating) stimergy, such as code, social
media, financial ledgers and executable contracts [ 5]. Closest to our proposal is the Solid eco-system [ 19] that, similarly,
targets “re-decentralizing the web” [ 21], and empowers individuals to control their data. Solid also features a marketplace
of services, including the dokieli decentralized annotation client for scientific research [ 3,4]. While Solid and dokieli
are inspiring initial steps, they are limited with regard to content discovery services or social incentives. More broadly,
where Solid is primarily a technology, OSA proposes an ecological perspective accounting for the embeddedness of
such technologies in wider social, educational and economic contexts. For example, a key extra-technological challenge
concerns changing norms around knowledge work. Similarly to how platforms changed the culture around certain
kinds of content creation, effectively turning us all into performers, well designed social networks could help shape the
norms and prestige associated with sense-making activities. Academic Twitter demonstrates that even without direct
economic incentives, social incentives lead experts to freely share high-quality information publicly [18].
Another key question concerns scale: for global-scale sense-making, any proposal must necessarily compete with
massive, well established platforms. While recent years are seeing a resurgence of personal knowledge management apps
(PKMs) enabling content creation and annotation, knowledge tends to remain siloed at the individual level; adaptation to
collective knowledge management (CKM) has been limited.1OSA is naturally congruent with the promising “protocols,
not platforms” approach [ 17]; rather than head-to-head competition between PKMs, existing PKM growth can be
bootstrapped for CKM by introducing interoperable protocols and storage for stigmergic primitives (e.g., links, tags). In
this way, data from across diverse PKM apps could be shared to contribute to collective sense-making efforts.
Many open questions remain out of scope of this short piece, which is best seen as a call to attention; success in
surmounting the formidable challenges faced today by humanity requires that “we give the right sort of attention
to the right sort of things” [ 24]. We have claimed that attending “to the right things” will require re-imagining our
1https://athensresearch.ghost.io/season-2/
4
From Users to (Sense)Makers HT ’22, June 28-July 1, 2022, Barcelona, Spain
socio-technological systems for governing collective attention; we hope our proposal will help galvanize action towards
this vital cause.
ACKNOWLEDGMENTS
We thank Zak Stein for inspiring our exploration, and we thank Nimrod Talmon and the DAOStack team for thoughtful
feedback and support. We also thank Metagov and RadicalXChange for cultivating the wonderful real-world and online
spaces that seeded this collaboration.
REFERENCES
[1]Joseph B. Bak-Coleman, Mark Alfano, Wolfram Barfuss, Carl T. Bergstrom, Miguel A. Centeno, Iain D. Couzin, Jonathan F. Donges, Mirta Galesic,
Andrew S. Gersick, Jennifer Jacquet, Albert B. Kao, Rachel E. Moran, Pawel Romanczuk, Daniel I. Rubenstein, Kaia J. Tombak, Jay J. Van Bavel,
and Elke U. Weber. 2021. Stewardship of global collective behavior. Proceedings of the National Academy of Sciences 118, 27 (2021), e2025764118.
https://doi.org/10.1073/pnas.2025764118
[2] Vannevar Bush. 1945. As we may think. The atlantic monthly 176, 1 (1945), 101–108.
[3] Sarven Capadisli. 2020. Linked research on the decentralised Web . Ph. D. Dissertation. https://csarven.ca/linked-research-decentralised-web
[4]Sarven Capadisli, Amy Guy, Ruben Verborgh, Christoph Lange, Sören Auer, and Tim Berners-Lee. 2017. Decentralised authoring, annotations and
notifications for a read-write web with dokieli. In International Conference on Web Engineering . Springer, 469–481.
[5]Fran Casino, Thomas K Dasaklis, and Constantinos Patsakis. 2019. A systematic literature review of blockchain-based applications: Current status,
classification and open issues. Telematics and Informatics 36 (2019), 55–81. https://doi.org/10.1016/j.tele.2018.11.006
[6] Andy Clark and David Chalmers. 1998. The extended mind. analysis 58, 1 (1998), 7–19.
[7]Mark Elliott. 2006. Stigmergic Collaboration: The Evolution of Group Work: Introduction. M/C Journal 9, 2 (may 2006). https://doi.org/10.5204/mcj.
2599
[8]Nigel R. Franks and J. L. Deneubourg. 1997. Self-organizing nest construction in ants: individual worker behaviour and the nest’s dynamics. Animal
Behaviour 54 (1997), 779–796.
[9] Todd M. Gureckis and Robert L. Goldstone. 2006. Thinking in groups. Pragmatics & Cognition 14 (2006), 293–311.
[10] C Johnson. 2011. The Information Diet: A Case for Conscious Consumption . O’Reilly Media. https://books.google.com/books?id=QrW62y9l3lYC
[11] Vasilis Kostakis, Vasilis Niaros, George Dafermos, and Michel Bauwens. 2015. Design global, manufacture local: Exploring the contours of an
emerging productive model. Futures 73 (2015), 126–135. https://doi.org/10.1016/j.futures.2015.09.001
[12] Anastasia Kozyreva, Stephan Lewandowsky, and Ralph Hertwig. 2020. Citizens Versus the Internet: Confronting Digital Challenges With Cognitive
Tools. Psychological Science in the Public Interest 21, 3 (2020), 103–156. https://doi.org/10.1177/1529100620946707
[13] N Levy. 2021. Bad Beliefs: Why They Happen to Good People . OUP Oxford. https://books.google.com/books?id=C%5C_ZQEAAAQBAJ
[14] Philipp Lorenz-Spreen, Stephan Lewandowsky, Cass R Sunstein, and Ralph Hertwig. 2020. How behavioural sciences can promote truth, autonomy
and democratic discourse online. Nature human behaviour 4, 11 (2020), 1102–1109.
[15] Artur Sancho Marques and José Figueiredo. 2013. Stigmergic hyperlink: A new social web object. Information Systems and Modern Society: Social
Change and Global Development (2013), 260–272. https://doi.org/10.4018/978-1-4666-2922-6.ch016
[16] Leslie Marsh and Christian Onof. 2008. Stigmergic epistemology, stigmergic cognition. Cognitive Systems Research 9, 1-2 (2008), 136–149.
https://doi.org/10.1016/j.cogsys.2007.06.009
[17] Mike Masnick. 2019. Protocols, Not Platforms. Knight First Amendment Institute (2019).
[18] Daniel S. Quintana. 2020. Twitter for Scientists [eBook edition]. https://doi.org/10.5281/ZENODO.3707741
[19] Andrei Sambra, Amy Guy, Sarven Capadisli, and Nicola Greco. 2016. Building Decentralized Applications for the Social Web. In Proceedings of the
25th International Conference Companion on World Wide Web (Montréal, Québec, Canada) (WWW ’16 Companion) . International World Wide Web
Conferences Steering Committee, Republic and Canton of Geneva, CHE, 1033–1034. https://doi.org/10.1145/2872518.2891060
[20] Chirag Shah and Emily M. Bender. 2022. Situating Search. In ACM SIGIR Conference on Human Information Interaction and Retrieval (Regensburg,
Germany) (CHIIR ’22) . Association for Computing Machinery, New York, NY, USA, 221–232. https://doi.org/10.1145/3498366.3505816
[21] Ruben Verborgh. 2022. Re-decentralizing the Web, for good this time. In Linking the World’s Information: A Collection of Essays on the Work of Sir
Tim Berners-Lee , Oshani Seneviratne and James Hendler (Eds.). ACM. https://ruben.verborgh.org/articles/redecentralizing-the-web/
[22] K E Weick and K E W Weick. 1995. Sensemaking in Organizations . SAGE Publications. https://books.google.com/books?id=nz1RT-xskeoC
[23] Jevin D West and Carl T. Bergstrom. 2021. Misinformation in and about science. Proceedings of the National Academy of Sciences 118, 15 (apr 2021),
e1912444117. https://doi.org/10.1073/pnas.1912444117
[24] James Williams. 2018. Stand out of our light: freedom and resistance in the attention economy . Cambridge University Press.
[25] S Zuboff. 2019. The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power . PublicAffairs. https://books.google.
com/books?id=lRqrDQAAQBAJ
5
HT ’22, June 28-July 1, 2022, Barcelona, Spain Tamari, Friedman, Fischer, Hebert and Shahaf
[26] Ethan Zuckerman. 2020. The Case for Digital Public Infrastructure. The Tech Giants, Monopoly Power, and Public Discourse: An Essay Series by the
Knight Institute, Columbia University (2020). https://knightcolumbia.org/content/the-case-for-digital-public-infrastructure
6
Chapter 8
Discussion and Conclusions
“The computer is only an arc of a larger circuit which always includes
a man and an environment” – Gregory Bateson
In this work, we explored environments in NLU research, both in theory as well
as their role as technical infrastructure supporting model training and evaluation,
and data collection and annotation. We developed an ecological NLU approach
foregrounding the role of environments. Our thesis was that (1) environments are
a crucial enabler of progress both on conceptual and empirical fronts in NLU. (2)
Environments receive relatively little attention compared with modelling (architec-
ture, optimization, learning algorithms). As a result, many research questions are
hindered by environment bottlenecks in which relevant frameworks or experiments
cannot be implemented due to the lack of suitable environments.
The research in this thesis spans five years, a very long time in contemporary
NLU which has been turbo-charged by large-scale industry-based resource invest-
ment. As a result, at the conclusion of the thesis we have the chance to look back
and see how our research has aged, and discuss future extensions of our work.
In Chapter 2 we identified an environment bottleneck at the conceptual level;
the standard language modelling formulation omits environments, accounting only
for distributional meaning that lies in the statistical relations between words. We
extended this model to a novel embodied language model, informed by theories
of embodied cognitive linguistics, that additionally accounts for the mental states
of interlocutors. In this model, language acts upon interlocutors’ world models
102
which are learned through interaction with an environment. We proposed situating
language in an agent-based context including perceptions, actions and an external
world that can be interacted with. How has this proposal fared? On one hand, in
the years since we published the work, rapid progress on pre-trained large language
models has demonstrated just how much linguistic meaning can be gleaned from
statistical patterns alone, given enough data and parameters. Few researchers,
ourselves included, expected the emergence of reasoning capabilities and world
knowledge to the degree displayed by the latest models such as GPT-4. That said,
our framework predicts models will fare poorly on language requiring the tracking
of dynamic world states (world-modelling), and compositional reasoning. Rigorous
evaluation indicates that this remains the case for newer models [37] as well as the
previous generation of LLMs tested in Chapter 5. Models like Chat-GPT still
fail to solve even the original bAbI benchmark reliably [38]. Finally, agent-based
language models are a promising and rapidly evolving field of research [39, 40,
41], highlighting the relevance of our proposal. Future work could systematically
benchmark such models on bAbI 2.0 (Chapter 5) or extensions of it. Much work
remains to better understand the nature of the implicit or vector world models such
as we explored in Chapter 6. Promising approaches include the incorporation of
neuro-symbolic learning algorithms to promote logic consistency [42], as well as
more systematic probing into the nature of learned world models [43].
In Chapters 3 & 4, we identified an environment bottleneck in the annota-
tion and representation methods used in NLU systems for scientific experimental
protocols. Prior frameworks enabled span-level annotation but did not support
applications requiring deeper process-level comprehension, such as text-to-code
for parsing instructions to machine executable programs in cloud laboratories.
Support for such applications necessitates new annotation methods, domain spe-
cific languages, and execution environments. We showed how text-based games
could be leveraged as inexpensive lab simulators supporting annotation, richer
interactive training, and synthetic data generation for process-level applications.
To our knowledge, our works were among the first to propose a framework for
parsing natural language experimental protocols to executable code. Recent years
have seen increased interest in this setting across biology, chemistry and mate-
rials science [44, 45, 46]. The ChemCrow system [47] demonstrated the power
103
of agent-based LLMs coupled with domain-specific interactive chemistry environ-
ments, or tools: the tool-augmented GPT-4 far out-performed a baseline GPT-4
model across a variety of tasks such as chemical synthesis and material design.
Using models to code interactive environments was proposed by us as a promis-
ing future direction in Appendix A and was recently demonstrated as a viable,
though still highly challenging possibility [48]. Another promising future direction
involves LLM agents learning through interaction with environments, as opposed
to frozen LLMs being prompted to call APIs in an iterative setting [49]. Training
multi-modal interactive LLMs [50] is an intriguing approach to the important open
question of whether embodied experience may improve language models’ (linguis-
tic) world-modelling capabilities, as discussed in Chapter 2. Finally, as discussed
briefly in Appendix A, environment research would benefit enormously from more
powerful computational libraries to support coding environments, similar to the
pivotal role played by PyTorch and Tensorflow for modelling research.
The understated role of environments in the success of LLMs
The impressive performance of LLMs, while challenging prior assumptions on the
limitations of distributional NLU approaches, at the same time also validate the
central premise of ecolgical NLU that the importance of modelling tends to be
overrated, and that environments matter far more than previously expected. This
can be seen from the relative simplicity of current LLM architecture and training
methods, which for the most part are Transformer models [51] pre-trained on next-
word prediction tasks, with multi-head attention as employed in the GPT line of
models [52]. Subsequent modelling breakthroughs have been relatively much less
important than simply scaling parameters and dataset size and diversity. Lifting
the hood on dataset size and diversity, we find: environments. Platforms like Red-
dit and Wikipedia form the backbone of LLM training datasets, but also happen
to be diverse, carefully constructed online environments to which humans have
collectively contributed a vast array of linguistic knowledge. We’ll try to make
the point more concrete with a thought experiment: imagine a parallel, pre-LLM
world (circa 2018), in which Stack Overflow and GitHub do not exist. In this
world, a group of AI researchers wants to develop an AI coding assistant that
104
can understand code and help write it. How would they go about solving the
problem? The group would perhaps consider recruiting experts to craft a dataset
of questions and answers, and would likely focus a lot of attention on modelling:
elaborate neuro-symbolic architectures, relational reasoning modules, graph-based
representations, etc. They would in all likelihood notundertake what we know
with hindsight to be the most critical move, which is, to build platforms like Stack
Overflow and Github . Those platforms would collect the quality human inter-
actions at the scale necessary for training a relatively simple pre-trained coding
assistant LLM like Codex [53]. The challenges involved in building that platform
would be human and engineering challenges, not modelling challenges; incentives,
UX, scale, moderation, and so on. For this reason, that route would be overlooked
by AI researchers, who are experts in building models, not environments. While
this story may be somewhat contrived, the question still stands: how many other
AI or NLU problems that we as a community are trying to solve with fancy new
modelling approaches, are actually more about building the right human interac-
tion environments?
Environments and models matter for society
Chapter 7 raises the future directions I personally am most excited about, as it
draws on that “environment vs model” question and applies it to real-world so-
cietal challenges of misinformation, polarization and information overload. Big
tech companies would like us to think that their corporate LLM conversational
agents are a one-stop shop for our epistemic needs, but many serious warnings
have been raised about these model-centric solutions and their harms [54, 55]. We
asked instead, what is the “Github of collective sensemaking” that will support the
meaningful human interactions that will power future sensemaking assistants? We
found another environment bottleneck: no such environments currently exist. The
nearest approximation can be found in social media platforms such as Twitter,
which, while demonstrating the occasional brilliance of AI-augmented collective
human intelligence, also reveal the perils of centralization and opaque data and
algorithms. We outlined a more human-centered, decentralized approach to collec-
tive sensemaking: we proposed empowering people with semantic web annotation
105
tools to mark, review and link between content, and using AI and social trust
graphs to leverage relevant “digital trails” left by others.
In summary, this thesis began with a search for better NLU models and is
ending as the start of a new search for better environments. With data-driven
algorithmic models rapidly gaining influence on human societies across all spheres
of life, seeing the often invisible environments at the nexus of humans, models
and data has perhaps never been more urgent. It is my hope that this thesis will
encourage other AI researchers to step back and take the ecological perspective,
or, to paraphrase Mace: to “ask not what’s inside your model, but what your
model’s inside of.”
106
Bibliography
[1] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss,
G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Win-
ter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,
C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, “Lan-
guage models are few-shot learners,” 2020.
[2] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds,
M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, L. Campbell-Gillingham,
J. Uesato, P.-S. Huang, R. Comanescu, F. Yang, A. See, S. Dathathri,
R. Greig, C. Chen, D. Fritz, J. S. Elias, R. Green, S. Mokra, N. Fernando,
B. Wu, R. Foley, S. Young, I. Gabriel, W. Isaac, J. Mellor, D. Hassabis,
K. Kavukcuoglu, L. A. Hendricks, and G. Irving, “Improving alignment of
dialogue agents via targeted human judgements,” 2022.
[3] OpenAI, “Gpt-4 technical report,” 2023.
[4] M. Mitchell, “Why ai is harder than we think,” 2021.
[5] E. Bender and A. Lascarides, Linguistic Fundamentals for Natural Language
Processing II: 100 Essentials from Semantics and Pragmatics . Synthesis Lec-
tures on Human Language Technologies, Springer International Publishing,
2022.
[6] D. C. Mollo and R. Milli` ere, “The vector grounding problem,” 2023.
107
[7] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang,
Y. Chen, L. Wang, A. T. Luu, W. Bi, F. Shi, and S. Shi, “Siren’s song in the
ai ocean: A survey on hallucination in large language models,” 2023.
[8] G. Gendron, Q. Bao, M. Witbrock, and G. Dobbie, “Large language models
are not abstract reasoners,” 2023.
[9] M. Mitchell, “Can Large Language Models Reason? —
aiguide.substack.com.” https://aiguide.substack.com/p/
can-large-language-models-reason . [Accessed 19-09-2023].
[10] J. Carney, “Thinking avant la lettre: A review of 4e cognition,” Evolutionary
studies in imaginative culture , vol. 4, no. 1, pp. 77–90, 2020.
[11] P. W. Fink, P. S. Foo, and W. H. Warren, “Catching fly balls in virtual reality:
A critical test of the outfielder problem,” Journal of Vision , vol. 9, pp. 14–14,
12 2009.
[12] D. G. Kelty-Stephen, P. E. Cisek, B. De Bari, J. Dixon, L. H. Favela, F. Has-
selman, F. Keijzer, V. Raja, J. B. Wagman, B. J. Thomas, and M. Man-
galam, “In search for an alternative to the computer metaphor of the mind
and brain,” Jun 2022.
[13] G. Lakoff and M. Johnson, “The metaphorical structure of the human con-
ceptual system,” Cognitive science , vol. 4, no. 2, pp. 195–208, 1980.
[14] G. Lakoff and M. Johnson, Metaphors we live by . University of Chicago Press,
2008.
[15] E. Dodge and G. Lakoff, “Image schemas: From linguistic analysis to neural
grounding,” From perception to meaning: Image schemas in cognitive linguis-
tics, pp. 57–91, 2005.
[16] R. Harris, The Linguistics Wars: Chomsky, Lakoff, and the Battle Over Deep
Structure . Oxford University Press, 2021.
[17] S. Harnad, “The symbol grounding problem,” Physica D: Nonlinear Phenom-
ena, vol. 42, no. 1, pp. 335 – 346, 1990.
108
[18] P. Vincent-Lamarre, A. B. Mass´ e, M. Lopes, M. Lord, O. Marcotte, and
S. Harnad, “The latent structure of dictionaries,” Topics in Cognitive Science ,
vol. 8, no. 3, p. 625–659, 2016.
[19] J. Duan, S. Yu, H. L. Tan, H. Zhu, and C. Tan, “A survey of embodied ai:
From simulators to research tasks,” IEEE Transactions on Emerging Topics
in Computational Intelligence , vol. 6, no. 2, pp. 230–244, 2022.
[20] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and acting
in partially observable stochastic domains,” Artificial intelligence , vol. 101,
no. 1-2, pp. 99–134, 1998.
[21] J. B. Hamrick, “Analogues of mental simulation and imagination in deep
learning,” Current Opinion in Behavioral Sciences , vol. 29, p. 8–16, 2019.
[22] N. Sambasivan, S. Kapania, H. Highfill, D. Akrong, P. Paritosh, and L. M.
Aroyo, ““everyone wants to do the model work, not the data work”: Data
cascades in high-stakes ai,” in Proceedings of the 2021 CHI Conference on
Human Factors in Computing Systems , CHI ’21, (New York, NY, USA), As-
sociation for Computing Machinery, 2021.
[23] A. Rogers, “Changing the world by changing the data,” in Proceedings of the
59th Annual Meeting of the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language Processing (Volume
1: Long Papers) , (Online), pp. 2182–2194, Association for Computational
Linguistics, Aug. 2021.
[24] I. v. Rooij, O. Guest, F. G. Adolfi, R. d. Haan, A. Kolokolova, and P. Rich,
“Reclaiming ai as a theoretical tool for cognitive science,” Aug 2023.
[25] A. Zador, S. Escola, B. Richards, B. ¨Olveczky, Y. Bengio, K. Boahen,
M. Botvinick, D. Chklovskii, A. Churchland, C. Clopath, et al. , “Catalyzing
next-generation artificial intelligence through neuroai,” Nature communica-
tions , vol. 14, no. 1, p. 1597, 2023.
[26] C. Kulkarni, W. Xu, A. Ritter, and R. Machiraju, “An annotated corpus
for machine reading of instructions in wet lab protocols,” in Proceedings of
109
the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 2 (Short
Papers) , (New Orleans, Louisiana), pp. 97–106, Association for Computa-
tional Linguistics, June 2018.
[27] S. Mysore, Z. Jensen, E. Kim, K. Huang, H.-S. Chang, E. Strubell, J. Flani-
gan, A. McCallum, and E. Olivetti, “The materials science procedural text
corpus: Annotating materials synthesis procedures with shallow semantic
structures,” in Proceedings of the 13th Linguistic Annotation Workshop ,
pp. 56–64, 2019.
[28] P. L. Lee and B. N. Miles, “Autoprotocol driven robotic cloud lab enables
systematic machine learning approaches to designing, optimizing, and discov-
ering novel biological synthesis pathways,” in SIMB Annual Meeting 2018 ,
SIMB, 2018.
[29] B. Miles and P. L. Lee, “Achieving reproducibility and closed-loop automa-
tion in biological experimentation with an iot-enabled lab of the future,”
SLAS TECHNOLOGY: Translating Life Sciences Innovation , vol. 23, no. 5,
pp. 432–439, 2018. PMID: 30045649.
[30] P. Stenetorp, S. Pyysalo, G. Topi´ c, T. Ohta, S. Ananiadou, and J. Tsujii,
“Brat: A web-based tool for nlp-assisted text annotation,” in Proceedings of
the Demonstrations at the 13th Conference of the European Chapter of the
Association for Computational Linguistics , EACL ’12, (USA), p. 102–107,
Association for Computational Linguistics, 2012.
[31] J. Weston, A. Bordes, S. Chopra, and T. Mikolov, “Towards ai-complete
question answering: A set of prerequisite toy tasks,” in 4th International
Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico,
May 2-4, 2016, Conference Track Proceedings (Y. Bengio and Y. LeCun, eds.),
2016.
[32] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert pre-
training approach,” 2019.
110
[33] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified
text-to-text transformer,” Journal of Machine Learning Research , vol. 21,
no. 140, pp. 1–67, 2020.
[34] N. F. Liu, T. Lee, R. Jia, and P. Liang, “Do question answering modeling
improvements hold across benchmarks?,” 2023.
[35] S. Storks, Q. Gao, Y. Zhang, and J. Chai, “Tiered reasoning for intuitive
physics: Toward verifiable commonsense language understanding,” in Find-
ings of the Association for Computational Linguistics: EMNLP 2021 , (Punta
Cana, Dominican Republic), pp. 4902–4918, Association for Computational
Linguistics, Nov. 2021.
[36] L. Marsh and C. Onof, “Stigmergic epistemology, stigmergic cognition,” Cog-
nitive Systems Research , vol. 9, no. 1-2, pp. 136–149, 2008.
[37] N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jiang, B. Y. Lin, P. West, C. Bhaga-
vatula, R. L. Bras, J. D. Hwang, S. Sanyal, S. Welleck, X. Ren, A. Ettinger,
Z. Harchaoui, and Y. Choi, “Faith and fate: Limits of transformers on com-
positionality,” 2023.
[38] J. Xiang, T. Tao, Y. Gu, T. Shu, Z. Wang, Z. Yang, and Z. Hu, “Language
models meet world models: Embodied experiences enhance language models,”
2023.
[39] L. Qiu, Y. Zhao, Y. Liang, P. Lu, W. Shi, Z. Yu, and S.-C. Zhu, “Towards
socially intelligent agents with mental state transition and human value,”
inProceedings of the 23rd Annual Meeting of the Special Interest Group on
Discourse and Dialogue , (Edinburgh, UK), pp. 146–158, Association for Com-
putational Linguistics, Sept. 2022.
[40] Z. Wang, G. Zhang, K. Yang, N. Shi, W. Zhou, S. Hao, G. Xiong, Y. Li,
M. Y. Sim, X. Chen, Q. Zhu, Z. Yang, A. Nik, Q. Liu, C. Lin, S. Wang,
R. Liu, W. Chen, K. Xu, D. Liu, Y. Guo, and J. Fu, “Interactive natural
language processing,” 2023.
111
[41] T. Sumers, S. Yao, K. Narasimhan, and T. L. Griffiths, “Cognitive architec-
tures for language agents,” 2023.
[42] T. Li, V. Gupta, M. Mehta, and V. Srikumar, “A logic-driven framework
for consistency of neural models,” in Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , (Hong
Kong, China), pp. 3924–3935, Association for Computational Linguistics,
Nov. 2019.
[43] N. Nanda, A. Lee, and M. Wattenberg, “Emergent linear representations in
world models of self-supervised sequence models,” 2023.
[44] Z. Wang, K. Cruse, Y. Fei, A. Chia, Y. Zeng, H. Huo, T. He, B. Deng,
O. Kononova, and G. Ceder, “Ulsa: unified language of synthesis actions for
the representation of inorganic synthesis protocols,” Digital Discovery , vol. 1,
no. 3, p. 313–324, 2022.
[45] N. H. Park, M. Manica, J. Born, J. L. Hedrick, T. Erdmann, D. Y. Zubarev,
N. Adell-Mill, and P. L. Arrechea, “Artificial intelligence driven design of cat-
alysts and materials for ring opening polymerization using a domain-specific
language,” Nature Communications , vol. 14, p. 3686, Jun 2023.
[46] Z. Zeng, Y.-C. Nie, N. Ding, Q.-J. Ding, W.-T. Ye, C. Yang, M. Sun, W. E,
R. Zhu, and Z. Liu, “Transcription between human-readable synthetic de-
scriptions and machine-executable instructions: an application of the latest
pre-training technology,” Chemical Science , 2023.
[47] A. M. Bran, S. Cox, A. D. White, and P. Schwaller, “Chemcrow: Augmenting
large-language models with chemistry tools,” 2023.
[48] R. Wang, G. Todd, E. Yuan, Z. Xiao, M.-A. Cˆ ot´ e, and P. Jansen, “Byte-
sized32: A corpus and challenge task for generating task-specific world models
expressed as text games,” 2023.
112
[49] T. Schick, J. Dwivedi-Yu, R. Dess` ı, R. Raileanu, M. Lomeli, L. Zettlemoyer,
N. Cancedda, and T. Scialom, “Toolformer: Language models can teach them-
selves to use tools,” 2023.
[50] J. Lin, Y. Du, O. Watkins, D. Hafner, P. Abbeel, D. Klein, and A. Dragan,
“Learning to model the world with language,” 2023.
[51] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in
Neural Information Processing Systems (I. Guyon, U. V. Luxburg, S. Ben-
gio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, eds.), vol. 30,
Curran Associates, Inc., 2017.
[52] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Lan-
guage models are unsupervised multitask learners,” 2019.
[53] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Ka-
plan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri,
G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray,
N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,
F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-
Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin,
S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam,
V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati,
K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever,
and W. Zaremba, “Evaluating large language models trained on code,” 2021.
[54] C. Shah and E. M. Bender, “Situating search,” in ACM SIGIR Conference on
Human Information Interaction and Retrieval , CHIIR ’22, (New York, NY,
USA), p. 221–232, Association for Computing Machinery, 2022.
[55] J. Saunders, “Surveillance graphs: Vulgarity and cloud orthodoxy in linked
data infrastructures,” May 2023.
113
Appendix A
Ecological Semantics:
Programming Environments for
Situated Language Understanding
Ronen Tamari, Gabriel Stanovsky, Dafna Shahaf, Reut Tsar-
faty
Preprint published electronically, 2020. (DOI: 10.48550/arXiv.2003.04567)
114
Published as a workshop paper at “Bridging AI and Cognitive Science” (ICLR 2020)
ECOLOGICAL SEMANTICS : PROGRAMMING ENVIRON -
MENTS FOR SITUATED LANGUAGE UNDERSTANDING
Ronen Tamari & Dafna Shahaf
The Hebrew University of Jerusalem
{ronent,dshahaf }@cs.huji.ac.ilGabriel Stanovsky & Reut Tsarfaty
Allen Institute for Artiﬁcial Intelligence
{gabis,reutt}@allenai.org
ABSTRACT
Large-scale natural language understanding (NLU) systems have made impressive
progress: they can be applied ﬂexibly across a variety of tasks, and employ mini-
mal structural assumptions. However, extensive empirical research has shown this
to be a double-edged sword, coming at the cost of shallow understanding: infe-
rior generalization, grounding and explainability. Grounded language learning ap-
proaches offer the promise of deeper understanding by situating learning in richer,
more structured training environments, but are limited in scale to relatively nar-
row, predeﬁned domains. How might we enjoy the best of both worlds: grounded,
general NLU? Following extensive contemporary cognitive science, we propose
treating environments as “ﬁrst-class citizens” in semantic representations, worthy
of research and development in their own right. Importantly, models should also
be partners in the creation and conﬁguration of environments, rather than just ac-
tors within them, as in existing approaches. To do so, we argue that models must
begin to understand and program in the language of affordances (which deﬁne
possible actions in a given situation) both for online, situated discourse compre-
hension, as well as large-scale, ofﬂine common-sense knowledge mining. To this
end we propose an environment-oriented ecological semantics, outlining theoret-
ical and practical approaches towards implementation. We further provide actual
demonstrations building upon interactive ﬁction programming languages.
1 I NTRODUCTION
“Ask not what’s inside your head, but what your head’s inside of.” (Mace, 1977)
Recovery of meaning is at the heart of the endeavor to build better natural language understanding
(NLU) systems. Semantics researchers study meaning representation, and in particular the relations
between language and cognitive representations (G ¨ardenfors, 2014).
A recurring point of contention in semantics research (Fodor & Pylyshyn, 1988; Mahon & Cara-
mazza, 2008) concerns the degree to which knowledge representation and language comprehension
involve a symbolic internal language of thought (LoT) (Fodor, 1975) or are embodied ; i.e., grounded
in the brain’s systems for action and perception (Feldman & Narayanan, 2004; Barsalou, 2007).
Current deep-learning methods for large-scale NLU, such as BERT (Devlin et al., 2018), incorporate
minimal cognitive biases and assume primarily distributional semantics (Firth, 1957). Extensive em-
pirical research shows this to be a double-edged sword: while affording widespread applicability to
a variety of tasks, such methods are limited by impoverished training environments (static datasets,
narrow contextual prediction, etc.) and struggle in settings requiring deeper understanding, such
as systematic generalization (Lake et al., 2019; McCoy et al., 2019), common-sense (Forbes et al.,
2019) and explainability (Gardner et al., 2019).
Contemporary cognitive science can be seen as adopting a more holistic approach; integrating sym-
bolic, embodied and distributional accounts (Lupyan & Lewis, 2019), but also focusing on the
crucial ecological component (Gibson, 1979; Hasson et al., 2020): cognition emerges from brain-
body-environment interaction. Systematic regularities in the interactions play a key role in inducing
systematic linguistic (Narayanan, 1997) and knowledge (Davis et al., 2020) representations. These
interactional regularities differ in fundamental ways from statistical regularities available to current
1
Published as a workshop paper at “Bridging AI and Cognitive Science” (ICLR 2020)
general NLU methods (Hasson et al., 2020), for example including perceptual, spatiotemporal and
causal dynamics (Rodd, 2020; Davis et al., 2020).
Situated (grounded) approaches (Mikolov et al., 2015; Liang, 2016) focus on mapping language
to executable forms, and highlight the importance of external environments (McClelland et al.,
2019); Hill et al. (2020) show the emergence of systemic generalization to be contingent on careful
task/environment design, rather than speciﬁc architectural engineering alone. However, while such
environments clearly play an important role in building NLU systems, they are (1) relatively narrow
and ﬁxed in terms of semantics (2) costly to create, especially multi-modal environments.
Here we propose an approach to address this limitation and extend grounded language approaches
towards more general domains, by harnessing the power of language to also create and shape envi-
ronments, rather than just to induce literal execution within them. In this important, yet relatively
unexplored role, language helps structure semantic knowledge and serves as a proxy for expensive
embodied experience (Lupyan & Bergen, 2016). To efﬁciently accomplish this remarkable feat, hu-
mans use the language of affordances (Gibson, 1979; Glenberg, 2008) to construct “mental worlds”;
shaping interactions by specifying what can be done in various situations, from concrete to abstract.
We propose that NLU systems should learn to understand (parse) and use such language (e.g., “This
bag can hold up to 20kg before bursting”, see §2), which we suggest has a natural programmatic
equivalent in the behavioral programming paradigm, such as interactive ﬁction languages.
In summary, we make the following more concrete contributions and proposals:
•Ecological Semantics: Outline for a theoretical and practical approach to a semantic parsing
framework for creation as well as interaction with environments through language. Design con-
siderations are informed by contemporary cognitive science, AI/NLU research and programming
language theory (PLT).
•We propose methods to inject rich, actionable external knowledge into the framework at scale,
building upon data mining and automated knowledge base construction (AKBC) research.
•We make available1simple interactive demonstrations as working examples showing how such
methods can be applied towards open challenges such as common-sense and causal reasoning.
2 M OTIVATING CHALLENGES : INCORPORATING WORLD KNOWLEDGE
Explicitly Provided Knowledge. Consider the example in ﬁgure 1, describing an everyday sit-
uation of shopping for fruit in a market. Completely trivial for humans, current NLU methods
ﬁnd such “what-if” questions highly challenging, even though the relevant affordances are made
explicit in the text. A textual entailment model judges it very likely that “The bag bursts.” for
X∈{no,one,two,three}2.
Assumed World Knowledge. In this common, yet more difﬁcult setting, the relevant knowledge
is implicitly assumed. Consider a prompt like “He put on a white t-shirt and blue jeans. Next, he
wore ”. A completion produced by GPT-2 (Radford et al., 2019) is “a gray cowboy hat, black cargo
pants, and white shoes. He also had a black baseball cap pulled low over his eyes”3.
Common-sense knowledge graphs are likely to be insufﬁcient for such problems; as shown in Forbes
et al. (2019), “neural language representations still only learn associations that are explicitly written
down”, even after being explicitly trained on a knowledge graph of objects and affordances. As
suggested by the work, mental simulations are crucial to common-sense in humans (Battaglia et al.,
2013), allowing the dynamic, affordance-guided construction of relevant representations at run-time
as needed, rather than wasting valuable space in memorizing large, ever-incomplete relation graphs.
Importantly, the ﬁrst problem should be simpler than the second: the required background knowl-
edge is made available in the text. It would be highly desirable to be able to act upon such informa-
tion. Recent work has begun to explore such capabilities (Zhong et al., 2020), but current methods
are largely limited in this respect (Luketina et al., 2019). In the following section, we propose a gen-
eral problem formulation for incorporating affordances, building upon cognitive linguistics theory.
1https://eco-sem.github.io/
2https://demo.allennlp.org/textual-entailment/
3https://talktotransformer.com/
2
Published as a workshop paper at “Bridging AI and Cognitive Science” (ICLR 2020)
Figure 1: Inform7 ecological semantic parsing example for §2 challenge. (1) Input prompt (2)
Pre-existing, compiled knowledge (3) Situation knowledge: simulation conﬁguration and indexical-
ization of referent objects (4) Run simulation to answer “what-if” question.
3 E COLOGICAL SEMANTICS
Mental simulations and affordances feature centrally in contemporary cognitive linguistics research.
According to one such theory, the Indexical Hypothesis (Glenberg, 2008), language comprehension
involves three key processes: (1) indexing objects, (2) deriving their affordances, and (3) meshing
them together into a coherent (action-based) simulation as directed by grammatical cues. Impor-
tantly, affordances generally cannot be derived directly from words, but rather rely on context and
pre-existing object representations.
3.1 C OMPUTATIONAL FORMULATION
The Indexical Hypothesis (IH) can be formulated naturally within the model-based framework used
in general AI mental simulation research (Hamrick, 2019). At the core of such frameworks is the
partially observable Markov Decision Process (POMDP) (Kaelbling et al., 1998), which governs the
relations between states ( s), actions (a), observations ( o) and rewards ( r). Speciﬁcally, we focus on
the recognition4I:O→S, transitionT:S×A→Sand policyπ:S→Afunctions.
Pre-existing knowledge regarding the environment (objects and their affordances) can be seen to
be primarily represented by T, with the emulator model being the neural correlate (Grush, 2004;
Glenberg, 2008). In the POMDP formulation, for a linguistic input (or observation) x, IH can be
formulated as (1) compose an initial state representation I(x) =s0of objects (we assume the
simple case where all objects are mentioned in x) (2) derive affordances, or the set of actions that
can be taken in the current situation (3) enact mental simulation by applying Twith chosen action.
Typically xis composed of multiple utterances (¯x1,...,¯xN)and so the simulation may be composed
of multiple actions a= (a0,...,a L−1). Slightly abusing notation, we can denote the full execution
T(s0,a)which yields a result state sL. IH can be seen as corresponding to the standard setting in
executable semantic parsing/grounded NLU works (Long et al., 2016):
Executable Semantic Parsing (Ex-SP). Given a linguistic input xand target intent (goal state)
g∗, output action sequence asuch thatT(I(x),a) =g∗. Most grounded/executable approaches
assume a ﬁxed, programmatic, domain-speciﬁc T(navigation environments, SQL engine, etc.) and
focus on learning a policy mapping from xtoa.
Our proposal thus focuses on “pushing the envelope” of Tto allow grounded understanding of more
general language. IH discusses the comprehension process in cases where the relevant object and
affordance information already exists. But how do we learn such representations in the ﬁrst place?
Embodied experience is one way, but a costly and slow one, so here we focus on the role of language
in shaping affordance knowledge, speciﬁcally modal language, like “All watermelons are portable.”
4Commonly denoted O−1, we denote here by Ifor Indexicalization.
3
Published as a workshop paper at “Bridging AI and Cognitive Science” (ICLR 2020)
Such language can more naturally be seen as modifying5the emulator T. Therefore, we propose
extending the representation of Tto allow it to change in time, T(t), modiﬁed by special eco-actions
´a. These do not change the current state, but rather only the executor (example in ﬁg. 1). We denote
regular executed actions as `a, and a scenario (containing possibly both `a,´aactions) as ˇa. The full
execution is then T(0:L−1)(s0,ˇa), which denotes applying T(t)at each timestep.
Ecological Semantic Parsing (Ec-SP). Given a linguistic input xand target intent (goal state) g∗,
output action sequence ˇasuch thatT(0:L−1)(I(x),ˇa) =g∗.
Figure 1 shows how Ec-SP can be utilized towards addressing the challenge problem from §2, which
is not handled by current Ex-SP methods, as the input language is out-of-domain (so a speciﬁc
executor would need to be created). The implementation uses Inform7 (Nelson, 2005), an interactive
ﬁction (IF) language (see §4). Interactive versions of the examples from §2 are available online.
We distinguish between compiled knowledge vs. situation knowledge: the former refers to existing
knowledge encompassed by the emulator (analogous to code libraries that just need to be imported),
the latter is new knowledge deﬁned online using eco-acts (analogous to writing a new program).
Clearly, a core issue to be managed is the scalable and incremental growth of the emulator: as in
regular programming, recurring ecological information (such as watermelons being portable) should
become part of the library, rather than having to be re-deﬁned anew in every situation.
4 A FFORDABLE AFFORDANCES : TOWARDS IMPLEMENTATION
Programmatic emulation of environments requires an appropriate programming formalism with
which environments can be ﬂexibly constructed and conﬁgured6. Our focus here is on purely text
based construction, from considerations of scale, to remain broadly applicable to general NLU;
multi-modal integration is an interesting future direction. We suggest that a natural paradigm for
such a purpose is Behavioral Programming (Harel et al., 2012), which can also be seen to include
certain IF languages, like Inform7 (Nelson, 2005). These languages are designed to be reminiscent
of natural language, and express semantics in terms of interactional affordances (indeed often using
modal verbs like can,mustn’t ) (Harel et al., 2012). Current frameworks for creating custom IF train-
ing environments (C ˆot´e et al., 2018; Tamari et al., 2019) require extensive re-conﬁguration for new
domains, and games must be pre-compiled rather than generated dynamically from textual inputs.
Most current IF works focus on solving existing games (Jain et al., 2019) or game construction for
human entertainment (Ammanabrolu et al., 2020).
Learning emulators at large-scale. This task is closely related to the grand AI challenge of
common-sense learning. In humans, common-sense is hard-coded through rich experience (Has-
son et al., 2020); it is reasonable to expect that approximating human emulators will require ex-
tensive hard-coding as well. In rendering this task tractable, We join Kordjamshidi et al. (2018) in
advocating a tighter loop between learning and programming to represent knowledge: AI should
be extensively utilized in hard-coding its own common-sense. Whereas earlier approaches typically
consisted of non-executable, relational knowledge graphs (KGs) (Speer et al., 2017), in our case
knowledge can be represented by code, executable in interactive simulations. KGs will likely be
useful for populating an initial “seed emulator”, as will AKBC methods for learning object (Elazar
et al., 2019) and action (Forbes & Choi, 2017) properties at scale. In Pustejovsky & Krishnaswamy
(2018), multimodal simulations are used to evaluate automatic affordance extraction. In Balint &
Allbeck (2017), game designers (for human games) utilized NLU methods for learning object affor-
dances. Finally, as symbolic knowledge is by nature incomplete, it will need to be superseded by
geometric, multi-modal knowledge representations (G ¨ardenfors, 2014; Pezeshkpour et al., 2018).
By affording NLU systems with the ability to programmatically emulate environments in the context
of both online discourse comprehension, as well as large-scale, ofﬂine common-sense knowledge
mining, we hope to advance research efforts towards grounded, general NLU.
5This is a delicate point- we currently assume the modiﬁcation is valid, and leave a more thorough discussion
of the rules governing what is possible to future work.
6This preliminary approach is naturally biased towards literal language, which is easier to simulate than
more abstract language. While a detailed analysis is out of scope, we note that literal language is seen to lay
the neural foundations for abstract language understanding (Lakoff & Johnson, 1980; Davis et al., 2020)
4
Published as a workshop paper at “Bridging AI and Cognitive Science” (ICLR 2020)
ACKNOWLEDGMENTS
We thank the Hyadata Lab at HUJI, and Yoav Goldberg, Ido Dagan, and the audience of the BIU-
NLP seminar for interesting discussion and thoughtful remarks. The last author is funded by an ERC
grant on Natural Language Programming , grant number 677352, for which we are grateful. This
work was also supported by the European Research Council (ERC) under the European Union’s
Horizon 2020 research and innovation programme (grant no. 852686, SIAM) and NSF-BSF grant
no. 2017741 (Shahaf).
REFERENCES
Prithviraj Ammanabrolu, Wesley Cheung, Dan Tu, William Broniec, and Mark O. Riedl. Bringing
stories alive: Generating interactive ﬁction worlds. CoRR , abs/2001.10161, 2020. URL http:
//arxiv.org/abs/2001.10161 .
J. Timothy Balint and Jan Allbeck. ALET: Agents Learning their Environment through Text. Com-
puter Animation and Virtual Worlds , 28(3-4):1–9, 2017. ISSN 1546427X. doi: 10.1002/cav.1759.
Lawrence W. Barsalou. Grounded Cognition. Annual Review of Psychology , 59(1):617–645, 2007.
ISSN 0066-4308. doi: 10.1146/annurev.psych.59.103006.093639.
Peter W. Battaglia, Jessica B. Hamrick, and Joshua B. Tenenbaum. Simulation as an engine of phys-
ical scene understanding. Proceedings of the National Academy of Sciences of the United States
of America , 110(45):18327–18332, 2013. ISSN 00278424. doi: 10.1073/pnas.1306572110.
Marc-Alexandre C ˆot´e,´Akos K ´ad´ar, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine,
James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy
Tay, and Adam Trischler. Textworld: A learning environment for text-based games. CoRR ,
abs/1806.11532, 2018.
Charles P. Davis, Gerry T. M. Altmann, and Eiling Yee. Situational systematicity: A role
for schema in understanding the differences between abstract and concrete concepts. Cogni-
tive Neuropsychology , pp. 1–12, jan 2020. ISSN 0264-3294. doi: 10.1080/02643294.2019.
1710124. URL https://www.tandfonline.com/doi/full/10.1080/02643294.
2019.1710124 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Yanai Elazar, Abhijit Mahabal, Deepak Ramachandran, Tania Bedrax-Weiss, and Dan Roth. How
Large Are Lions? Inducing Distributions over Quantitative Attributes. 2019. URL http://
arxiv.org/abs/1906.01327 .
Jerome Feldman and Srinivas Narayanan. Embodied meaning in a neural theory of language. Brain
and Language , 89(2):385–392, 2004. ISSN 0093934X. doi: 10.1016/S0093-934X(03)00355-9.
John R Firth. A synopsis of linguistic theory, 1930-1955. Studies in linguistic analysis , 1957.
Jerry A Fodor. The language of thought , volume 5. Harvard university press, 1975.
Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analy-
sis.Cognition , 1988.
Maxwell Forbes and Yejin Choi. VERB PHYSICS: Relative physical knowledge of actions and
objects. ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics,
Proceedings of the Conference (Long Papers) , 1:266–276, 2017. doi: 10.18653/v1/P17-1025.
Maxwell Forbes, Ari Holtzman, and Yejin Choi. Do neural language representations learn physical
commonsense? Proceedings of the 41st Annual Conference of the Cognitive Science Society ,
2019.
P. G¨ardenfors. The Geometry of Meaning: Semantics Based on Conceptual Spaces . The MIT Press.
MIT Press, 2014. ISBN 9780262026789. URL https://books.google.co.il/books?
id=QDOkAgAAQBAJ .
5
Published as a workshop paper at “Bridging AI and Cognitive Science” (ICLR 2020)
Matt Gardner, Jonathan Berant, Hannaneh Hajishirzi, Alon Talmor, and Sewon Min. On making
reading comprehension more comprehensive. In Proceedings of the 2nd Workshop on Machine
Reading for Question Answering , pp. 105–112, Hong Kong, China, November 2019. Association
for Computational Linguistics. doi: 10.18653/v1/D19-5815. URL https://www.aclweb.
org/anthology/D19-5815 .
James Jerome Gibson. The ecological approach to visual perception. 1979.
Arthur M. Glenberg. Toward the Integration of Bodily States, Language, and Ac-
tion. In Gun R. Semin and Eliot R. Smith (eds.), Embodied Grounding , pp. 43–
70. Cambridge University Press, Cambridge, 2008. ISBN 9780511805837. doi: 10.
1017/CBO9780511805837.003. URL https://www.cambridge.org/core/product/
identifier/CBO9780511805837A010/type/book{_}part .
Rick Grush. The emulation theory of representation: Motor control, imagery, and percep-
tion. Behavioral and Brain Sciences , 27(3):377–396, 2004. ISSN 0140525X. doi: 10.1017/
S0140525X04000093.
Jessica B Hamrick. Analogues of mental simulation and imagination in deep learning. Current
Opinion in Behavioral Sciences , 2019. ISSN 23521546. doi: 10.1016/j.cobeha.2018.12.011.
David Harel, Assaf Marron, and Gera Weiss. Behavioral programming. Communications of the
ACM , 55(7):90–100, 2012. ISSN 00010782. doi: 10.1145/2209249.2209270.
Uri Hasson, Samuel A. Nastase, and Ariel Goldstein. Direct Fit to Nature: An Evolutionary Per-
spective on Biological and Artiﬁcial Neural Networks. Neuron , 105(3):416–434, 2020. ISSN
10974199. doi: 10.1016/j.neuron.2019.12.002. URL https://doi.org/10.1016/j.
neuron.2019.12.002 .
Felix Hill, Andrew Lampinen, Rosalia Schneider, Stephen Clark, Matthew Botvinick, James L.
McClelland, and Adam Santoro. Environmental drivers of systematicity and generalization in a
situated agent. In International Conference on Learning Representations , 2020. URL https:
//openreview.net/forum?id=SklGryBtwr .
Vishal Jain, William Fedus, Hugo Larochelle, Doina Precup, and Marc G. Bellemare. Algorithmic
Improvements for Deep Reinforcement Learning applied to Interactive Fiction. 2019. URL
http://arxiv.org/abs/1911.12511 .
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in
partially observable stochastic domains. Artiﬁcial intelligence , 101(1-2):99–134, 1998.
Parisa Kordjamshidi, Dan Roth, and Kristian Kersting. Systems AI: A declarative learning based
programming perspective. IJCAI International Joint Conference on Artiﬁcial Intelligence , 2018-
July:5464–5471, 2018. ISSN 10450823.
Brenden Lake, Tal Linzen, and Marco Baroni. Human few-shot learning of compositional instruc-
tions. In Ashok Goel, Colleen Seifert, and Christian Freksa (eds.), Proceedings of the 41st Annual
Conference of the Cognitive Science Society , pp. 611–616. Cognitive Science Society, Montreal,
Canada, 2019.
George Lakoff and Mark Johnson. The metaphorical structure of the human conceptual system.
Cognitive science , 4(2):195–208, 1980.
Percy Liang. Learning executable semantic parsers for natural language understanding. Communi-
cations of the ACM , 59(9):68–76, 2016. ISSN 15577317. doi: 10.1145/2866568.
Reginald Long, Panupong Pasupat, and Percy Liang. Simpler context-dependent logical forms via
model projections. 54th Annual Meeting of the Association for Computational Linguistics, ACL
2016 - Long Papers , 3:1456–1465, 2016. doi: 10.18653/v1/p16-1138.
Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefen-
stette, Shimon Whiteson, and Tim Rockt ¨aschel. A Survey of Reinforcement Learning Informed
by Natural Language. jun 2019. URL http://arxiv.org/abs/1906.03926 .
6
Published as a workshop paper at “Bridging AI and Cognitive Science” (ICLR 2020)
Gary Lupyan and Benjamin Bergen. How Language Programs the Mind. Topics in Cognitive
Science , 8(2):408–424, 2016. ISSN 17568765. doi: 10.1111/tops.12155.
Gary Lupyan and Molly Lewis. From words-as-mappings to words-as-cues: the role of lan-
guage in semantic knowledge. Language, Cognition and Neuroscience , 34(10):1319–1337,
nov 2019. ISSN 2327-3798. doi: 10.1080/23273798.2017.1404114. URL https://www.
tandfonline.com/doi/full/10.1080/23273798.2017.1404114 .
William M Mace. James j. gibson’s strategy for perceiving: Ask not what’s inside your head, but
what’s your head inside of. Perceiving, acting, and knowing: Towards an ecological psychology ,
1977.
Bradford Z. Mahon and Alfonso Caramazza. A critical look at the embodied cognition hypothesis
and a new proposal for grounding conceptual content. Journal of Physiology Paris , 102(1-3):
59–70, 2008. ISSN 09284257. doi: 10.1016/j.jphysparis.2008.03.004.
James L. McClelland, Felix Hill, Maja Rudolph, Jason Baldridge, and Hinrich Sch ¨utze. Extending
Machine Language Models toward Human-Level Language Understanding. pp. 1–8, 2019. URL
http://arxiv.org/abs/1912.05877 .
R. Thomas McCoy, Jung-Hyun Min, and Tal Linzen. Berts of a feather do not generalize to-
gether: Large variability in generalization across models with similar test set performance. ArXiv ,
abs/1911.02969, 2019.
Tomas Mikolov, Armand Joulin, and Marco Baroni. A roadmap towards machine intelligence.
ArXiv , abs/1511.08130, 2015.
Srinivas Narayanan. Knowledge-based Action Representations for Metaphor and Aspect (KARMA) .
PhD dissertation, The University of California, 1997.
Graham Nelson. Natural Language, Semantic Analysis and Interactive Fiction. IF Theory
Reader , (April 2005):141–188, 2005. URL http://inform-fiction.org/manual/
if{_}theory.html .
Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. Embedding multimodal relational data
for knowledge base completion. In Proceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing , pp. 3208–3218, Brussels, Belgium, October-November
2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1359. URL https:
//www.aclweb.org/anthology/D18-1359 .
James Pustejovsky and Nikhil Krishnaswamy. Every Object Tells a Story. In Proceedings of
the Workshop Events and Stories in the News 2018 , pp. 1–6, Santa Fe, New Mexico, U.S.A,
aug 2018. Association for Computational Linguistics. URL https://www.aclweb.org/
anthology/W18-4301 .
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019.
Jennifer M. Rodd. Settling Into Semantic Space: An Ambiguity-Focused Account of Word-Meaning
Access. Perspectives on Psychological Science , pp. 174569161988586, jan 2020. ISSN 1745-
6916. doi: 10.1177/1745691619885860. URL http://journals.sagepub.com/doi/
10.1177/1745691619885860 .
Robert Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of
general knowledge. In Thirty-First AAAI Conference on Artiﬁcial Intelligence , 2017.
Ronen Tamari, Hiroyuki Shindo, Dafna Shahaf, and Yuji Matsumoto. Playing by the book: An
interactive game approach for action graph extraction from text. In Proceedings of the Work-
shop on Extracting Structured Knowledge from Scientiﬁc Publications , pp. 62–71, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-2609.
URL https://www.aclweb.org/anthology/W19-2609 .
Victor Zhong, Tim Rockt ¨aschel, and Edward Grefenstette. Rtfm: Generalising to new environment
dynamics via reading. In International Conference on Learning Representations , 2020. URL
https://openreview.net/forum?id=SJgob6NKvH .
7
122
