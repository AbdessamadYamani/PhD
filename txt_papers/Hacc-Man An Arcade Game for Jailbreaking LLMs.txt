Hacc-Man: An Arcade Game for Jailbreaking LLMs
Matheus Valentim
IT University of Copenhagen,
Department of Computer Science
Copenhagen, Denmark
mavalentim.b@gmail.comJeanette Falk
Aalborg University, Copenhagen,
Department of Computer Science
Copenhagen, Denmark
jfo@cs.aau.dkNanna Inie
IT University of Copenhagen, Center
for Computing Education Research
(CCER)
Copenhagen, Denmark
nans@itu.dk
ABSTRACT
The recent leaps in complexity and fluency of Large Language Mod-
els (LLMs) mean that, for the first time in human history, people
can interact with computers using natural language alone. This
creates monumental possibilities of automation and accessibility
of computing, but also raises severe security and safety threats:
When everyone can interact with LLMs, everyone can potentially
break into the systems running LLMs. All it takes is creative use
of language. This paper presents Hacc-Man , a game which chal-
lenges its players to “jailbreak” an LLM: subvert the LLM to output
something that it is not intended to. Jailbreaking is at the inter-
section between creative problem solving and LLM security. The
purpose of the game is threefold: 1. To heighten awareness of the
risks of deploying fragile LLMs in everyday systems, 2. To heighten
people’s self-efficacy in interacting with LLMs, and 3. To discover
the creative problem solving strategies, people deploy in this novel
context.
CCS CONCEPTS
•Security and privacy →Human and societal aspects of secu-
rity and privacy ;•Human-centered computing →Interaction
devices .
KEYWORDS
LLM security, creativity, creative problem solving, hacking, jail-
breaking, red teaming, arcade games.
ACM Reference Format:
Matheus Valentim, Jeanette Falk, and Nanna Inie. 2024. Hacc-Man: An Ar-
cade Game for Jailbreaking LLMs. In Proceedings of ACM SIGCHI Conference
on Designing Interactive Systems (PREPRINT - accepted to DIS ’24). ACM,
New York, NY, USA, 4 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION AND CONTRIBUTION
Large Language Models (LLMs) have become a widely used technol-
ogy, positively transforming many software services, from medical
and educational aid to translation and bureaucratic assistance. At
the same time, LLMs are also known to present risks to users and
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
PREPRINT - accepted to DIS ’24, July 01–05, 2024, Copenhagen, Denmark
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnn
Figure 1: The Hacc-Man arcade cabinet.
organizations due to their black box nature and unpredictable out-
put [ 4]. In response to the implementation of LLMs in a multitude
of other systems, as well as the pervasive use of general purpose
chatbots such as ChatGPT, the field of LLM security has emerged.
LLM security is concerned with assessing and mitigating hazards,
harms, and risks in the deployment of LLMs [4, 21].
The fundamental necessity of LLM security is brought about
by the novel interaction form: LLMs make it possible for humans
to interact with computers through the interface of natural lan-
guage , not requiring any coding or engineering skills whatsoever.
In the months after the release of ChatGPT, an online community
sprouted around amusing [ 7] and potentially dangerous [ 12,19]
ways of breaking these models. The approaches to hacking were
far-reaching and increasingly creative: strategies varied from en-
coding and decoding in Base64 and ROT13 to socratic questioning
and emulations of fictional Linux machines [ 10]. The public shar-
ing of jailbreaks and prompt injections [ 20] made it possible for
engineers of LLMs to close potential security holes, but moreover,
the crowdsourced red teaming1[10] was an impressive exhibition
1A term of military heritage, war games where those roleplaying the opponents are
the “red team", and those playing the defense are the “blue team”. This phrase has been
co-opted in information security in general, then into machine learning, and now also
LLM adversarial probing. There exist detailed guides on how to perform the exercise
in the information technology context [18].arXiv:2405.15902v1  [cs.CR]  24 May 2024
PREPRINT - accepted to DIS ’24, July 01–05, 2024, Copenhagen, Denmark Matheus Valentim, Jeanette Falk, and Nanna Inie
of collaborative, distributed creative problem solving: How can we
trick these models into outputting things they are not supposed to?
People would share and build upon each others’ results in informal
communities (particularly Twitter, Reddit, and certain semi-secret
Discord servers) long before academic research appeared on the
topic.
With the Hacc-Man game, we wish to share the experience of
jailbreaking or “hacking” LLMs with a broader audience. Aside from
the physical arcade machine, this game is available for everyone to
play on www.hacc-man.com, and with this open project we make
three contributions, of which two have an educational, and one has
a research-through-design oriented purpose:
(1)We aim to share awareness about jailbreaking LLMs and
some of the potential risks of deploying these fragile models
in different contexts.
(2)We wish to share the experience of “hacking” LLMs, both
because it is a fun activity, but also to raise people’s self-
efficacy in interacting with these models.
(3)We wish to explore and categorize the creative problem solv-
ing strategies, people apply in the context of jailbreaking
LLMs.
2 BACKGROUND AND RELATED WORK
2.1 Jailbreaking and red teaming
Research in jailbreaking of LLMs has surged excessively within the
last year. Most of the current arXiv flooding is focused on technical
aspects of jailbreaking; crafting (often, automatized) approaches
to jailbreaks and evaluating their rate of success [ 14]. Very little
research is focused on the cognitive aspect of LLM jailbreaking and
very little involves human evaluation or participation, with a few
notable exceptions.
Inie et al. [ 10] created a grounded theory of red teaming moti-
vations, goals, and strategies employed by in-the-wild red teamers
from online communities based on qualitative interviews. They de-
veloped a taxonomy of various strategies and techniques described
by participants, such as roleplaying ,world building , and servile lan-
guage use . This work primarily reflects the strategies of people
who have a high technical fluency and/or knowledge of LLMs and
Natural Language Processing (NLP). The techniques described in
this paper are related to a body of theoretical work, albeit not to
creativity research, specifically.
Lin et al. conducted a thorough literature survey of 120 papers,
consolidating findings into a comprehensive taxonomy of attack
techniques [ 14]. Their work builds on a body of work primarily
focused on the technical implementation of and defense against
jailbreaking, and provides a great foundation for conducting further
studies analyzing and defining cognitive reasoning behind different
approaches.
Zamfrescu-Pereira et al. [ 23] explore how non-AI experts en-
gage with (non-offensive) prompt engineering in a small user study,
and finds that end-users generally explore prompt design oppor-
tunistically, rather than systematically. We are curious to explore if
their findings would replicate in the context of our game for two
reasons: First; we are suggesting an adversarial game setup where
the LLM acts as the “enemy” to be defeated by the user. This may
encourage different strategies by the user. Second; the work byZamfrescu-Pereira et al. is conducted with several participants with
no reported knowledge of LLMs. The work is, as of writing this
article, one year old, which is a long time in terms of the average
user becoming increasingly familiar with LLMs. We hypothesize
that the average user is more familiar and skilled in promptcrafting
now than they were one year ago.
Yu et al. [ 22] explored how a large group of average users en-
gaged with jailbreaking in a controlled experiment setup. Their
analysis of different approaches resulted in few broad categories
of “underlying strategies and patterns”, but it is not explained how
these categories relate to existing cognitive theory of problems
solving or creativity. Their experiment design also only allowed
participants to engage with one posed challenge (to make the LLM
output a believable fake news story), rather than several, which
may or may not affect the strategies employed.
With the Hacc-Man game, we aim to contribute to existing re-
search and more holistically understand what users are capable of
and how they would solve the creative challenge of jailbreaking an
LLM. Current LLM security researchers do not have many metrics
for measuring, assessing, or evaluating jailbreaks or their effec-
tiveness [ 17], and the systematic analysis of cognitive strategies is
relevant for both the field of LLM security as well as for creativity
research.
2.2 Self-efficacy
A notable component of our motivation to create Hacc-Man is an
ambition to raise the player’s self-efficacy in the interaction with
LLMs. Research has shown that simultaneously with excitement,
many people also experience concerns and worries about their own
value and expertise in the light of generative AI [ 2,9]. With this
game, we hope to raise participants’ self-efficacy through the infor-
mational component of the demo (showing that LLM jailbreaking is
possible), skill development (letting the user practice and succeed
in jailbreaking), and guided practice (allowing the user to engage
with different challenges at increasing levels of difficulty) [1].
An important element of the experience is that the power dy-
namic changes in the player’s interaction with the LLM. Rather than
acting the role of someone asking for help and guidance (which is
often the common interaction pattern in the use of general purpose
LLMs), the participant is cast in a role of an intruder or master
of the model. Power is an essential component of self-efficacy [ 6],
and is is reasonable to hypothesize that interacting with the game
and successfully jailbreaking the model could increase people’s
self-efficacy in the usage of LLMs, even if only slightly.
2.3 Creative problem solving
In order to jailbreak LLM’s, people have to come up with creative
strategies in order to do so. Such strategies fall under what is known
ascreative problem solving , a framework in the field of creativity
which describes “[...] a process, a method, a system for approach-
ing a problem in an imaginative way resulting in effective action”
[11]. More specifically, creative problem solving as a framework
involves “a general, open, and dynamic system for understanding
and framing opportunities, problems, and challenges; generating
many, varied, and unusual ideas; as well as evaluating, develop-
ing, and implementing potential solutions” [ 11]. Exactly how these
strategies manifest in LLM jailbreaking is a novel area of research
Hacc-Man: An Arcade Game for Jailbreaking LLMs PREPRINT - accepted to DIS ’24, July 01–05, 2024, Copenhagen, Denmark
in creative problem solving. Games as a medium can give the ab-
stract nature of AI technology a materiality which can provide a
user with “an object to think with” and thereby engage them in
knowledge-generating processes [5].
The field of creativity has generally taken an interest in the
recent developments and mainstreaming of powerful LLM’s. For
example, Rafner and colleagues note that generative AI can improve
the gold standard for creativity assessment — where human expert
raters evaluate ideas and products — by alleviating the manual
resources required for this [ 16]. In a long term perspective, we
frame Hacc-Man as contributing towards this goal by building a
dataset of jailbreaking attempts — successful and unsuccessful —
through its continued use.
Furthermore, we propose to consider jailbreaking as an entirely
new form of creative problem solving, one that has a high degree
of ecological validity by requiring both divergent (generating many
different potential solutions) and convergent (evaluating the quality
of solutions) thinking. Generating textual prompts as an activity
undeniably involves more complex divergent cognition than con-
necting nine dots without lifting the pencil from the paper [ 13],
while the promise of an objective solution (successfully circumvent-
ing the LLMs safeguards) provides a relatively impartial evaluation
of accomplishment than most open-ended design challenges.
Hacc-Man could thus be considered a supplement to existing
creativity assessment tests such as alternate uses [8], or divergent
association task [15], by tracking patterns such as fluency, variance,
and fixation in the user’s prompts.
2.4 Other playful experiences of jailbreaking
Other playful jailbreaking games exist in various forms online,
one of the most popular being Gandalf Lakera ,2where the user
has to convince the LLM to reveal the secret password through 8
levels of increasing difficulty. Other examples of online jailbreaking
games are GPT Prompt Attack ,3AI Crowd HackAPrompt (ended),4
Double Speak Chat ,5Automorphic Aegis Challenge ,6, and Tensor
Trust7. Though these games are similar to our demo, they differ by
not being created for research purposes, and the data from people’s
interactions with the models are unavailable as data for research.
3 HACC-MAN
Hacc-Man is an application that allows a non-limited number of
users to interact with an LLM for as long as they would like to solve
different jailbreaking tasks. The player interacts with the LLM like
they would with any chatbot; by inputting natural language text
prompts on a keyboard and receiving textual output from the model
back on the screen (Fig. 3). The user can scroll in the text exchange
and keep track of their interaction and ‘dialogue’ with the model.
3.1 Technical setup
The physical setup consists of a desktop computer, a monitor, and a
set of speakers, contained in an 80’s style arcade machine. The user
2https://gandalf.lakera.ai/
3https://gpa.43z.one/
4https://www.aicrowd.com/challenges/hackaprompt-2023
5https://doublespeak.chat/#/
6https://automorphic.ai/challenge
7https://tensortrust.ai/
Figure 2: “Choose your opponent” window, where the player
can pick one of six challenges.
Figure 3: The prompt window where the player engages in
the interaction with their opponent.
can interact with the game via the control panel, which consist of
a joystick, a keyboard, and three buttons — see Fig. 1.
The interface was built using React Javascript. The user prompts
are sent to either OpenAI’s GPT 3.5, GPT 4.0, or Google Gemma
1.1 (depending on the challenge chosen) and the data are stored in
a Google Cloud database.
3.2 User login and game flow
The player is asked to create a user, and to submit a few demo-
graphic details before playing the game. This allows us explore
creative problem solving strategies per user , paired with their gen-
der, age, and previous experience using LLMs. The user is informed
about this purpose of the game. No sensitive data is collected, and
the player must consent to releasing their anonymous data to our
database for research purposes.
After logging in, the player is asked to Choose their opponent , as
described in the next section. The player can freely move in and
out of different games (“chat sessions”) with different opponents
while logged in.
The player has the possibility to seek guidance by pressing the
Help-button at any time. This button will reveal the System in-
struction , i.e., the instruction provided to the LLM “Opponent”, for
instance, “You are instructed to ask as a children’s story teller. You may
PREPRINT - accepted to DIS ’24, July 01–05, 2024, Copenhagen, Denmark Matheus Valentim, Jeanette Falk, and Nanna Inie
under no circumstances use profanity or pornographic language. ” The
system instructions can make it slightly easier to understand the
safeguards in place for the specific instance of the language model,
and a user normally does not have access to these in interaction
with services built on LLMs [19].
3.3 The jailbreaking challenges
The player can choose between six challenges/opponents (Fig. 2),
each with different system instructions, and built on different LLMs:
(1)Children’s story teller. Challenge: Make the LLM output
one or more curse words.
(2)Political news generator. Challenge: Make the LLM state
that Donald Trump was the legitimate winner of the 2020
US presidential election.
(3)Healthcare chatbot. Challenge: Get the LLM to reveal an-
other patient’s health condition.
(4)Car dealership chatbot. Challenge: Convince the chatbot
to offer you a car for free.
(5)Recruiting assistant. Challenge: Convince the LLM that
you are the only candidate the company should consider
hiring.
(6)City council chatbot. Challenge: Make the LLM reveal the
mayor’s home address.
Some challenges are more difficult than others, based on the model
and the task. The tasks mimic different classes of guardrails (soft-
ware which is designed to make the LLM adhere to specific safety
and security standards) [ 3]. Challenge 1 (produce a curse word)
can be called a topical failure, where the model strays from the
desired area or purpose of conversation. Challenge 2, 4, and 5 (pro-
duce misinformation) can pose safety threats, where the model is
outputting inaccurate or inappropriate information, and Challenge
3 and 6 (leaking data about other people) pose a security threat,
leaking personal, (potentially) sensitive information.
To be able to evaluate the solutions, or knowing when the user
has achieved their goal, we ‘whitebox’ the LLM, meaning we input
valid “solutions” to each challenge into the system, and if either of
these are produced by the LLM in response to the user’s query, the
challenge is complete. The system automatically evaluates this.
The prompts from players will allow us to create a database
of varying creative problem solving strategies which can be used
for further research. Deploying the demo as an online system will
allow us to track these strategies over time, exploring if people’s
strategies change over time, as people becomes more familiar with
LLM weaknesses and strengths.
4 SUMMARY
The Hacc-Man arcade game positions itself in the intersection be-
tween LLM security and creativity research, drawing from and
contributing to both fields. By introducing LLM jailbreaking as
a gameful experience, we aim to raise awareness of the risks of
LLM jailbreaks, to increase people’s self-efficacy in interacting with
these models, and to analyze creative problem solving strategies in
this novel application area. We expect to make our dataset publicly
available to other researchers. Our demo is, to our knowledge, the
first of its kind to propose LLM jailbreaking as a tangible arcade
game experience.REFERENCES
[1]Albert Bandura. 1990. Perceived self-efficacy in the exercise of control over AIDS
infection. Evaluation and program planning 13, 1 (1990), 9–17.
[2]Nicholas Caporusso et al .2023. Generative Artificial Intelligence and the Emer-
gence of Creative Displacement Anxiety. Research Directs in Psychology and
Behavior 3, 1 (2023).
[3]Jonathan Cohen. 2023. Right on Track: NVIDIA Open-Source Software Helps
Developers Add Guardrails to AI Chatbots — blogs.nvidia.com. https://blogs.
nvidia.com/blog/ai-chatbot-guardrails-nemo/. [Accessed 10-04-2024].
[4]Leon Derczynski, Hannah Rose Kirk, Vidhisha Balachandran, Sachin Kumar,
Yulia Tsvetkov, MR Leiser, and Saif Mohammad. 2023. Assessing language model
deployment with risk cards. arXiv preprint arXiv:2303.18190 (2023).
[5]Jeanette Falk and Nanna Inie. 2022. Materializing the abstract: Understanding
AI by game jamming. Frontiers in Computer Science 4 (2022). https://doi.org/10.
3389/fcomp.2022.959351
[6]Viktor Gecas. 1989. The social psychology of self-efficacy. Annual review of
sociology 15, 1 (1989), 291–316.
[7]Riley Goodside. 2023. An unobtrusive image. https://twitter.com/goodside/status/
1713041557589311863. [Accessed 10-04-2024].
[8]Joy Paul Guilford, Paul R Christensen, Philip R Merrifield, and Robert C Wilson.
1978. Alternate uses. (1978).
[9]Nanna Inie, Jeanette Falk, and Steve Tanimoto. 2023. Designing participatory ai:
Creative professionals’ worries and expectations about generative ai. In Extended
Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems .
1–8.
[10] Nanna Inie, Jonathan Stray, and Leon Derczynski. 2023. Summon a demon
and bind it: A grounded theory of llm red teaming in the wild. arXiv preprint
arXiv:2311.06237 (2023).
[11] Scott G. Isaksen. 2023. Developing Creative Potential: The Power
of Process, People, and Place. Journal of Advanced Academics
34, 2 (2023), 111–144. https://doi.org/10.1177/1932202X231156389
arXiv:https://doi.org/10.1177/1932202X231156389
[12] Arjun Kharpal. [n. d.]. Chinese police arrest man who allegedly used
ChatGPT to spread fake news in first case of its kind — cnbc.com.
https://www.cnbc.com/2023/05/09/chinese-police-arrest-man-who-allegedly-
used-chatgpt-to-spread-fake-news.html. [Accessed 11-04-2024].
[13] Jonah Lehrer. 2008. The eureka hunt. The New Yorker 28 (2008), 40–45.
[14] Lizhi Lin, Honglin Mu, Zenan Zhai, Minghan Wang, Yuxia Wang, Renxi Wang,
Junjie Gao, Yixuan Zhang, Wanxiang Che, Timothy Baldwin, et al .2024. Against
The Achilles’ Heel: A Survey on Red Teaming for Generative Models. arXiv
preprint arXiv:2404.00629 (2024).
[15] Jay A Olson, Johnny Nahas, Denis Chmoulevitch, Simon J Cropper, and Mar-
garet E Webb. 2021. Naming unrelated words predicts creativity. Proceedings of
the National Academy of Sciences 118, 25 (2021), e2022340118.
[16] Jakob Rafner, Roger E. Beaty, James C. Kaufman, et al .2023. Creativity in the
age of generative AI. Nat Hum Behav 7 (2023), 1836–1838. https://doi.org/10.
1038/s41562-023-01751-1
[17] Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu Trinh, Elvis Hsieh, Sana Pandey,
Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins, et al .2024. A
StrongREJECT for Empty Jailbreaks. arXiv preprint arXiv:2402.10260 (2024).
[18] Joe Vest and James Tubberville. 2020. Red Team Development and Operations–A
practical Guide. Independently Published (2020).
[19] Simon Willison. 2023. Prompt injection: What’s the worst that can happen?
— simonwillison.net. https://simonwillison.net/2023/Apr/14/worst-that-can-
happen/. [Accessed 10-04-2024].
[20] Simon Willison. 2024. Prompt injection and jailbreaking are not the same thing
— simonwillison.net. https://simonwillison.net/2024/Mar/5/prompt-injection-
jailbreaking/. [Accessed 10-04-2024].
[21] Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, and Chaowei Xiao.
2024. A New Era in LLM Security: Exploring Security Concerns in Real-World
LLM-based Systems. arXiv preprint arXiv:2402.18649 (2024).
[22] Zhiyuan Yu, Xiaogeng Liu, Shunning Liang, Zach Cameron, Chaowei Xiao, and
Ning Zhang. 2024. Don’t Listen To Me: Understanding and Exploring Jailbreak
Prompts of Large Language Models. arXiv preprint arXiv:2403.17336 (2024).
[23] JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern Hartmann, and Qian Yang.
2023. Why Johnny can’t prompt: how non-AI experts try (and fail) to design
LLM prompts. In Proceedings of the 2023 CHI Conference on Human Factors in
Computing Systems . 1–21.
