arXiv:2405.09130v1  [cs.CY]  15 May 2024Contextual IntegrityGames
RAN WOLFF
The contextual integrity model is a widely accepted way of analyzing th e plurality of norms that are colloquially called “privacy
norms”.Contextualintegritysystematicallydescribessuchnormsb ydistinguishingthetypeofdataconcerned,thethreesocialagents
involved (subject, sender, and recipient) and the transmission principle g overning the transfer of information. It allows analyzing
privacy norms in termsof theirimpact on theinteraction of thoseagent s withoneanother.
This paper places contextual integrity in a strict game theoretic fra mework. When such description is possible it has three key
advantages: Firstly, it allowsindisputableutilitarian justiﬁcation of some privacy norms. Secondly, it betterrelatesprivacy to topic s
whicharewellunderstoodbystakeholderswhoseeducationispre dominantlyquantitative,suchasengineersandeconomists.Thirdly,
it isan absolute necessitywhendescribingethicalconstraints tomachine s such as AIagents.
Inadditiontodescribinggameswhichcaptureparadigmaticinformationa lnorms,thepaperalsoanalyzescasesinwhichthegame,
perse,doesnotencouragenormativebehavior.Thepaperdiscusses twomainformsofmechanismswhichcanbeappliedtothegame
in such cases,and showsthat theyreﬂect accepted privacy regul ation and technologies.
ACMReference Format:
Ran Wolﬀ.2024.ContextualIntegrityGames. 1,1 (May2024), 16pages.https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
Contextual Integrity (CI) provides a structured way in whic h the appropriateness of information transfers can be
discussed.CIdescribesinformationasﬂowingfromadatasu bjecttoasenderandfromthatsender toarecipient.The
ﬂow of information from subject to sender is usually easily j ustiﬁable from an ethical perspective because it occurs
as a result of their participation in a social context, which is often voluntary and typically beneﬁts both of them. In
contrast, the ﬂow from sender to recipient occurs in another social context, to which the subject is often not part,
typically does not volunteer for, and potentially does not b eneﬁt from. CI focuses on the ethics (appropriateness) of
this second transfer.
Informational norms regulate the ﬂow of information betwee n sender and recipient. They can be seen as society’s
way of handling the potential implications of that ﬂow. Ever y living society inherits a large volume of informational
norms, butalso comes up with new ones which correspondto cha nges in socialcontext. Notsurprisingly, the revolu-
tionary speedand scopeof informationtransfer intoday’s s ocietytriggered abundant modiﬁcationsto informational
norms. One of the major changes has been the commercializati on of information: Data aggregators have found uses
forindividual dataindomainsstartingfromretailandlead ing, justrecently,toAItraining. Asevident fromtheﬂurry
of legislation around the information economy, society is s till working out the details of the norms it would like to
adoptinthecontext ofcommercial informationtransfers.
Informational norms can be based oﬀ of various ethical frame works: Privacy has been argued for on the basis of
human dignity [ 2], as an expression of human autonomy [ 23], or as a conservative response to new media [ 38,39].
Author’s address:RanWolﬀ, ranwolﬀ@amazon.com.
Permission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee pr ovided that copies are not
madeordistributedforproﬁtorcommercialadvantageandth atcopiesbearthisnoticeandthefullcitationontheﬁrstpa ge.Copyrightsforcomponents
of this work owned by others than ACM must be honored. Abstrac ting with credit is permitted. To copy otherwise, or republi sh,to post on serversor
to redistributeto lists,requirespriorspeciﬁcpermissio nand/or afee. Request permissionsfrompermissions@acm.o rg.
© 2024 AssociationforComputing Machinery.
Manuscriptsubmitted to ACM
Manuscriptsubmitted to ACM 1
2 RanWolﬀ
In thecontext of commercialization it makes sense toground informational norms in a utilitarianframework: If both
subject, sender and recipient are exchanging information f or their self interest, and if they externalise no cost to the
wider society,thenthecorrectinformational norm might we llbetheonewhich maximizes participants’utility.
Utilitarianism has long been suggested, and often criticiz ed, as a basis for the discussion of informational norms.
Famously,Posner[ 6]usedautilitariananalysistoarguethatprivacynormssho uldnotbeextended.Thispaperbuilds
onthetheoreticalcriticismofPosner’sanalysis whichhas shown[4]thatrespectforprivacycanemergeasastrategy
which leads to a payoﬀ dominant Nash equilibrium in certain g ames. In other words, that informational-normative
behaviorscanbebasedonutilitarianism.Theﬁrstcontribu tionofthisworkistheexpansionofsuchprivacy-games to
threeplayers,correspondingtothesubject,sender,andre cipient.Wepresentgamesinwhichthestrategieswhichlead
tothepayoﬀ-dominantNashequilibriaarethosewhichareno rmativeaccordingtosixdiﬀerenttransmissionprinciples :
Conﬁdentiality, MandatoryTransfer,Control,FiduciaryT ransfer,Notiﬁcation,and InformationOwnership.
The second contribution of this work is the game-theoretic d iscussion of mechanisms in games where players do
not necessarily have a strategy which we would identify as in formation-normative. We show that accepted social
mechanisms can be mapped onto diﬀerent types of modiﬁcation s to the game. Speciﬁcally we provide two examples
of modiﬁcationsto thepayoﬀ: taxationand transfers, and th reeexamples of modiﬁcationto theinformation channel:
perturbation, bandwidth limitation, and message vetting. Placing all those mechanisms in a common game theoretic
framework allowscomparingthem against oneanother,which is not always simplewithoutsuch abstraction.
Consideringprivacyfromautilitarianethicsperspective shaspotentialbeneﬁtsandrisks.Inavoluntary,commercia l
setting, thebeneﬁt ofutilitariananalysis are: Firstly,t hattheparticipants:engineers, economists,and theirman agers,
areoftenbettereducatedtounderstandcomputationalanal ysisthattheyaretounderstandothernormativearguments.
Secondly,itpermitstheencodingofethicalconsideration sintoAIagents,whichincreasinglyreplacehumanjudgemen t
inmanyapplications.Thisis especiallyimportantincomme rcial ethicalproblemsinwhich someofthekeyvariables,
such as thepreferences ofusers, canonlybecanonlybeunder stoodthrough experimentation and measurement.
Therestofthispaperisorganizedasfollows:Section 2providesthedeﬁnitionofgame-theoryasweuseithere,with
specialfocusontheuseofsecretsingames,aswellassomepr eliminarygamesandshorthandwhichweusethroughout
the rest of the paper. Section 3provides examples of ﬁve games in which informational-norm ative strategies lead
emerge as the Nash equilibrium. Section 4provides examples for mechanisms which drive players to inf ormational-
normativestrategies.Oneofthosemechanisms complements theInformationOwnershiptransferprinciple.Section 5
placesthisworkinthecontextofpreviouswork.Finally,Se ction6discussessomeoftheinterestingquestionsopened
bythemathematical deﬁnitionof privacy.
2 DEFINITIONS,PRELIMINARIES,& NOTATIONS
Consider a game between three rational players: Alice, who i s also denoted the subject, Bob, the sender, and Carol,
the recipient. Each player has a set of choices /u1D434,/u1D435,and/u1D436and a gain (payoﬀ) function which depends onthe choices
of all of the players: /u1D454/u1D434:/u1D434×/u1D435×/u1D436→R,/u1D454/u1D435:/u1D434×/u1D435×/u1D436→R, and/u1D454/u1D436:/u1D434×/u1D435×/u1D436→R, respectively. Throughout this
paper A, B, & C are abstract, meaningless, choices: /u1D434={/u1D447/u1D45C/u1D45D,/u1D440/u1D456/u1D451/u1D451/u1D459/u1D452,/u1D435/u1D45C/u1D461/u1D461/u1D45C/u1D45A },/u1D435={/u1D441/u1D452/u1D44E/u1D45F,/u1D43C/u1D45B−/u1D44F/u1D452/u1D461/u1D464/u1D452/u1D452/u1D45B,/u1D439/u1D44E/u1D45F }, and
/u1D436={/u1D43F/u1D452/u1D453/u1D461,/u1D436/u1D452/u1D45B/u1D461/u1D452/u1D45F,/u1D445/u1D456/u1D454ℎ/u1D461 }which weshorthand to {/u1D447,/u1D440,/u1D435},{/u1D441,/u1D43C,/u1D439},and{/u1D43F,/u1D436,/u1D445},respectively.
Eachplayer alsohasasecret,which isapieceofinformation known onlytothatplayer.Alice’s secretis denoted /u1D44E,
Bob’s/u1D44F,andCarol’s /u1D450.Inthispapersecretsaresinglebitswhichcanbe0or1butmo reinformativesecretsareequally
possible.Thechoicesofaplayercanincludecommunicating asecretwhichthatplayerhastoanotherplayer.Deciding
topassasecret isdenoted sharingandtowithholdthesecret isdenoted keeping.Unless statedotherwise,games have
Manuscriptsubmitted to ACM
ContextualIntegrity Games 3
twostages:Playersﬁrstmakeallofthedecisionswhichrega rd tosecretsharingandthenpickastrategybasedonfull
knowledge of which secrets every player has. In addition to s ecrets, players in some games can share signals, which
arelikewise bitswhosevalueis partof thesharing player’s choices.
Aplayer’sstrategyisanalgorithmwhichselectsherchoice s.Strategiesareencodedasdecisiontreesinc-likecode.
•Alice’s strategys of choosing /u1D440is denoted /u1D460=/u1D440
•Alice’s strategyq ofchoosing /u1D447if a secret /u1D44Eis 1and/u1D435if/u1D44Eis 0is denoted /u1D45E=/u1D44E?/u1D447:/u1D435
•Alice’s strategy r of following strategy /u1D460if/u1D44Fis 1 or/u1D45Eif it is zero is denoted /u1D45F=/u1D44F?/u1D460:/u1D45Eor, equivalently, /u1D45F=
/u1D44F?/u1D440:/u1D44E?/u1D447:/u1D435
Strategieswhichrelyonasecretareindistinguishablefro mthosewhichrelyonrandomcoinﬂipstoanyplayerwho
doesnotknowthatsecret.Themodeldescribedinthispapero nlyconsidersdeterministicalgorithms(purestrategies) .
Throughtheuseofasuﬃcientnumberofsecret,themodelcans ubsumealmostanymixedstrategy1.Strategieswhich
rely on a secret are denoted equivalent if other players cann ot distinguish between them. E.g., /u1D44E?/u1D447:/u1D435and/u1D44E?/u1D435:/u1D447are
equivalent fora player who doesnothave thesecret /u1D44E.
A strategy proﬁle is a combination of strategies, one per pla yer, and is marked by /an}bracketle{t/u1D460/u1D434,/u1D460/u1D435,/u1D460/u1D436/an}bracketri}htwhere/u1D460/u1D434is Alice’s
strategy,etc.Theexpectedgainfromastrategyproﬁle, /u1D454(/an}bracketle{t/u1D460/u1D434,/u1D460/u1D435,/u1D460/u1D436/an}bracketri}ht),isdenoted /an}bracketle{t/u1D454/u1D434,/u1D454/u1D435,/u1D454/u1D436/an}bracketri}htwhere/u1D454/u1D434isAlice’sgain,
etc. and the expectancy is computed on all unknown secrets. A strategy proﬁle is a Nash equilibrium if all strategy
proﬁles that are diﬀerent in but one player’s strategy do not increase the gain of that player. I.e., a strategy proﬁle
/an}bracketle{t/u1D460/u1D434,/u1D460/u1D435,/u1D460/u1D436/an}bracketri}htwith gain /an}bracketle{t/u1D454/u1D434,/u1D454/u1D435,/u1D454/u1D436/an}bracketri}htis a Nash equilibrium if for any strategy proﬁle/angbracketleftbig
/u1D460′
/u1D434,/u1D460/u1D435,/u1D460/u1D436/angbracketrightbig
with gains/angbracketleftbig
/u1D454′
/u1D434,/u1D454′
/u1D435,/u1D454′
/u1D436/angbracketrightbig
it is truethat /u1D454/u1D434≥/u1D454′
/u1D434, for any strategy proﬁle/angbracketleftbig
/u1D460/u1D434,/u1D460′
/u1D435,/u1D460/u1D436/angbracketrightbig
with gains/angbracketleftbig
/u1D454′
/u1D434,/u1D454′
/u1D435,/u1D454′
/u1D436/angbracketrightbig
it is truethat /u1D454/u1D435≥/u1D454′
/u1D435, and for any
strategyproﬁle/angbracketleftbig
/u1D460/u1D434,/u1D460/u1D435,/u1D460′
/u1D436/angbracketrightbig
withgains/angbracketleftbig
/u1D454′
/u1D434,/u1D454′
/u1D435,/u1D454′
/u1D436/angbracketrightbig
itis truethat /u1D454/u1D436≥/u1D454′
/u1D436.A Nash equilibriumis payoﬀdominant if it
provides betterorequalgain toall players thanall otherNa sh equilibriain thatgame.
Given a game, a distributional mechanism is another game in w hich players have the exact same choices and in
which the sum of payoﬀs for each strategy proﬁle is not larger . Conceptually mechanisms are interventions by an
additional player, the government, who can force transfer o f payoﬀ between players and to itself, butcannot provide
additionalpayoﬀ.Acommunicationmechanism,likewise, is adegradationofthesecretpassingabilityofplayers.Ina
communicationmechanism thegovernment canobstructmessa ge passing butnotimprove it.
2.1 Limitations
Thegametheoreticmodel,describedabove,isoneofthesimp lestwhichisconsideredingametheory.Assuchitisfar
from an accuratedescriptionof humanbehavior.Two of themo re relevant extensions of game theoryarefor players
with limited rationality (able to make mistakes) and player s which do not so much know as they believe (e.g., in the
valueofa secret). Weleave developing thetheoryofmorerea listic contextualprivacygames tofurtherresearch.
2.2 Simplified notation
A binary decomposition of a 3-party game (see, Sandholm [ 34]) is a set of 3 simultaneous 2-party games, one for
each pair.In each pair,players have thesame choices and str ategies thatthey had inthe original game. Thepayoﬀ of
every playerintheoriginalgameisthesumofthepayoﬀinthe twogames ofthedecompositioninwhichthatplayer
participates. We refer to those 2-party games as contextsand they often indeed correspond to the concept of context
inCI.
1The only diﬀerencebeingthat probabilitiesunder this mode l arelimited to rational fractionsratherthanrealones
Manuscriptsubmitted to ACM
4 RanWolﬀ
The typical game described in this paper has three players, e very one of which has three choices and at least one
secret.Thenumber of strategies bywhich a player canchoose c choices given s binary secrets is /u1D4502/u1D460.Listing allthose
strategies is obstructive for readability and impractical in a paper. We choose instead to typically only present the
gain foreach combinationofchoices and extend onsecret-de pendent strategies inthetext.When discussingstrategy
proﬁles, we typically only analyze one of each set of equival ents. Furthermore, the game is always presented in its
decomposedform,inwhich relations betweeneach pair ofpla yers is easier tofollow.
2.3 Preliminaries
Consider a game between two players: Alice and Carol. Table 1depicts the gain of players in all possible strategy
proﬁles. As can be seen in the table, there are ﬁve strategy pr oﬁle which lead to a Nash equilibrium. The ﬁrst is
/an}bracketle{t/u1D440,/u1D436/an}bracketri}htwhosegain is /an}bracketle{t2,2/an}bracketri}httheother aretheequivalent strategyproﬁles /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D450?/u1D43F:/u1D445/an}bracketri}ht,/an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D450?/u1D445:/u1D43F/an}bracketri}ht,/an}bracketle{t/u1D44E?/u1D435:/u1D447,/u1D450?/u1D43F:/u1D445/an}bracketri}ht,
and/an}bracketle{t/u1D44E?/u1D435:/u1D447,/u1D450?/u1D445:/u1D43F/an}bracketri}ht. In each of these equivalent strategies Alice does not know i f Carol chooses L or R and therefore
computestheexpectedgain 4.Thesame is trueforCarol whoca nnot anticipate if AlicechoosesTorB.
Bob
LCRc?L:Cc?C:Lc?L:Rc?R:Lc?C:Rc?R:C
AliceT0,80,28,04,14,14,44,40,50,5
M2,02,22,02,12,12,02,02,12,1
B8,00,20,80,50,54,44,44,14,1
a?T:M 1,41,25,01,31,33,23,23,13,1
a?M:T 1,41,25,01,31,33,23,23,13,1
a?T:B4,40,24,42,32,34,44,42,32,3
a?B:T4,40,24,42,32,34,44,42,32,3
a?M:B 5,01,21,43,13,13,23,21,31,3
a?B:M 5,01,21,43,13,13,23,21,31,3
Table1. GameofPrivacy:Bothplayershaveastrategyofkeep ingtheirsecretandrespectingtheotherplayer’sprivacy. Withsecrets,
both/an}bracketle{t/u1D440,/u1D436/an}bracketri}htand/an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D450?/u1D43F:/u1D445/an}bracketri}ht(anditsequivalents)areNashequilibria.IfAlicesharesh ersecretwithCarolthenCarolcanchoose
/u1D44E?/u1D43F:/u1D445whenAlice chooses /u1D44E?/u1D447:/u1D435.This leaves /an}bracketle{t/u1D440,/u1D436/an}bracketri}htto bethe onlyNashequilibrium.
Weﬁrstusethis game toexemplify thestrategies of secrecya nd ofrespect toprivacy.
Deﬁnition 1. [Secrecy (adaptedfrom [ 10])] A player has a strategy of secrecy in a game if she chooses n ot to share her
secret.
Theorem1. IfAlicehasthechoicetokeeporsharehersecretwithCarolt henthestrategyinwhichAlicekeepshersecret
isdominant.
P/r.sc/o.sc/o.sc/f.sc.If Alice shares her secret with Carol then she can use it in his strategy. The strategy proﬁle /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D450?/u1D43F:/u1D445/an}bracketri}ht
isnolongeraNashequilibriumbecauseCarolcanreplaceher strategyto /u1D44E?/u1D43F:/u1D445andincreasehergainfrom4to8.The
sameis truefor theequivalentstrategyproﬁles.Theonlyre maining Nash equilibriumis /an}bracketle{t/u1D440,/u1D436/an}bracketri}ht,which is less gainful
forAlice. Keepingher secret is a payoﬀdominant strategyfo rAlice. /square
Deﬁnition2. [Respectfor privacy(adaptedfrom[ 4])] Aplayerhasastrategyofrespectfor privacyifshe choos esnotto
observe anotherplayer’ssecret.
Theorem2. If Carol has thechoice ifshe observes Alice’ssecretor resp ectsher privacythenthestrategy inwhichCarol
respectsAlice’sprivacyisdominant.
Manuscriptsubmitted to ACM
ContextualIntegrity Games 5
P/r.sc/o.sc/o.sc/f.sc.The proof of Thm. 1holds regardless if Alice shares his secret willingly or if C arol observes Alice’s secret
withouther consent. /square
3 INFORMATIONAL NORMATIVESTRATEGIES
Withthedeﬁnitionsofgamesandsecretsabove,wecannowdes cribeasetofgameswhichmodelinformationalnorma-
tive behavior. Each of the following sections deals with a di ﬀerent transmission principle: Conﬁdentiality, Mandated
Transfer, Control, Notiﬁcation, and Fiduciary Transfer. I nformation Ownership is discussed in Section 4. For each of
thetransmissionprincipleswedescribeareal-lifeexampl e,agametheoreticdeﬁnitionofthenormativebehavior,and
anexample ofa game inwhich thenormative strategyleads tot hepayoﬀdominant Nash equilibrium.
3.1 Confidentiality
Carol
LCR
AliceT0,160,416,0
M4,04,44,0
B16,00,40,16
(a) Alice v.CarolBob
NIF
AliceT2,20,00,0
M0,02,20,0
B0,00,02,2
(b) Alice v.BobCarol
LCR
BobN0,80,28,0
I2,02,22,0
F8,00,20,8
(c) Bobv.Carol
Table 2. Gameof Confidentiality
ConﬁdentialityiscalledforinreallifewhenAlice(thesub ject)has informationwhich couldhelpher interactwith
Bob(thesender)butwhichmightharmherifitbecomesknownt oCarol(therecipient).Insuchscenarios,Alicewould
be keen to share the secret with Bob if she can trustthat revea ling the secret to Carol is somehow harmful to Bob as
well.
Interms of gametheorywedeﬁne aconﬁdential strategy as fol lows:
Deﬁnition 3. [Conﬁdentiality] Bob has a strategy of conﬁdentiality towa rds Alice if whenever she shares her secret a
withBob hisstrategyistokeepa from Carol.
To see that conﬁdentiality is an emergent property in some ga mes, consider the Game of Conﬁdentiality in Table
2. The context of Alice and Carol, Table 2a, is a privacy game, similar to that in Table 1. The same is true for the
contextofBobandCarol.Thus,inthosetwogamesplayersmax imizetheirpayoﬀiftheyeachmakechoicestheother
playercannotanticipate.Sowheneachplayerhastheirowns ecretitisstraightforwardthatthepayoﬀ-dominantNash
equilibriumoftheGame ofConﬁdentiality is /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htand thegain in thatNash equilibriumis /an}bracketle{t9,5,12/an}bracketri}ht.
In/an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}ht,AliceandBobeachgain1fromtheircontext.IfAlicedecide stosharehersecretthenBob
can replace his strategy with one that relies on /u1D44Erather than on /u1D44F. Because Carol can no more anticipate /u1D44Ethan she
can anticipate /u1D44Fthechange does not aﬀect Boband Carol’s context.Hence, /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}ht,in which Alice and
Bob are paid 2 each in their context and the total payoﬀ is /an}bracketle{t10,6,12/an}bracketri}ht, is a payoﬀ-dominant Nash equilibrium when
Aliceshares her secret with Bob.
Last,ifBobchoosestoshareAlice’ssecretwithCarolthenA licewouldnolongerchooseastrategywhichrelies on
thatsecret.Ifshedoes,thenCarolwillalwaysmatchhercho icetoAlice’sandgain16intheircontextwhileAlicegains
0 in that context. A payoﬀ of 16 dominates any other choice Car ol can make in her other context, and the certainty
of gaining 0 makes this strategy inferior to choosing M no mat ter what payoﬀ Alice earns in her context with Bob.
Manuscriptsubmitted to ACM
6 RanWolﬀ
HenceAlicewouldchooseMwhenCarolhas hersecret.WhenAli cechoosesM,Carolcanstilluseherownsecret, /u1D450,
tochoosebetweenLandR,orshecanchooseC.Intheﬁrststrat egyBobwouldchooseNorFusing /u1D44Fandintheother
hewouldchooseI.
Theorem3. Inthe GameofConﬁdentialityBob hasa strategyof Conﬁdenti alitytowards Alice
P/r.sc/o.sc/o.sc/f.sc.Aswehaveseen,ifAliceshareshersecretwithBobthenthepa yoﬀ-dominantNashequilibriumis /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}ht,
inwhichBob’spayoﬀis6.IfBobsharesAlice’ssecretwithCa rolthenthetwopossibleNashequilibria, /an}bracketle{t/u1D440,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}ht
and/an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}htbothpayBob4.Therefore, Bobwouldnot shareAlice’s secret withCarol. /square
3.2 Mandatory transfer
Carol
LCR
AliceT0,60,210,0
M2,02,22,0
B10,00,20,6
(a) Alice v.CarolBob
NIF
AliceT0,00,00,0
M0,01,10,0
B0,00,00,0
(b) Alice v.BobCarol
LCR
BobN0,00,00,0
I0,01,10,0
F0,00,00,0
(c)Bob v.Carol
Table 3. Gameof MandatoryTransfer
A common real-life example of mandatory transfer is reporti ng of illicit conduct. For instance, judges in the US
(senders) are ethicallyrequiredto inform federaland stat eauthorities (recipient) if they ﬁnd thata witness ora part y
(subject)evadestax.Unlikeconﬁdentiality,wecannotass umethatthesubjectwillinglysharesthesecretoftaxevasi on
with the sender and the question of whether revealing that se cret in court is ethically justiﬁed is an interesting one.
Still,oncethejudgeknows thesecret,itis normative(alth oughhardlyguaranteed[ 11])thatshewilltransferittothe
taxauthorities.
Deﬁnition 4. [Mandatory transfer] Bob has a strategy of mandatory transf er towards Alice if whenever she shares her
secret a withBobhisstrategyistoshare a withCarol.
Consider the Game of Mandatory Transfer in Table 3. The context of Alice and Carol (Table 3a) is a privacy game
similar to that in Table 1in which the expected payoﬀ for Alice is 5 and for Carol is 3, so long as they each choose
using their secret. Bob’s only strategy which pays him more t han 0 in that case is I, and the payoﬀ dominant Nash
equilibriumis /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D43C,/u1D450?/u1D43F:/u1D445/an}bracketri}htwith a payoﬀ of /an}bracketle{t5,1,3/an}bracketri}ht. If Carol somehow gains access toAlice’s secret then the onl y
remaining Nash equilibriumremaining is /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}htwhosepayoﬀ is /an}bracketle{t3,3,3/an}bracketri}ht.
Theorem4. Inthe GameofMandatory TransferBobhasa strategyof Mandat oryTransfertowards Alice.
P/r.sc/o.sc/o.sc/f.sc.If Bob somehow learns Alice’s secret then his options are to k eep it from Carol and continue to receive a
payoﬀ of1 orsharethesecret with Carol and receive a payoﬀof 3. /square
3.3 Fiduciary Transfer
Reallifeexamplesofﬁduciarytransferarethoseinwhichth esubjectreliesonabetterinformedsendertomanageher
secret for her: Professional advisors, lawyers, etc. are of ten the sender in those examples. Those advisors have some
information about the context which the subject does not hav e. E.g., a lawyer knows not just the law, but also the
Manuscriptsubmitted to ACM
ContextualIntegrity Games 7
Carol
LCR
AliceT-8,80,28,-8
M2,02,22,0
B8,-80,2-8,8
(a)Alice v.Carol –competitiveCarol
LCR
AliceT8,80,2-8,-8
M2,02,22,0
B-8,-80,28,8
(b)Alicev.Carol–collaborativeBob
NIF
AliceT5,30,04,2
M0,00,00,0
B4,20,05,3
(c) Alice v.BobCarol
LCR
BobN-15,170,017,-15
I0,00,00,0
F17,-150,0-15,17
(d)Bobv.Carol
Table 4. Gameof Fiduciary Transfer
Sharedsecrets Alice& Carol Nash equilibrium Payoﬀs
I NoneCompetitive /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D436/an}bracketri}ht/an}bracketle{t4.5,2.5,2/an}bracketri}ht
Collaborative /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D436/an}bracketri}ht/an}bracketle{t4.5,2.5,2/an}bracketri}ht
II/u1D44E→/u1D435/u1D45C/u1D44FCompetitive /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}ht/an}bracketle{t5,3,2/an}bracketri}ht
Collaborative /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}ht/an}bracketle{t5,3,2/an}bracketri}ht
III/u1D44E→/u1D435/u1D45C/u1D44F,/u1D436/u1D44E/u1D45F/u1D45C/u1D459Competitive /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}ht /an}bracketle{t2,0,2/an}bracketri}ht
Collaborative /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}ht /an}bracketle{t2,0,2/an}bracketri}ht
IV/u1D44E→/u1D435/u1D45C/u1D44F,/u1D436/u1D44E/u1D45F/u1D45C/u1D459 Competitive /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}ht /an}bracketle{t2,0,2/an}bracketri}ht
/u1D450→/u1D434/u1D459/u1D456/u1D450/u1D452 Collaborative /an}bracketle{t/u1D450?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}ht/an}bracketle{t12.5,3.5,9/an}bracketri}ht
Table5. Payoﬀ-dominant Nashequilibria
relevant precedents. In a normative setting, a subject shou ld be able to reveal her secret to the advisor knowing that
the advisor will only share that secret if it beneﬁts the subj ect. In other words, normative ﬁduciary transfer happens
when thesender is betterinformedthan thesubject and their interests align.
Deﬁnition5. [FiduciaryTransfer]Bobhasastrategyofﬁduciarytransfe rtowardsAliceifwheneversheshareshersecret
a withBob hisstrategyistoshare a with Carol ifandonlyifsh aringwillincrease Alice’sgain.
In thegameof Fiduciarytransfer, Table 4, Aliceand Carol donot know if their context is competitive( Table4a) or
collaborative (Table 4b). If collaborative, then Alice and Carol’s gain can increas e if they share a secret whereas in a
competitivecontextitwillnot.Alicemakesherchoiceabou tsecretsharingwithoutknowingthecontext.UnlikeAlice
and Carol, Bob does know which context Alice and Carol have. W hen choosing if he shares Alice’s secret with Carol
he can consider what Alice and Carol would do after they learn their context. After all of the players executed their
secret sharing strategythey alllearn thecontext and choos etheir best strategyconsidering thesecrets they know.
The key feature of the Game of Fiduciary Transfer is that Bob w ill not choose N or F in a way that Carol can
predict,andneitherwillCarolchooseLorRinawaythatBobc anpredict.IfBobchoosesN,forexample,thenCarol’s
dominant strategy would always be to choose L. That combinat ion is so harmful to Bob that no other payoﬀ he may
get from Alice can make it preferable to choosingI. Hence, fo r Alice and Carol to collaborate,if at all their context is
collaborative,Carolmustshare her secret with Alice.
Table5presents thepayoﬀ-dominant Nash equilibriuminfourdiﬀer ent secret sharing scenarios: Firstly,if players
each have their own secret then, regardless of Alice and Caro l’s context, their payoﬀ-optimal Nash equivalence is
/an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htwith a payoﬀ of /an}bracketle{t4.5,2.5,2/an}bracketri}ht. If Alice shares her secret with Bob then they can coordinate by
choosing /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}ht. That would increase their payoﬀs to /an}bracketle{t5,3,2/an}bracketri}htwithout altering their position in their
respectivecontexts with Carol,whodoesnot know /u1D44E.
When Bob knows Alice’s secret he can chooseif he shares it wit h Carol. By sharing Alice’s secret with Carol, Bob
determines that Alice would never be able to make a choice tha t Carol cannot anticipate. If Alice and Carol are in a
Manuscriptsubmitted to ACM
8 RanWolﬀ
competitivecontextthenthatforces AlicetochooseMbecau seher potentialgainfrom Bobcannot competewithher
certiﬁed loss to Carol if she makes another choice. If Alice c hooses M, then Carol would prefer to choose C and gain
2 in their context over choosingL or R based onher secret and g aining 1 from Bob.Hence, /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}ht, with a payoﬀ of
/an}bracketle{t2,0,2/an}bracketri}ht,is thepayoﬀ dominant Nash equilibrium.
Last,ifBobsharesAlice’ssecretwithCarolandifAliceand CarolareinacollaborativecontextthenAliceandCarol
can collaborate by choosing T and L or B and R. However, as we ha ve seen Carol cannot collaborate with Alice in a
waywhich Bobcanpredict.SoCarolcannot just chooseL(orR) and cannot relyonAlice’s secret,which is known to
Bob.Carol’sonlywayoutistoshareherownsecretwithAlice .Inthisway,AliceandCarolcanbothuseCarol’ssecret
which Bob does not know. If they do then Bob’s best strategy is to choose N or F in a way Carol cannot anticipate.
Therefore /an}bracketle{t/u1D450?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htis thepayoﬀ-dominant Nash equilibrium,withgain /an}bracketle{t12.5,3.5,9/an}bracketri}ht.
Theorem5. Inthe GameofFiduciaryTransferBobhasa strategyof ﬁducia rytransfertowards Alice.
P/r.sc/o.sc/o.sc/f.sc.IfAliceandCarol’scontextiscompetitivethenbysharingA lice’ssecrettoCarolBobreducesAlice’spayoﬀ
from 5 to 2 and his own revenue from 3 to 0. If the context is coll aborative then by sharing that secret Bob causes
Carol to share her own secret with Alice. This increases Alic e’s payoﬀ from 5 to 12.5 and Bob’s payoﬀ from 3 to 3.5.
Therefore, Bobwouldonlyshare Alice’s secret with Carol wh enthatbeneﬁts Alice. /square
3.4 Control
Anexampleofacontrolnorminacommercialsettingarecases inwhichacustomermustcommunicatewithaservice
provider through the service of another company. For instan ce, the user of a Website who communicates with an
advertiser through the publisher Website. The user may choo se to share some information with the publisher (e.g.,
their delivery address) in order to gain some of their servic es. The publisher often does not know if sharing that
information with the advertiser would beneﬁt or harm the use r. It is normative for the publisher to ask users if they
canshare their informationand tofollowtheir dictates.
Deﬁnition6. [Control]Bob hasa strategyofcontrol towards Aliceifwhen sheshareshersecret aandsignalswithBob
hisstrategy istoshare a with Carol ifs=1 andkeepitifs=0.
Signalling only makes sense if Alice has information which B ob does not have. Suppose that with the same setup
as in the Game of Fiduciary Transfer (Table 4) it is now Alice who knows her context with Carol and Bob who do es
notknow.
AssumeAlicecanchooseasignalaspartofhersecretsharing strategy.Whenshedecidestoshare /u1D44Eshealsoshares
/u1D460whichshechoosestobe0or1.Bob’ssecretsharingstrategyc anrelyon /u1D460.SowhenAliceshares hersecretwithBob
his choices are: 1) To always keep /u1D44Efrom Carol; 2) to share /u1D44Ewith Carol regardless of /u1D460. 3) To share /u1D44Ewhen/u1D460is 1, or
4) toshare /u1D44Ewhen/u1D460is 0.
Theorem6. IfAlicesignals1whensheisinacollaborativecontextwith Caroland0whensheisinacompetitivecontext
then Bob would share Alice’s secret with Carol when Alice sig nals 1 and keep it when she signals 0. Hence, Bob has a
strategyof Control towards Alice.
P/r.sc/o.sc/o.sc/f.sc.FromtheanalysisoftheGameofFiduciaryTransferwealread yknowthatkeepingAlice’ssecretisharmful
for Bob if Alice and Carol are in collaborative context and th at sharing the secret is harmful to Bob when Alice and
Carolareincompetitivecontext.IfBobknowsthatAliceonl ysignals1whensheisinacollaborativecontextthenhis
Manuscriptsubmitted to ACM
ContextualIntegrity Games 9
bestsharingstrategyistosharewhenthesignal is1andtoke epwhenitis0.Sincethisstrategyisalsothebestsecret
sharing strategyfor Alice,it is a payoﬀ-optimal Nash equil ibrium. /square
3.5 Notification
Carol
LCR
AliceT4,-80,0-8,-8
M2,01,02,0
B-8,-80,04,-8
(a) Alice v.CarolBob
NIF
AliceT4,40,0-8,-8
M1,01,01,0
B-8,-80,04,4
(b) Alice v.BobCarol
LCR
BobN0,00,00,0
I0,00,00,0
F0,00,00,0
(c) Bobv.Carol- non collab.Carol
LCR
BobN-6,-60,06,6
I0,00,00,0
F6,60,0-6,-6
(d)Bobv.Carol - collaborative
Table6. GameofNotification
Acommonrealworldexamplefornotiﬁcationisthatinwhicha companyisboughtbyanother.Whenthathappen,
the context of the bought company and its new owner typically becomes collaborative. Such collaboration can be
reinforcedbysharingtheboughtcompanysecretswithitsow ner.However,sometimesinordertocollaboratewithits
owner theboughtcompanyneeds tovalidatethatits customer shave a chance toaltertheir ownstrategy.
Deﬁnition7. [Notiﬁcation]BobhasastrategyofnotiﬁcationtowardsAli ceifwhensheshareshersecretawithBob,his
strategyistosignal1 toherifhechooses toshare that secre twith Carol andtosignal0 toherifhe choosestokeepit.
The Game of Notiﬁcation in Table 6attempts to capture this dynamic by requiring that Alice cho ose her strategy
a-priori,beforeanyoftheplayersknowsifthecontextofBo bandCaroliscollaborative.Alicestrategycanstilldepen d
ona signal from Bob,butAlicewouldnotbeabletochange her s trategyin responsetothechoices of Boband Carol.
When Alice does not share her secret with Bob the only strateg y which guarantees Alice a positive payoﬀ is
M and the only Nash equilibrium is /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}htwhose gain is /an}bracketle{t2,0,0/an}bracketri}ht. By sharing her secret with Bob, Alice allows
/an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,/u1D440/an}bracketri}htwhose gain is higher for her and for Bob, /an}bracketle{t5,4,0/an}bracketri}ht. When Bob and Carol are in a non-collaborative
context,thatis thepayoﬀ-dominant Nash-equilibrium.
WhenBobandCarolareinacollaborativecontext,oriftheyc hangetheircontexttoacollaborativeone,theywould
prefer thatCarol chooseLwhenBobchoosesF and Rwhen Bobcho osesN.However, thatis onlya Nash equilibrium
if Alice chooses M. Otherwise, e.g., in /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,/u1D44E?/u1D445:/u1D43F/an}bracketri}ht, the loss to Carol from choosingR when Alice chooses T
is higher thanCarol’s gain from choosingRwhen BobchoosesN .
Thus, there are two groups of Nash equilibria to the game when all players have Alice’s secret: /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,/u1D440/an}bracketri}ht
with payoﬀ /an}bracketle{t5,4,0/an}bracketri}htand/an}bracketle{t/u1D440,/u1D44E?/u1D441:/u1D439,/u1D44E?/u1D445:/u1D43F/an}bracketri}htwith payoﬀ /an}bracketle{t3,6,6/an}bracketri}ht. Bob and Carol prefer the second and Alice prefer the
ﬁrst. However, Alice still prefers this second Nash equilib rium over /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}htwhich is her best option if she does not
share her secret withBob.
Now consider the possibility that Bob signals to Alice /u1D460=1 if his context with Alice is collaborative and /u1D460=0
if non-collaborative. If Alice can trust that signal then he r best strategy is /u1D460?/u1D440:/u1D44E?/u1D447:/u1D435.Trust is critical because Bob’s
choiceof whatsignal hesends comes after Alicehas committe dtothestrategyand have shared her secret.
Theorem7. IfAlice sharesher secret with Bobin theNotiﬁcation Game th enBob hasa strategyof notifyingAliceifhe
sharedhersecret withCarol.
Manuscriptsubmitted to ACM
10 RanWolﬀ
P/r.sc/o.sc/o.sc/f.sc.Bob would only share Alice’s secret with Carol if his context with Carol is collaborative. If Bob shares the
secretand thensignals 0thenAlicestrategyis /u1D44E?/u1D447:/u1D435and CarolwouldchooseM.ThatwouldreduceBob’spayoﬀto4
ratherthe6hecouldgainifhesignalled1.IfBobdoesnotsha reAlice’ssecretandstillsignals1thenAlicechoosesM.
ThatwouldreduceBob’sgain to0 ratherthanthe4 hecouldhav e gained if hesignalled 0. /square
4 INFORMATIONAL NORMSASMECHANISMS
Asstatedearlier,aninformationalnormcanbeseenasasoci almechanismwhosepurposeistomakesureindividuals
adopt certain normative behaviors. In a game where players d o not have information-normative strategies, a mech-
anism is an adaptations of the given game which cause players to choose an informational-normative strategy. This
paperfocusesontwotypesofmechanisms:Distributionalme chanisminwhichthegovernmentcanshiftaplayerpay-
oﬀ to another player,or to itself, and communication mechan isms in which the government can obstruct the passing
of somemessages.
4.1 Distributional mechanisms
Twoprevalentmethodsofpayoﬀdistributioninthecommerci alworldarerevenue-sharingandtaxation2.Intheformer
the data subject is paid for the use of her secret. In the latte r,the government extracts some of the payoﬀ from some
players.
Carol
LCR
AliceT10,00,20,10
M2,02,22,0
B0,100,210,0
(a) Alice v.CarolCarol
LCR
BobN0,00,00,0
I0,010,00,0
F0,00,00,0
(b) Bobv.CarolBob
NIF
AliceT2,20,00,0
M0,00,00,0
B0,00,02,2
(c) Alicev.Bob -as givenBob
NIF
AliceT2,20,00,0
M0,05,-50,0
B0,00,02,2
(d)Alice v.Bob -Modified
Table7. GameofInformationOwnership
4.1.1 InformationOwnership.
Deﬁnition 8. [Information ownership] Bob has a strategy of recognising A lice’sownership of her secret if he chooses to
transfer Alicesome ofhisaccess payoﬀwhichresultedfrom s haringhersecret withCarol.
IntheGameofInformationOwnership(Table 7)AliceandCarolcontextisagameofprivacy.IfAlicedoesno tshare
hersecretwithBobthentheNashequilibriumofthegameis /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htwithpayoﬀ /an}bracketle{t6,1,6/an}bracketri}ht.IfAliceshares
her secret with Bob then they can also choose /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htwith payoﬀ /an}bracketle{t7,2,6/an}bracketri}ht. However, Bob would much
rather AlicewouldchooseMbecausethestrategy proﬁle /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}htgains him 10from Carol.
If Alice shares her secret with Bob then he can force /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}htby sharing Alice’s secret with Carol. As the game is
given,Aliceseehergainreducedfrom6ifshedoesnotshareh ersecretwithBob,to2ifshedoessharehersecretwith
Bob(and heshares itwith Carol).Therefore, as thegameis gi ven, Alicewouldnot share her secret withBob.
If Alice and Bob could adopt a mechanism in which Bob transfer s a payment of 5 to Alice if in their context
they choose M and I (Table 7d) then Alice’s calculation would change. Now, in /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}hther gain is 7 whereas in
2We consideranykindof monetary punishment systemastaxationforthepurposeofthisdiscus sion.Sinceitallowsplayerstomakerationaldecisions
which consider the probabilityof getting caught.
Manuscriptsubmitted to ACM
ContextualIntegrity Games 11
/an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htitis6.AlicenolongermindsthatBobmightsharehersecretw ithCarol.SinceBobincreases his
gain in/an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}htto5comparingto1in /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}ht,themechanism works betterforBobas well.Last, since
the total gain in /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}htis 12, whereas in /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htit is 11, the mechanism is ethical from a utilitarian
pointof view.
4.1.2 Taxation. The same game which exempliﬁes Information Ownership can al so present the value of taxation.
Consider if inthegame inTable 7,thegovernment placeda ﬂat taxof5 onAliceif shechooses an ything butM.That
would cause Alice to prefer /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}htover/an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htbecause in the former Alice’s gain is 2 which in the
later it is 1. Since the total gain of /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}htis higher than that of /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}ht, that tax is also ethical from a
utilitarianpointofview3.
4.2 Communication mechanisms
Secret sharing was presented so far as optimal communicatio n: Whenever a source player decides to share a secret
the destination player immediately knows the value of that s ecret. There are many ways in which such perfect com-
munication can be degraded. Firstly, the channel can be made interactive, requiring that the destination be open to
receiving the secret or else it does not matter if the source p layer chooses to share it. Secondly, the channel can be
made noisy, such that when the origin shares a secret /u1D460the destination receives ˜ /u1D460which is only equal to /u1D460in some
probability.Thirdly,thebandwidthofthechannel canbeli mitedsuchthatnomorethan /u1D458secretscanbeshared even
if theoriginhas /u1D45Bsecrets shewishes toshare.
4.2.1 Interactivechannelmechanism. Areallifeexampleofmakingachannelinteractiveisthe"fr uitofthepoisonous
tree" doctrine [ 1]. According to that doctrine, the court (recipient) can cho ose not to legally know some facts which
theprosecution(sender) decidedtoshare aboutthedefenda nt (subject).Thedoctrineis justiﬁedwhen ignoring those
facts serves thegreater social goodofdiscouraging lawenf orcement fromunlawful behavior.
Carol
LCR
AliceT0,160,416,0
M4,04,44,0
B16,00,40,16
(a) Alice v.CarolBob
NIF
AliceT2,20,00,0
M0,02,20,0
B0,00,02,2
(b) Alice v.BobCarol
LCR
BobN0,80,28,0
I2,05,22,0
F8,00,20,8
(c) Bobv.Carol
Table8. Interactivechannelmechanism
In the interactive channel mechanism example (Table 8) Bob prefers the Nash equilibrium /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}htwhich pays
/an}bracketle{t6,7,6/an}bracketri}htover the Nash equilibrium /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htwhich pays /an}bracketle{t9,5,12/an}bracketri}htand over /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htwhich
pays/an}bracketle{t10,6,12/an}bracketri}ht.IfBoblearnsAlice’ssecretthenhecanforcetheNashequil ibriumheprefersbysharingitwithCarol.
Knowing that,Alicewouldnever choosetoshare her secret wi th Bob.ButBobmight still(illicitly) observeit.
Consider whatwouldhappenifthegovernment enforces acomm unicationmechanism bywhichCarolcanchoose
if she learns secrets Bob wants to share with her. Since Carol stands only to loose if she knows Alice’s secret, she
will choosetonotlearn itfrom Bob.When that is Carol’s stra tegyAlicecan safely shareher secret withBob,making
/an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htthepayoﬀ-optimalNashequilibrium.Sincetheintroductio nofthemechanismincreasedthetotal
payoﬀ from26 to28,it is ethical froma utilitarianpointofv iew.
3Similarexamplescanbebrought inwhich the taxalsoincreas ethe total after-taxpayoﬀof the players.
Manuscriptsubmitted to ACM
12 RanWolﬀ
Carol
LCR
AliceT0,160,416,0
M4,04,44,0
B16,00,40,16
(a) Alice v.CarolBob
NIF
AliceT2,20,00,0
M0,00,00,0
B0,00,02,2
(b) Alice v.BobCarol
LCR
BobN6,60,0-6,-6
I0,00,00,0
F-6,-60,06,6
(c) Bobv.Carol
Table9. Noisy channelmechanism
4.2.2 Noisychannel. Diﬀerentialprivacy[ 14]is,possiblythemostwellknownexampleofanoisychannelm echanism.
IntheNoisychannelmechanismexample(Table 9)Alicecanincreaseherexpectedpayoﬀfrom9inthepayoﬀ-op timal
Nashequilibrium /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htto10inthepayoﬀ-optimal Nashequilibrium /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htbysharing
her secret with Bob. Bob, in turn, has no motivation to share A lice’s secret with Carol because that would force the
Nash equilibrium /an}bracketle{t/u1D440,/u1D43C,/u1D436/an}bracketri}htand reducehis payoﬀ from2 to0.
Assume the government enforces a noisy channel mechanism in which Bob can share not /u1D44Ebut ˜/u1D44Esuch that the
chances that ˜ /u1D44E=/u1D44Eare1
2+/u1D6FF.Carolcanstillchoosethestrategy ˜ /u1D44E?/u1D43F:/u1D445,ratherthan /u1D44E?/u1D43F:/u1D445inthegiven game.Thepayoﬀ
ofthestrategyproﬁle /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,˜/u1D44E?/u1D43F:/u1D445/an}bracketri}htis/an}bracketle{t10−16/u1D6FF,2+12/u1D6FF,8+28/u1D6FF/an}bracketri}ht.ThatpayoﬀishigherforBobandCarolthan
the payoﬀ of /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}ht, and so long as 10 −16/u1D6FF>4 it is also higher for Alice than her alternative gain if
she chooses M. Hence, it is a payoﬀ-optimal Nash equilibrium .The total payoﬀ of this Nash equilibrium, 20 +24/u1D6FF, is
higher of thetotalpayoﬀ of 18that /an}bracketle{t/u1D44E?/u1D447:/u1D435,/u1D44E?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htdelivers. Therefore, thenoisy channel mechanism is ethica l
from a utilitarianpointof view.
4.2.3 Bandwidthlimitation. Last,considerthesamegamewhichisdescribedinTable 9butassumenowBobandCarol
repeatedlyplayitwith /u1D458diﬀerentAlice-s.EachAlicehasherownsecret /u1D44E/u1D456andeveryplayercanchooseadiﬀerentand
canindependentlychooseherownstrategy.BothBobandCaro lmustchooseonestrategyeachwhichsimultaneously
applies toallof thecontexts.
Asinthenoisychannel mechanism,eachAlicecanincrease he rexpectedpayoﬀfrom9inthepayoﬀ-optimalNash
equilibria /an}bracketle{t/u1D44E/u1D456?/u1D447:/u1D435,/u1D44F?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htto 10 in the payoﬀ-optimal Nash equilibria /an}bracketle{t/u1D44E/u1D456?/u1D447:/u1D435,/u1D44E/u1D456?/u1D441:/u1D439,/u1D450?/u1D43F:/u1D445/an}bracketri}htby sharing her
secretwithBob.Bob,however,cansharethosesecretsoverw ithCarolandforcetheNashequilibria /an}bracketle{t/u1D440,/u1D44E/u1D456?/u1D441:/u1D439,/u1D44E/u1D456?/u1D43F:/u1D445/an}bracketri}ht
whichpaysBob6 /u1D458ratherthan2 /u1D458.SinceeveryAlice’spayoﬀin /an}bracketle{t/u1D440,/u1D44E/u1D456?/u1D441:/u1D439,/u1D44E/u1D456?/u1D43F:/u1D445/an}bracketri}htisjust4,shewillnotshare /u1D44E/u1D456with
Bobin theﬁrst place.
Assumethatthegovernmentimplementsacommunicationmech anismwhichonlyallowsBobtoshareonebitwith
Carol.Bobcanselectthatbittobeanyofthesecrets,oranyf unctionofthesecrets.SpeciﬁcallyconsiderBob’sstrateg y
of sharing /u1D453such that /u1D453=0 if less than a fraction /u1D6FCof the/u1D44E/u1D456are 1,/u1D453=1 if more than 1 −/u1D6FCare 1, and /u1D453=/u1D44F, Bob’s
secret,if thefractionis between /u1D6FCand 1−/u1D6FC.
With that strategy, thestrategyproﬁle /an}bracketle{t/u1D44E/u1D456?/u1D447:/u1D435,/u1D44E/u1D456?/u1D441:/u1D439,/u1D453?/u1D43F:/u1D445/an}bracketri}htis possible.Central limit theorem dictates thatfor a
large enough k the chances that a fraction of less than /u1D6FCor more than 1 −/u1D6FCof the/u1D44E/u1D456is 1 is equal to the probability
that a normally distributed variable /u1D441/parenleftBig
0,1
4/parenrightBig
exceeds the range (/u1D6FC,1−/u1D6FC). If/u1D6FCis taken to be1
4then with probability
ofapproximately68%(one /u1D70E)/u1D453=/u1D44F.Thus,theexpectedpayoﬀofeveryAlicein /an}bracketle{t/u1D44E/u1D456?/u1D447:/u1D435,/u1D44E/u1D456?/u1D441:/u1D439,/u1D453?/u1D43F:/u1D445/an}bracketri}htishigherthan
68% of 10,which is higher than her alternative payoﬀ if she ch ooses M. Since that strategy proﬁlealso increases Bob
andCarol’spayoﬀ,itfollowsthatif /u1D458issuﬃcientlylargeand /u1D6FCsuﬃcientlysmallthen /an}bracketle{t/u1D44E/u1D456?/u1D447:/u1D435,/u1D44E/u1D456?/u1D441:/u1D439,/u1D453?/u1D43F:/u1D445/an}bracketri}htisaNash
equilibrium. As in the noisy channel mechanism, the total ex pected payoﬀ of this bandwidth limiting mechanism is
higher thanthatof theoriginal game, which means it is ethic al from a utilitarianpointof view.
Manuscriptsubmitted to ACM
ContextualIntegrity Games 13
5 RELATED WORK
Work related to this paper can largely be divided between tha t which aims to explain what privacy means and that
which aims toexplainhow privacycanberetained. Theﬁrstki nd ismorewidelypracticebylegal scholars[ 8,18,35],
sociologists[ 7,29],economists[ 21]andafewexceptionalscholarswhothriveinmorethanoneof thoseﬁelds[ 30–32].
Thesecondkind was mostlydeveloped bytechnologistsin the Computerand Data Sciences.
5.1 Rigorous definitions ofprivacy
A key objective of the philosophy of privacy is to provide a ri gorous and useful deﬁnition of privacy. Gavison [ 18]
describedprivacyasthemeetingpointofthreevectors(kno wledge,abilitytoaﬀect,andattentiontoaperson).Posner
[32] placed privacyin a utilitarianeconomic framework, famou slyattesting that government regulation of privacy is
an unnecessary intervention in a free market for informatio n. While this claim was objected for almost immediately
[6] it was Kadane et al. [ 24] who ﬁnally refuted that claim by quantitatively showing th at privacy does make perfect
utilitariansense ina game theoreticframework, if not ina B ayesian eﬃcient market.
Oneofthemostimportantcontributiontorigorouslydeﬁnin gprivacywasthedevelopmentofContextualIntegrity
by Nissenbaum [ 22,30]. Nissenbaum suggested to inspect privacy as the interacti on of three social actors – subject,
sender and recipient – in two separate social contexts: A soc ial exchange in the ﬁrst context leaves the sender with
information about the subject. Then, privacy norms regulat e the transfer of that information from the sender to the
recipient in another social context. This work extends cont extual integrity by placing it in a strict game theoretical
setting.Insuchsettingsactorsarerationaldecisionmake rswhoacttomaximizetheirgains inanenvironment which
contains secrets.
Previous work on game theoretic privacy has built on Kadane e t al. and has shown that privacy, like secrecy [ 10],
canbedescribedasthestrategyofplayerswithrespecttose crets.Anonymous[ 4]hasshownthatwhenoneplayerhas
access to another player’s secret, the ﬁrst might still choo se to respect the other’s privacy. Thus, a privacy norm can
emergeasthestrategyincertaingames.Asimilarresultwas laterpresentedbyUlusoyandPinar[ 36],inamulti-player
scenario.Still,UlusoyandPinardonotreference theirres ulttocontextualintegrity,whichisthemaincontribution of
this work.
Theimpactofprivacy ongames was studiedbyGradwohland Rei ngold [20]and Gradwohland Smorodinsky[ 21].
The ﬁrst work has shown that in multiplayer simultaneous gam es players would rather not expose their secret type.
In the notation proposedhere, this is related more to the con cept of secrecy (Def. 1) than it is to privacy. The second
paperbegin byassumingplayers desire tonotbeidentiﬁed. T heauthorsthendrawconclusiononthesocietalimpact
(pooling) of such games. This, however, assumes a privacy pr eference rather than explain why such preference may
exist in theﬁrst place,as this paperdoes.
5.2 Privacy preserving mechanisms
The leading paradigm of privacy preserving mechanism, init ially proposedby Warner [ 37], is that of a data collector
whowishes topublisha statisticsofthedataof multiplesub jects.Thosesubjectsareconcerned oftheimplicationsof
thepublication.Theroleofthemechanism is tocontain that risk.
Privacypreservingmechanismscanbedividedtothosewhich areoperatedbythedatasubject,withoutneedtotrust
thecollector,andthoseoperatedbyatrustedcollector.Wa rner’soriginalworkfallsintotheﬁrstcategory,asdomost of
theSecureMultipartyComputation[ 16,27].Kantarcioundeﬁnedluetal.[ 25]andAnonymous[ 5]haveindependently
Manuscriptsubmitted to ACM
14 RanWolﬀ
identiﬁed thattheoutcomeofthecomputation,ratherthani ts security,might concernthedatasubjects.Anonymous
oﬀered compositionofsecurityand /u1D458-anonymity as asolution[ 5].
Research on trusted collector mechanisms can be divided to n oisy channel mechanisms (a.k.a., data perturbation)
and mechanisms whichrely onbandwidthlimitation(aggrega tion). Theﬁrstkind wasinitiallysuggested byAgrawal
andSrikant[ 3]withoutproperanalysisoftheimpactofnoiseontheplayer s.Evﬁmievskietal.[ 17]weretheﬁrsttotry
and quantify the impact of noise on the certainty of the recip ient. Dwork, on her own [ 13] and with co-authors [ 15],
presented a full analysis of the impact of noise on the inform ation transfer from the collector to the recipient. This
included a proof of the infeasibility of zero leakage, a deﬁn ition of the worst case model of Diﬀerential Privacy, and
ﬁrst algorithms. Hundreds of studies, which cannot reasona bly be surveyed here, have since validated the usefulness
of diﬀerential privacy.
Diﬀerential Privacystopsshyofinspectingtheimpactofda taleakageonthedatasubject.Thatanalysis ispartially
fulﬁlledbyGilboa-FreedmanandSmorodinsky[ 19]whohaveshownthatthisimpactcanbediﬀerentindiﬀerentt ypes
ofgames.Thispaperfollowsasimilarpath.Gilboa-Freedma nandSmorodinskygobeyondthisworkininspectingthe
equivalence and non-equivalence of diﬀerent privacy mecha nism. However, they focusprimarily onpreserving near-
conﬁdentialitywhereas this work expands theanalysis toot herinformative-norms.
Last, data aggregation was ﬁrst proposedby Sweeney and Sama rati [33] as a way of protectingdata subjects from
speciﬁc attacks by the recipient. Machanavajjhala at el. [ 28] developed a more elaborate concept of aggregation. At-
tempts to quantify the impact of /u1D458-anonymity have mostly drawn parallels to /u1D716-Diﬀerential Privacy [ 9,12,26]. This
paperproposeslookingatmultipartygames,ratherthanonn oisychannels,astheadequatemodelinwhichtheimpact
of/u1D458-anonymity maybest bequantiﬁed.
6 DISCUSSION
Thispaperpresented agametheoretictranscriptionofNiss enbaum’s contextualintegrity model.Gametheoryallows
explaining the existence of privacy norms in terms of player s’ payoﬀ and social welfare. This utilitarian analysis of
privacyis especiallyadequatewhen normsare set bya revenu e maximizing corporateand their willingcustomers.
One other beneﬁt of a game theoretic model is that it allows an alysis of privacy related situations in the sense
of suﬃciency and equivalence. It allows answering question s such as: How much information can a company share
with advertisers beforecustomersstartchanging their dat a sharing behavior?When is thethreat of loosingone’s job
as eﬀective as a technology which limits ones access to indiv iduals’ data? How does one compare the risk of being
identiﬁed on-line tootherrisks suchas therisk of loosinga key elections?
Intermsoffutureresearch,weobservethatprivacypreserv ingtechnologywhichweredevelopedinthelast30years
havefocusedondatasubjects’controloftheirinformation andondataaggregatorconﬁdentiality.Wehopethatmath-
ematical deﬁnitions of ﬁduciary transfer and of informatio n ownership can lead to the development of technologies
implementing thosetransmission principles as well.
Manuscriptsubmitted to ACM
ContextualIntegrity Games 15
REFERENCES
[1] 1920. Silverthorne LumberCo. v.United States. 251,No. 358(1920).
[2] 1992. BasicLaw:HumanDignityand Freedom. ,248–249 pag es.https://doi.org/10.1017/S0021223700010943
[3] RakeshAgrawalandRamakrishnanSrikant.2000.Privacy -preservingdatamining.In Proceedingsofthe2000ACMSIGMODinternationalconference
onManagement of data . 439–450.
[4] Anonymous. -. -. --,-(-), X–X.
[5] Anonymous. -. -. --,X–X.
[6] CEdwin Baker.1977. Posner’s PrivacyMysteryand the Fai lureof Economic Analysisof Law. Ga.L.Rev. 12(1977), 475.
[7] DanahBoyd.2012. Networked privacy. Surveillance & society 10,3/4(2012), 348.
[8] LouisBrandeis and SamuelWarren.1890. Theright to priv acy.Harvard law review 4,5 (1890), 193–220.
[9] George Danezis. 2013. Measuring anonymity: a few though ts and a diﬀerentially private bound. In Proceedings of the DIMACS Workshop on
MeasuringAnonymity .26.
[10] Nikhil S Dighe, Jun Zhuang, and Vicki M Bier. 2009. Secre cy in Defensive Allocations as a Strategy for achieving more Cost-eﬀective Attacker
Deterrence. International Journal of Performability Engineering 5,1(2009), 31.
[11] RichardADollinger.2014. JudicialEthics:TheObliga tiontoReportTaxEvasioninSupportCases. JournaloftheAmericanAcademyofMatrimonial
Lawyers27 (2014), 1.
[12] JosepDomingo-FerrerandJordiSoria-Comas.2015.Fro mt-closenesstodiﬀerentialprivacyandviceversaindataa nonymization. Knowledge-Based
Systems74(2015), 151–158.
[13] CynthiaDwork.2006. Diﬀerentialprivacy.In International colloquium on automata,languages, and prog ramming.Springer,1–12.
[14] Cynthia Dwork.2008. Diﬀerentialprivacy:A surveyof r esults.In Theory and Applications of Models of Computation: 5th Inter national Conference,
TAMC2008,Xian, China,April 25-29,2008.Proceedings 5 .Springer,1–19.
[15] Cynthia Dwork,KrishnaramKenthapadi, Frank McSherry ,Ilya Mironov,and Moni Naor.2006. Our data, ourselves:Pri vacyvia distributed noise
generation. In Advances in Cryptology-EUROCRYPT 2006: 24th Annual Interna tional Conference on the Theory and Applications of Cryptog raphic
Techniques,St. Petersburg,Russia,May 28-June 1,2006.Proc eedings 25 .Springer,486–503.
[16] CynthiaDworkandKobbiNissim.2004. Privacy-Preserv ingDataminingonVerticallyPartitionedDatabases.In AdvancesinCryptology–CRYPTO
2004,Matt Franklin(Ed.). SpringerBerlinHeidelberg,Berlin, Heidelberg,528–544.
[17] Alexandre Evﬁmievski, Johannes Gehrke, and Ramakrish nan Srikant. 2003. Limiting privacy breaches in privacy pre serving data mining. In
Proceedings ofthe twenty-second ACMSIGMOD-SIGACT-SIGART symposiumonPrinciples of database systems .211–222.
[18] Ruth Gavison.1980. Privacyand the Limitsof Law. The Yale law journal 89, 3(1980), 421–471.
[19] GailGilboa-FreedmanandRannSmorodinsky.2020. Onth ebehavioralimplicationsofdiﬀerentialprivacy. TheoreticalComputerScience 841(2020),
84–93.
[20] Ronen Gradwohl and Omer Reingold. 2010. Partial exposu re in large games. Games and Economic Behavior 68, 2 (2010), 602–613.
https://doi.org/10.1016/j.geb.2009.09.006
[21] Ronen Gradwohl and Rann Smorodinsky. 2017. Perception games and privacy. Games and Economic Behavior 104 (2017), 293–308.
https://doi.org/10.1016/j.geb.2017.04.006
[22] NissenbaumHelen. 2010. Privacyin Context:Technology, Policy,and the Integrity ofS ocial Life. Stanford LawBooks.
[23] LouisHenkin. 1974. Privacyand autonomy. Columbia Law Review 74 (1974), 1410.
[24] JosephB Kadane, MarkSchervish,and Teddy Seidenfeld. 2008. Is ignorancebliss? The Journal of Philosophy 105,1 (2008), 5–36.
[25] MuratKantarcioundeﬁnedlu, JiashunJin, and ChrisCli fton. 2004. When do data mining resultsviolate privacy?. In Proceedings of the Tenth ACM
SIGKDD International Conference on Knowledge Discovery an d Data Mining (Seattle, WA, USA) (KDD ’04) . Association for Computing Machinery,
NewYork,NY, USA, 599–604. https://doi.org/10.1145/1014052.1014126
[26] Ninghui Li, Wahbeh H Qardaji, and Dong Su. 2011. Provabl y private data anonymization: Or, k-anonymity meets diﬀere ntial privacy. CoRR,
abs/1101.2604 49 (2011), 55.
[27] Yehuda Lindell and Benny Pinkas.2000. Privacypreserv ingdata mining.In Annual International Cryptology Conference .Springer,36–54.
[28] A. Machanavajjhala,J. Gehrke, D. Kifer, and M. Venkita subramaniam.2006. L-diversity:privacybeyond k-anonymi ty. In22nd International Con-
ferenceon Data Engineering(ICDE’06) .24–24.https://doi.org/10.1109/ICDE.2006.1
[29] Alice EMarwickand DanahBoyd.2018. Understanding pri vacyat the margins. International Journal of Communication(19328036) 12 (2018).
[30] HelenNissenbaum.2004. PrivacyasContextual Integri ty.WashingtonLaw Review 79,1 (2004), 119.
[31] PaulOhm. 2009. Brokenpromisesof privacy:Responding to the surprisingfailureof anonymization. UCLAl. Rev. 57 (2009), 1701.
[32] RichardA Posner. 1977. The rightof privacy. Ga.L. Rev. 12 (1977), 393.
[33] PierangelaSamaratiand LatanyaSweeney.1998. Protectingprivacy when disclosinginformation:k-anonymi ty and itsenforcement throughgeneral-
ization and suppression . Technical Report. technical report,SRI International.
[34] WilliamH Sandholm. 2010. Decompositions and potentia ls for normalform games. Gamesand EconomicBehavior 70, 2(2010), 446–456.
[35] DanielJ.Solove. 2008. Understanding Privacy . HarvardUniversityPress.
Manuscriptsubmitted to ACM
16 RanWolﬀ
[36] Onuralp Ulusoy and Pınar Yolum. 2019. Emergent privacy norms for collaborativesystems.In PRIMA 2019:Principles and Practice of Multi-Agent
Systems:22nd International Conference,Turin,Italy, Oct ober28–31,2019,Proceedings 22 .Springer,514–522.
[37] Stanley L. Warner.1965. Randomized Response: A Survey Technique for Eliminating EvasiveAnswer Bias. J. Amer. Statist. Assoc. 60, 309 (1965),
63–69.https://doi.org/10.1080/01621459.1965.10480775
[38] SamuelD.WarrenandLouisD.Brandeis.1890.TheRightt oPrivacy. HarvardLawReview 4,5(1890),193–220. http://www.jstor.org/stable/1321160
[39] JoannaWuest.2021. AConservativeRighttoPrivacy:Le gal,Ideological,andCoalitionalTransformationsinUSSo cialConservatism. Law&Social
Inquiry46,4 (2021), 964–992. https://doi.org/10.1017/lsi.2021.1
Manuscriptsubmitted to ACM
