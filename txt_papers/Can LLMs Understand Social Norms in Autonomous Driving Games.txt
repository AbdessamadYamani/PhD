Can LLMs Understand Social Norms in Autonomous Driving Games?
Boxuan Wang, Haonan Duan, Yanhao Feng, Xu Chen, Xuan Di, Yongjie Fu, Zhaobin Mo Member, IEEE
Abstract — Social norm is defined as a shared standard of
acceptable behavior in a society. The emergence of social norms
fosters coordination among agents without any hard-coded
rules, which is crucial for the large-scale deployment of au-
tonomous vehicles (A Vs) in an intelligent transportation system.
This paper explores the application of large language models
(LLMs) in understanding and modeling social norms in au-
tonomous driving games. We introduce LLMs into autonomous
driving games as intelligent agents who make decisions accord-
ing to text prompts. These agents are referred to as LLM agents.
Our framework involves LLM agents playing Markov games
in a multi-agent system (MAS), allowing us to investigate the
emergence of social norms among individual agents. We aim
to identify social norms by designing prompts and utilizing
LLMs on textual information related to the environment setup
and the observations of LLM agents. Using the OpenAI Chat
API powered by GPT-4.0, we conduct experiments to simulate
interactions and evaluate the performance of LLM agents in
two driving scenarios: unsignalized intersection and highway
platoon. The results show that LLM agents can handle dy-
namically changing environments in Markov games, and social
norms evolve among LLM agents in both scenarios. In the
intersection game, LLM agents tend to adopt a conservative
driving policy when facing a potential car crash. The advantage
of LLM agents in games lies in their strong operability and
analyzability, which facilitate experimental design.
Key-words: Social Norm, LLM, Autonomous Driving
I. I NTRODUCTION
Large Language Models (LLMs) have demonstrated re-
markable capabilities in natural language understanding, en-
abling them to process and generate human-like text across a
wide range of domains [1]–[5]. The increasing popularity of
LLMs facilitates the design of numerous applications, allow-
ing LLMs to frequently interact with us in our daily lives.
With enhanced capabilities, LLMs hold considerable promise
Boxuan Wang is with the Department of Mechanical Engineer-
ing, Columbia University, New York City, NY 10027 USA (e-mail:
bw2812@columbia.edu).
Haonan Duan is with the Data Science Institute, Columbia University,
New York City, NY 10027 USA (e-mail: hd2545@columbia.edu).
Yanhao Feng is with the Department of Statistics, Columbia University,
New York City, NY 10027 USA (e-mail: yf2684@columbia.edu).
Xu Chen is with the Department of Civil Engineering and Engineering
Mechanics, Columbia University, New York City, NY 10027 USA (e-mail:
xc2412@columbia.edu).
Xuan Di is with the Department of Civil Engineering and Engineering
Mechanics, Columbia University, New York, NY , 10027 USA, and also with
the Data Science Institute, Columbia University, New York, NY , 10027 USA
(e-mail: sharon.di@columbia.edu).
Yongjie Fu is with the Department of Civil Engineering and Engineering
Mechanics, Columbia University, New York City, NY 10027 USA (e-mail:
yf2578@columbia.edu).
Zhaobin Mo is with the Department of Civil Engineering and Engineering
Mechanics, Columbia University, New York City, NY 10027 USA (e-mail:
zm2302@columbia.edu).for applications in traffic planning and autonomous driving.
This paper aims to study how LLM-guided agents behave
and interact with each other in autonomous driving scenarios.
Specifically, we utilize LLMs to analyze autonomous driving
games in which LLM-guided agents are tasked with making
decisions in simulated driving environments. Our goal is to
investigate whether these agents can understand social norms
in autonomous driving games.
In recent years, autonomous driving technology has wit-
nessed significant advancements in recent years, promising
safer and more efficient transportation systems. However,
the integration of autonomous vehicles into society [6]
raises concerns about their interaction with human drivers,
pedestrians, and other road users. One critical issue of this
interaction is the adherence to social norms, the informal
rules that guide vehicles to navigate the road. Understanding
and effectively modeling these social norms are essential for
the development of autonomous driving systems capable of
navigating real-world scenarios effectively.
In this paper, we explore the potential of leveraging Large
Language Models (LLMs) to understand social norms in
autonomous driving games. By utilizing LLMs on textual
information related to autonomous driving scenarios, we can
teach them to identify the social norms among vehicles on
the road. By observing and analyzing the choices made by
players in these games, we can extract implicit social norms
that regulate their behavior. Our research aims to contribute
to the development of socially aware autonomous driving
systems that can effectively navigate complex driving envi-
ronments. Leveraging LLMs to understand and model social
norms can empower safer interactions between autonomous
vehicles and other road users.
A. Related work
To explore LLMs’ capability of understanding human be-
haviors [7]–[9], there has been a growing trend of analyzing
LLMs in the context of game theory [10], [11], including
fairness [12], dilemma [13] and rationality [14]–[16]. Many
studies employ LLMs to replace humans as research subjects
[17], [18]. To study complex interactions among players in
game theory problems, LLMs are introduced into game-
theoretic frameworks as intelligent agents, referred to as
LLM-based agents [19], [20]. These LLM-based agents
empower the development of sophisticated systems [21]–[24]
where players’ behaviors can be simulated via LLMs. For
example, LLM-based agents are utilized to study coopera-
tion and coordination behaviors in the prisoner’s dilemma
[13]. This paper further explores the capability of LLM-arXiv:2408.12680v2  [cs.AI]  1 Sep 2024
based agents regarding social outcomes in games. More
specifically, we investigate whether LLM-based agents can
form desired social norms in autonomous driving games to
improve driving efficiency and road safety.
Social norms have been widely studied in games. Most lit-
erature on social norms primarily focuses on stateless matrix
games [25]–[29]. To capture dynamic environments, Markov
games are developed to study the emergence of social norms
[30], [31] in sequential decision making. A learning-based
framework is proposed to study how social norms evolve in
autonomous driving games [32], where agents are guided by
a deep reinforcement learning (DRL) algorithm. Compared
to DRL-based agents, LLM-based agents have the following
advantages: strong operability, relatively simple experimen-
tal design in game theory, and strong analyzability [22].
Therefore, we introduce LLM-based agents into autonomous
driving games to handle dynamically changing driving envi-
ronments.
B. Contributions of this paper
This work focuses on how LLMs can facilitate navigation
in autonomous driving games. Our study utilizes the OpenAI
Chat API [33], powered by GPT-4.0, to conduct experiments.
Our contributions include:
1) We propose a framework where LLM agents play
Markov games in a multi-agent system (MAS) in order
to investigate whether social norms emerge among
individual agents.
2) We design prompts to simulate interactions in au-
tonomous driving games. The prompts consist of “sys-
tem” messages outlining the general settings (e.g.,
the road layout) and “user” messages regarding the
observations and decision making of LLM agents.
3) We apply the proposed framework to two traffic scenar-
ios: the unsignalized intersection and highway platoon,
to examine social norms formed by LLM agents.
The rest of the paper is organized as follows: In Section II,
we first present preliminaries regarding Markov game, LLMs
and social norm. Section III introduces our framework in
which LLM agents play Markov games via prompt design.
Section IV presents numerical experiments and results. Sec-
tion V concludes.
II. P RELIMINARY
A. Markov Game
A Markov game, also known as a stochastic game, is
a mathematical framework that generalizes both Markov
Decision Processes (MDPs) and game theory to model
dynamic interactions among rational players in uncertain
environments. In its general form, it is defined by a state set
Sand action setsA1, ...,A𝑛for each corresponding agent in
the environment [34]. Each agent also has a corresponding
reward functionR𝑖:S×A 𝑖→Rfor agent 𝑖. The goal of the
Markov Game is to find an optimal policy for each agent. In
this paper, we investigate the behavior and decision-making
of LLMs when they play the Markov Game as agents. We
useSto denote the overall environment, A𝑖to denote theaction set of agent 𝑖,O𝑖to denote the observation set of
agent 𝑖,R𝑖to denote the current reward of agent 𝑖. LLM
chooses one of the actions from A𝑖, withO𝑖and partialS
as input. In this paper, the action choosing policy of an agent
was solely decided by LLMs. Our primary focus is on two
games as follows:
Scenario 1-Unsignalized intersection : The environment
Sis visualized in Figure 2(a), with Road 1 extending from
the West to the East and Road 2 extending from the North
to the South. The green and red cars represent the agents
traveling on Road 1 and Road 2, respectively. Agents aim
to cross the intersection, reach the end of the roads, and
maximize their cumulative rewards. Table I summarizes the
setup of agent 𝑖at the beginning of time 𝑡. The location of
agent 𝑖,𝑥𝑖,𝑡corresponds to the cell it currently occupies. At
each time step, each agent can choose ”Go” or ”Stop”, each
with different action reward. The observation 𝑜𝑖,𝑡is a subset
of current state space S. Background vehicles (marked in
white) with predetermined driving strategies are non-strategic
players in the driving environment. The game terminates
when all agents complete their trips or a crash occurs.
Scenario 2-Highway platoon : The environmentSconsists
of two lanes, shows in Figure 2(b), going from North to
South. Red and green agents each occupy one lane. Agents
aim to reach the end of the lanes and maximize their
cumulative rewards. In addition to the ”Go” and ”Stop”
action, agents can now also choose to switch lanes. We
defined the success of the game as forming a platoon at
least once and completing the trips without any car crashes.
A platoon is characterized by the presence of cars in the
same lane. With the incentive of the reward and the overall
goal, the agents will aim to complete their trips as quickly as
possible, while avoiding accidents and attempting to maintain
platoons. Table I summarize the setup of the platoon game.
(a)Scenario 1
 (b)Scenario 2
Fig. 2: Game scenarios
Scenario 1
𝑠𝑖,𝑡∈S Location 𝑥𝑖,𝑡and cumulative reward 𝜌𝑖,𝑡
𝑜𝑖,𝑡∈O Observation of the current environment
𝑎𝑖,𝑡∈A Go: move forward by one cell; Stop: no move
𝑟𝑖,𝑡∈R Go: -2; Stop: -2; Crash: -5
Scenario 2
𝑠𝑖,𝑡∈S Location 𝑥𝑖,𝑡and cumulative reward 𝜌𝑖,𝑡
𝑜𝑖,𝑡∈O Observation of the current state space
𝑎𝑖,𝑡∈A Go; Stop; Lane change: switch to the other lane
𝑟𝑖,𝑡∈R Go; Stop; Lane: -4; Crash: -5; Platoon: +2
TABLE I: The setup of game scenarios
Fig. 1: LLM for autonomous driving games
B. Large Language Models
A large language model (LLM) is a type of artificial
intelligence algorithm designed to understand, generate, and
interact with human language at a large scale [33]. These
models are trained on vast amounts of text data, learning
patterns, vocabulary, grammar, and even nuances of language
such as context, tone, and implications. They use a specific
architecture known as Transformer [35], which allows them
to efficiently process sequences of words and predict the next
word in a sentence, understand the meaning of a text, or
generate new text that follows a given prompt [36]. LLMs are
highly versatile AI tools with applications spanning multiple
industries and fields [37]. They excel in tasks such as content
creation, customer support, education, language translation,
software development, legal documentation, healthcare re-
search, optimization, sentiment analysis, and accessibility
technologies. In this paper, we examine the application of
LLMs in autonomous driving games to see if how they make
decisions and form social norms.
C. Social Norms
Social norms represent the collective expectations regard-
ing appropriate conduct within groups [38]. They encompass
both the unwritten agreements that dictate how society
members should act, and the formalized regulations and laws
that are established. Social norms in car driving [32] are
the unwritten rules that guide driver behavior on the road.
Our primary focus is on examining whether expected social
norms can evolve among LLM agents. The expected social
norm in Scenario 1 is “yielding to others” at the intersection.
The expected social norm in Scenario 2 is the formation of
a platoon among agents.
III. M ETHODOLOGY
In this section, we present the framework of leveraging
LLMs to understand social norms in Markov games.
In Figure 1, we utilize prompt-chaining for LLM in-
teractions in the Markov game. At each time step, the
program invoked the chat model from the perspective of
each individual agent. The games are submitted to LLMs as
prompts in which the respective game, including the choice
options, is described. Once LLMs have made their choices(i.e., the actions made by strategic and non-strategic agents),
which we track as a completion of the given text, we update
the prompts and then submit the new prompt to LLMs for
the next state in the game. The textual information includes
the reward and each agent’s current location. When a car
completes its trip, we remove it from the game, meaning that
it is no longer observed by other cars, or prompted about its
actions. Its location and reward stop updating as well. This
prevents any potential interference with the cars still in the
game. The output includes the decision making, rewards and
states of agents in the Markov game, which is used to analyze
social norms.
Prompt design : The chat model facilitates conversations
through three distinct roles, including “user”, “assistant”,
and “system” (See Figure 3). “User” represents the hu-
man participant interacting with the AI. The “assistant”
role embodies the AI providing responses to the user’s
prompts. Finally, the “system” role is primarily responsible
for defining the assistant’s behavior—this can involve setting
a specific language, character, or viewpoint for the assistant.
The chat model prompt is structured as a series of messages,
each identified by the role of the sender and the message
content. By concatenating past interactions from both the
user and assistant, the model can more effectively engage in
multi-turn conversations. The detailed prompts are in Section
IV. In our research, we assume that the agents will not be
capable of making predictions based on any historical context
regarding previous states. This leads us to use only one
“system” message and one “user” message to construct our
prompt. The “system” message outlines the general settings,
defining parameters like the environment space S(i.e., the
road layout), action space A(i.e., all the permitted moves),
and the reward function R. Meanwhile, the “user” message
takes the perspective of a specific car. This message first
identifies agent 𝑖(i.e., its color) and its present observation
𝑜𝑖(i.e., the locations of all the cars and its own cumulative
reward). After that, the prompt asks the agent about the
action 𝑎𝑖it should take and the resulting location due to
the action of choice.
We employ the OpenAI Chat API, specifically the GPT-
4.0 model, to conduct our experiments. Notably, GPT-4.0 has
demonstrated significant improvement over its predecessor,
Fig. 3: Prompt design
GPT-3.5, by at least 20%, as per OpenAI’s assertions, on an
array of benchmarks such as the SuperGLUE language com-
prehension and the LAMBADA linguistic modeling tests.
As an advanced version of the transformer-based language
model [33], GPT-4.0 holds considerable potential for traffic
planning and autonomous driving.
Note that our focus is not cooperative game. Therefore,
we ensure that when an agent made a decision, it does
not have access to the concurrent decisions made by others.
We do not ask the chat model to simulate the entire game
in one prompted response, as it has access to the private
observations of all agents and can implicitly use other agents’
private observations to make decisions. Instead, we simulate
the traffic in a turn-based manner, where each turn represents
a time step. Each inquiry to the chat model could only take
the perspective of one agent. To further prevent access to
concurrent decisions made by other agents, the game state
is updated at the end of each time step, after receiving
responses from all agents.
IV. E XPERIMENTS
In this section, we conduct experiments on two game
scenarios to understand how social norms can be formed
when LLMs play Markov games.
Experiment setup : The environments of both the inter-
section and platoon scenarios adopt a grid representation
consisting of multiple cells, with each cell assigned a pair of
coordinates to denote the location of vehicles. The red and
green cars are strategic agents, while the white car serves
as a background vehicle. All background vehicles are not
involved in the formation of social norms. The setup is stored
in a configuration containing all prompts, a list of cars,
and their initial positions. Given that the general settings
is consistent for all agents and is unaffected by time, the
system message portion of the prompt is stored as a static
string. On the other hand, each car’s private observation,
influenced by time and specific agent conditions, is stored as
templates. Each time these templates are accessed, they are
customized in accordance with present conditions. Each car
could observe both the location and the cumulative reward
of itself, but could only see the locations of all other cars. To
easily concatenate the observation about all other cars, we
store the user message as two separate templates: one for the
observing itself, and one for all other cars.
In both scenarios, we utilize the OpenAI API with the
specified parameters: model = gpt-4, temperature = 0.1, maxtoken = 10. Increasing the temperature enhances the ran-
domness of the response [39]. Our testing reveal that raising
the temperature beyond 1.0 sometimes results in unstable
behaviors. Further increasing the temperature to 2.0 leads to
responses that are essentially unintelligible, as exemplified in
Figure 4. The max token parameter restricts the maximum
length of the response. As per OpenAI’s tokenizer, the
language model can consistently provide responses in the
preferred format within 10 tokens [40].
Fig. 4: Responses (temperature = 2.0)
A. Scenario 1-Unsignalized intersection
Fig. 5: Prompts and responses - Scenario 1
Figure 5 shows an example of our prompts for scenario
where the environment of an unsignalized intersection has a
9 by 9 grid. In order to inspect the behavior of LLMs, we
simulate the Markov Game 50 times. Table II summarizes
the performance of LLMs when varying numbers of back-
ground vehicles. The results demonstrate the formation of
social norms at the unsignalized intersection (i.e., yielding
# of BVs # of tests # of Yields Rate
0 50 48 0.96
1 50 50 1
2 50 50 1
3 50 50 1
4 50 50 1
TABLE II: Performance - unsignalized intersection
to others). It is noteworthy that crashes may occur when
no background vehicles are present. This can be attributed
to the fact that the presence of background vehicles restricts
the action space for ego cars, thereby reducing the likelihood
of vehicles encounters at the intersection and facing a crash.
We now look into the case when there are no background
vehicles. The results show that the red car tends to early
stop before several grids of the intersection. The early stop
behavior measures the waiting time of vehicles. It is defined
as the stop action of vehicles when they haven’t arrived at
the intersection (5,5), i.e., vehicles stop at positions other
than (4,5) or (5,4), 1 grid away from the intersection. Figure
6 visualizes the line plot of the number of early stops in each
simulation. The x-axis represents the simulation. The y-axis
represents the number of early stops in each simulation. The
red and green lines denote the early stop behaviors of red
and green vehicles, respectively. The crash cases are marked
by blue dotted lines. The average number of early stops of
the red car is 1.42, and the average of early stops of green
car is 0.0. The results show that a conservative driving policy
is adopted by the red car when playing the Markov game at
the unsignalized intersection.
We then investigate the crash cases, which occurr in the
6th and 37th simulations. In the 6th simulation, the red car
does not stop earlier and decides to proceed at the same time
as the green car, resulting in a crash. In the 37th simulation,
the red car stops earlier, while the green car stops 1 grid
away from the intersection. When the red car arrives at
the intersection, both cars decide to proceed simultaneously,
leading to a crash.
Fig. 6: Early stop behaviors
To eliminate the early stop behaviors and shorten the
waiting time of vehicles, we increase the reward of Go action
from -2 to 0. We re-simulate the Markov game for 50 times.
The results show that (1) The average number of early stops
for the red car decreases from 1.42 to 0.48, suggesting that
the reward design can influence driving behavior. (2) The
success rate of adherence to social norms decreases from
0.96 to 0.94 due to an increase in the number of car crashes.It indicates a trade-off between waiting time and road safety
in the intersection game. The decrease in waiting time would
increase the possibility of crashes.
B. Scenario 2-Highway Platoon
Figure 7 shows an example of prompts for scenario where
the environment of a highway platoon has a 2 by 9 grid.
Fig. 7: Prompts and responses - Scenario 2
# of BVs # of tests # of Pla-
toonsRate
0 50 48 0.96
1 50 46 0.92
2 50 49 0.98
3 50 43 0.86
4 50 40 0.80
TABLE III: Performance - highway
Table III summarizes the performance of LLM agents
when varying numbers of background vehicles. The results
show that the success rate of forming platoons is high, in-
dicating the emergence of social norms among LLM agents.
Figure 8 demonstrates lane-change behaviors. The histogram
visualizes the frequency of lane-change actions. The x-axis
denotes the number of lane-change actions and the y-axis
is the number of simulations. We can tell that there is at
most three lane-changes per simulation, and one lane-change
in most cases. The green car changes lane more frequently
in a comparison to the red car. Figure 9 is the stacked
histogram of the time step when the agents choose to change
the lane. It is shown that most of the LLM agents change
lanes before the fourth time step. It indicates that LLM agents
are inclined to change lanes and form platoons at an early
stage, demonstrating an awareness that platoon formation is
an efficient driving strategy on highways. Figure 10 plots
the histogram of percentage time of platoon in each game.
Moreover, in the majority of simulations where platoon is
formed, there are more than a 60 percent of the time that
two agents forming a platoon on the highway.
Fig. 8: Lane-change frequency
Fig. 9: Time step of lane-change
Fig. 10: Time length of forming a platoon
V. C ONCLUSION ANDFUTURE WORK
This paper has demonstrated the potential of LLMs in
understanding and modeling social norms in autonomous
driving scenarios. By introducing LLM agents into au-
tonomous driving games, we have observed the emergence
of social norms among agents navigating complex driving
environments. Our experiments have shown that LLM agents
can adapt and conform to social norms, thereby contributing
to safer and more efficient driving behaviors. Overall, the
advantage of employing LLM agents in autonomous driving
games lies in their strong operability and analyzability, which
facilitate experimental design and provide valuable insights
into the dynamics of social norms in driving environments.
By further investigating the behavior of LLM agents invarious driving scenarios and refining our experimental
framework, we can contribute to the development of more
socially aware autonomous driving systems.
This work can be extended in the following ways: (1)
We will utilize LLM agents in more complex real-world
scenarios, such as sequential social dilemmas. (2) We aim
to explore how to construct a unified, controllable, and
efficient framework for simulating strategic interactions and
facilitating game design using LLMs. (3) We will compare
the behaviors of human players and LLM agents to determine
the extent to which LLMs can achieve strategic reasoning.
REFERENCES
[1] K. Ruan, X. He, J. Wang, X. Zhou, H. Feng, and A. Kebarighotbi,
“S2e: Towards an end-to-end entity resolution solution from acoustic
signal,” in ICASSP 2024 - 2024 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) , 2024, pp. 10 441–
10 445.
[2] K. Yin, C. Liu, A. Mostafavi, and X. Hu, “Crisissense-llm: Instruction
fine-tuned large language model for multi-label social media text
classification in disaster informatics,” 2024. [Online]. Available:
https://arxiv.org/abs/2406.15477
[3] Y . Fu, Y . Li, and X. Di, “Gendds: Generating diverse driving video
scenarios with prompt-to-video generative model,” in 2024 IEEE In-
ternational Conference on Intelligent Transportation Systems (ITSC) ,
2024.
[4] G. Bai, Z. Chai, C. Ling, S. Wang, J. Lu, N. Zhang, T. Shi,
Z. Yu, M. Zhu, Y . Zhang et al. , “Beyond efficiency: A systematic
survey of resource-efficient large language models,” arXiv preprint
arXiv:2401.00625 , 2024.
[5] G. Bai, Y . Li, C. Ling, K. Kim, and L. Zhao, “Gradient-free adap-
tive global pruning for pre-trained language models,” arXiv preprint
arXiv:2402.17946 , 2024.
[6] X. Chen, X. Di, and Z. Li, “Social learning for sequential driving
dilemmas,” Games , vol. 14, no. 3, 2023.
[7] M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh, “Reward design
with language models,” in The Eleventh International Conference on
Learning Representations , 2023.
[8] Y . Bai, A. Jones, and K. Ndousse, “Training a helpful and harmless
assistant with reinforcement learning from human feedback,” ArXiv ,
vol. abs/2204.05862, 2022.
[9] K. Ruan, X. Wang, and X. Di, “From twitter to reasoner: Understand
mobility travel modes and sentiment using large language models,”
in2024 IEEE International Conference on Intelligent Transportation
Systems (ITSC) , 2024.
[10] S. Mao, Y . Cai, Y . Xia, W. Wu, X. Wang, F. Wang, T. Ge, and F. Wei,
“Alympics: Language agents meet game theory,” 2023.
[11] M. Chahine, T.-H. Wang, H. Zhang, W. Xiao, D. Rus, and
C. Gan, “Large language models can design game-theoretic
objectives for multi-agent planning,” 2024. [Online]. Available:
https://openreview.net/forum?id=DnkCvB8iXR
[12] J. Horton, “Large language models as simulated economic agents:
What can we learn from homo silicus?” SSRN Electronic Journal ,
01 2023.
[13] E. Akata, L. Schulz, J. Coda-Forno, S. J. Oh, M. Bethge, and
E. Schulz, “Playing repeated games with large language models,”
2023.
[14] J. Brand, A. Israeli, and D. Ngwe, “Using gpt for market research,”
SSRN Electronic Journal , 2023.
[15] Y . Chen, T. X. Liu, Y . Shan, and S. Zhong, “The emergence of
economic rationality of gpt,” Proceedings of the National Academy
of Sciences , vol. 120, no. 51, p. e2316205120, 2023.
[16] D. Dillion, N. Tandon, Y . Gu, and K. Gray, “Can ai language models
replace human participants?” Trends in Cognitive Sciences , vol. 27,
no. 7, pp. 597–600, 2023.
[17] G. V . Aher, R. I. Arriaga, and A. T. Kalai, “Using large language
models to simulate multiple humans and replicate human subject
studies,” in Proceedings of the 40th International Conference on
Machine Learning , ser. Proceedings of Machine Learning Research,
vol. 202. PMLR, 23–29 Jul 2023, pp. 337–371.
[18] L. Argyle, E. Busby, N. Fulda, J. Gubler, C. Rytting, and D. Wingate,
“Out of one, many: Using language models to simulate human
samples,” Political Analysis , vol. 31, pp. 1–15, 02 2023.
[19] T. R. Sumers, S. Yao, K. Narasimhan, and T. L. Griffiths, “Cognitive
architectures for language agents,” 2024.
[20] Y . Xu, S. Wang, P. Li, F. Luo, X. Wang, W. Liu, and Y . Liu, “Exploring
large language models for communication games: An empirical study
on werewolf,” 2023.
[21] L. Ouyang, J. Wu, and X. Jiang, “Training language models to follow
instructions with human feedback,” NeurIPS , 2022.
[22] C. Fan, J. Chen, Y . Jin, and H. He, “Can large language models serve
as rational players in game theory? a systematic analysis,” 2023.
[23] K. Yin and A. Mostafavi, “Deep learning-driven community resilience
rating based on intertwined socio-technical systems features,” 2023.
[Online]. Available: https://arxiv.org/abs/2311.01661
[24] C. Peng, D. Zhang, and U. Mitra, “Graph identification and upper
confidence evaluation for causal bandits with linear models,” in
ICASSP 2024 - 2024 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , 2024, pp. 7165–7169.
[25] J. Delgado, “Emergence of social conventions in complex networks,”
Artificial Intelligence , vol. 141, pp. 171–185, 10 2002.
[26] S. Sen and S. Airiau, “Emergence of norms through social learning,”
inProceedings of the 20th International Joint Conference on Artifical
Intelligence , ser. IJCAI’07. San Francisco, CA, USA: Morgan
Kaufmann Publishers Inc, 2007, p. 1507–1512.
[27] D. Villatoro, J. Sabater-Mir, and S. Sen, “Social instruments for robust
convention emergence,” in IJCAI International Joint Conference on
Artificial Intelligence , 01 2011, pp. 420–425.
[28] C. Yu, M. Zhang, F. Ren, and X. Luo, “Emergence of social norms
through collective learning in networked agent societies,” in Proceed-
ings of the 2013 International Conference on Autonomous Agents and
Multi-Agent Systems , ser. AAMAS ’13. Richland, SC: International
Foundation for Autonomous Agents and Multiagent Systems, 2013, p.
475–482.
[29] H. Franks, N. Griffiths, and A. Jhumka, “Manipulating convention
emergence using influencer agents,” Autonomous Agents and Multi-
Agent Systems , vol. 26, 05 2013.
[30] A. Lerer and A. Peysakhovich, “Learning existing social conventions
via observationally augmented self-play,” in Proceedings of the 2019
AAAI/ACM Conference on AI, Ethics, and Society , ser. AIES ’19.
New York, NY , USA: Association for Computing Machinery, 2019,
p. 107–114. [Online]. Available: https://doi.org/10.1145/3306618.33
14268
[31] R. K ¨oster, K. McKee, R. Everett, L. Weidinger, W. Isaac, E. Hughes,
E. Duenez-Guzman, T. Graepel, M. Botvinick, and J. Leibo, Model-
free conventions in multi-agent reinforcement learning with heteroge-
neous preferences , 10 2020.
[32] X. Chen, Z. Li, and X. Di, “Social learning in markov games:
Empowering autonomous driving,” in 2022 IEEE Intelligent Vehicles
Symposium (IV) , 2022, pp. 478–483.
[33] OpenAI, J. Achiam, and et al., “GPT-4 Technical Report,” 2024.
[34] M. L. Littma, “Markov games as a framework for multi-agent rein-
forcement learning,” ICML , pp. 157––163, 1994.
[35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” 2023.
[36] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. B. Shaikh,
N. Akhtar, J. Wu, S. Mirjalili et al. , “Large language models: a
comprehensive survey of its applications, challenges, limitations, and
future prospects,” Authorea Preprints , 2023.
[37] H. Mohapatra and S. R. Mishra, “Exploring ai tool’s versatile re-
sponses: An in-depth analysis across different industries and its
performance evaluation,” 2023.
[38] K. Nyborg, J. M. Anderies, A. Dannenberg, T. Lindahl, C. Schill,
M. Schl ¨uter, W. N. Adger, K. J. Arrow, S. Barrett, S. Carpenter et al. ,
“Social norms as solutions,” Science , vol. 354, no. 6308, pp. 42–43,
2016.
[39] OpenAI. (2024) OpenAI API Reference. [Online]. Available:
https://platform.openai.com/docs/api-reference/chat
[40] ——. (2024) OpenAI Tokenizer. [Online]. Available: https://platform
.openai.com/tokenizer
