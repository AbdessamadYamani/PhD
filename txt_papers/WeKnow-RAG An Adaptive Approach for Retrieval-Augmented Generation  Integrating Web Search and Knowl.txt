WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented
Generation Integrating Web Search and Knowledge Graphs
Weijian Xie
Ping An Bank Co., Ltd.
Shenzhen, Guangdong, China
tsewkviko@gmail.comXuefeng Liang
Tsinghua University
Haidian, Beijing, China
liang1xuefeng@gmail.comYuhui Liu
Central South University
Changsha, Hunan, China
liuyuhui@csu.edu.cn
Kaihua Ni
Shanghai Yiwo Information
Technology Co., Ltd.
Changning, Shanghai, China
nikaihua008@gmail.comHong Cheng
Kwangwoon University, Department
of Computer Engineering
Nowon Gu, Seoul, South Korea
kwchenghong@gmail.comZetian Hu
Tsinghua University
Haidian, Beijing, China
huzt22@mails.tsinghua.edu.cn
Abstract
Large Language Models (LLMs) have greatly contributed to the
development of adaptive intelligent agents and are positioned as
an important way to achieve Artificial General Intelligence (AGI).
However, LLMs are prone to produce factually incorrect informa-
tion and often produce "phantom" content that undermines their
reliability, which poses a serious challenge for their deployment
in real-world scenarios. Enhancing LLMs by combining external
databases and information retrieval mechanisms is an effective path.
To address the above challenges, we propose a new approach called
WeKnow-RAG , which integrates Web search and Know ledge
Graphs into a "Retrieval-Augmented Generation ( RAG )" system.
First, the accuracy and reliability of LLM responses are improved
by combining the structured representation of Knowledge Graphs
with the flexibility of dense vector retrieval. WeKnow-RAG then
utilizes domain-specific knowledge graphs to satisfy a variety of
queries and domains, thereby improving performance on factual
information and complex reasoning tasks by employing multi-stage
web page retrieval techniques using both sparse and dense retrieval
methods. Our approach effectively balances the efficiency and accu-
racy of information retrieval, thus improving the overall retrieval
process. Finally, we also integrate a self-assessment mechanism for
the LLM to evaluate the trustworthiness of the answers it generates.
Our approach proves its outstanding effectiveness in a wide range
of offline experiments and online submissions.
CCS Concepts
•Computing methodologies →Artificial intelligence ;Machine
learning .
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD Cup CRAG Workshop ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-XXXX-X/18/06Keywords
Retrieval-Augmented Generation, Large Language Model, Knowl-
edge Graphs, Domain adaptation fine-tuning
ACM Reference Format:
Weijian Xie, Xuefeng Liang, Yuhui Liu, Kaihua Ni, Hong Cheng, and Zetian
Hu. 2024. WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented
Generation Integrating Web Search and Knowledge Graphs. In Proceedings
of 2024 KDD Cup Workshop for Retrieval Augmented Generation (KDD Cup
CRAG Workshop ’24). ACM, New York, NY, USA, 8 pages.
1 Introduction
Large language models (LLMs) [ 26] have significantly propelled the
development of adaptable intelligent agents, positioning themselves
as a promising pathway towards achieving artificial general intelli-
gence (AGI). Despite these advancements, the inherent nature of
LLMs introduces critical challenges that obstruct their deployment
in real-world production scenarios [ 12]. One of main key issues
is the tendency of LLMs to produce factually incorrect informa-
tion, often resulting in "hallucinated" content that undermines their
reliability [ 40]. [30] demonstrated that GPT-4’s exhibits excellent
performances in responding to questions about both slow-changing
or fast-changing facts, with an accuracy rate below 15%. However,
even for stable (never-changing) facts, GPT-4’s accuracy in address-
ing questions about less popular (torso-to-tail) entities is under 35%
[29].
Researchers have implemented a variety of methods to enhance
knowledge in LLMs, one of which is Retrieval-Augmented Genera-
tion (RAG) method. RAG method enhances LLMs by incorporating
external databases and information retrieval mechanisms [ 38]. This
technique dynamically incorporates relevant information into the
LLMs’ prompt during inference without altering the weights of
the used LLMs. By integrating external knowledge, RAG methods
can reduce hallucinations of LLMs achieving better performance
that can surpass traditional fine-tuning approaches, especially in
applications that require high precision and up-to-date information
[18].
Implementations of RAG methods typically depend on the dense
vector similarity search for retrieval. Yet, this technique, which
divides the corpus into text chunks and uses dense retrieval systems
exclusively, falls short for complex queries [ 18]. Some methods
address this issue by employing metadata filtering or hybrid searcharXiv:2408.07611v2  [cs.CL]  28 Aug 2024
KDD Cup CRAG Workshop ’24, August 25–29, 2024, Barcelona, Spain Xie et al.
techniques [ 34], but these methods are constrained by the pre-
defined scope of metadata by developers. Furthermore, achieving
the necessary granularity to answer complex queries within similar
vector space chunks remains challenging [ 16]. The inefficiency
comes from the method’s inability to selectively retrieve relevant
information, leading to large amounts of chunk data retrieval that
may not directly answer the queries [21].
An ideal RAG system should retrieve only the essential content,
reducing the inclusion of irrelevant information. This is where
Knowledge Graphs (KGs) can assist by providing a structured and
explicit representation of entities and relationships that are more
precise than information retrieval through vector similarity [ 23].
KGs enable searching for "things, not strings" by maintaining exten-
sive collections of explicit facts structured as accurate, mutable, and
interpretable knowledge triples [ 5]. A knowledge triple typically
represents a fact in the format (entity) - relationship →(entity)
[8]. In addition, KGs can expand by continuously incorporating
new information, and experts can build domain-specific KGs to
deliver precise and trustworthy data within particular fields [ 1].
Well-known KGs such as Freebase, Wikidata, and YAGO could serve
as practical examples of this concept. Considerable researches have
emerged at the crossroads of graph-based methodologies and LLMs,
showcasing applications like reasoning over graphs and enhancing
the integration of graph data with LLMs [7].
To tackle the above challenges, we propose a novel approach
named WeKnow-RAG that integrates Web Search and Know ledge
Graphs in a Retrieval-Augmented Generation ( RAG ) system. Our
approach combines the structured representation of KGs with the
flexibility of dense vector retrieval to enhance the accuracy and
reliability of LLM responses.
The main contributions of our work are as follows:
•We develop a domain-specific KG-enhanced RAG system that
adapts to different types of queries and domains, improving
performance on both factual and complex reasoning tasks.
•We introduce a multi-stage retrieval method for web pages
that leverages both sparse and dense retrieval techniques,
effectively balancing efficiency and accuracy in information
retrieval.
•We implement a self-assessment mechanism for LLMs to
evaluate their confidence in generated answers, reducing
hallucinations and improving overall response quality.
•We present an adaptive framework that intelligently com-
bines KG-based and web-based RAG methods based on the
characteristics of different domains.
The proposed WeKnow-RAG won 3rd place in the final evalua-
tion of Task 3 at the Meta KDD CUP 2024, which was assessed using
the Comprehensive RAG Benchmark (CRAG), a factual question-
answering benchmark consisting of numerous question-answer
pairs and mock APIs to simulate web and Knowledge Graph search.
Our experimental results on the CRAG dataset demonstrate the
effectiveness of our approach, achieving significant improvements
in accuracy and reducing hallucinations across various domains
and question types.2 Related Work
In order to improve the accuracy and reliability of LLMs in question
answer tasks, many previous works have proven effective. Accord-
ing to whether the parameters of LLMs to be modified or not, we
categorize them into two main approaches [10]:
•Fine-tuning and calibration : Fine-tuning the LLM on a
specific domain or task can improve its accuracy and reduce
hallucinatory responses. In addition, calibrating the model to
provide uncertainty estimates with responses can help users
assess the reliability of the generated information[ 39]. This
approach requires modifying the parameters of the LLMs.
•External Knowledge Integration : Integrating external
knowledge sources into LLM can help enhance its compre-
hension and reduce hallucinatory responses. These sources
can be massively updated web pages, databases, knowledge
graphs, additional LLMs, etc. RAG is a method of utiliz-
ing many knowledge sources to provide informed answers,
thereby improving the accuracy of LLMs’ generated responses[ 9].
This approach does not require modification of the parame-
ters of the LLMs.
2.1 Fine-tuning and calibration
High-quality QA datasets are crucial for LLMs fine-tuning and
calibration. These datasets typically contain a diverse and well-
structured collection of questions and answers, enabling the model
to learn and generalize across different scenarios.There are some
famous high-quality QA dataset:
•SQuAD [26]:Consisting of over 100,000 questions posed on
a set of Wikipedia articles. The answers are often spans of
text directly extracted from the given passages.
•Natural Questions [17]:Created by Google, this dataset con-
tains real questions from Google Search users, with answers
provided as spans of text from Wikipedia pages.
•TriviaQA [15]:Contains over 650,000 question-answer pairs,
where the questions are sourced from trivia websites, and
the answers are verified against evidence documents.
•MS MARCO [3]:A large-scale dataset that includes real anonymized
queries from Bing search logs, with answers generated by
humans based on a set of retrieved documents.
However, in practical QA scenarios like this competition that
contain a lot of noisy information and html code, it’s hard to use
this dataset directly for fine-tuning. And these open source datasets
only contains a small amount of QA-pairs in special domain like
finance, sports, music and movie.
2.2 External Knowledge Integration
Recent endeavors have leveraged RAG to improve LLMs across
diverse tasks [ 4,11,32], particularly those requiring up-to-date and
accurate knowledge such as question answering (QA), AI4Science,
and software engineering. For instance, Lozano et al. [ 20] devel-
oped a scientific QA system that dynamically retrieves scientific
literature. MolReGPT [ 19] uses RAG to boost ChatGPT’s in-context
learning for molecular discovery. Moreover, RAG has been shown to
effectively mitigate hallucinations in conversational tasks [ 14,27].
WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation ... KDD Cup CRAG Workshop ’24, August 25–29, 2024, Barcelona, Spain
Recent progress in integrating LLMs with KGs has also been re-
markable. Jin et al. [ 13] reviewed this integration comprehensively,
classifying LLM roles as Predictors, Encoders, and Aligners. For
graph-based reasoning, Think-on-Graph [ 28] and Reasoning-on-
Graph [ 22] improve LLMs’ reasoning capabilities by incorporating
KGs. Yang et al. [ 36] suggest enhancing LLMs’ factual reasoning in
different training stages using KGs. In LLM-based QA, Wen et al.’s
Mindmap [ 33] and Qi et al. [ 25] utilize KGs to advance LLM infer-
ence in specific fields like medicine and food. These developments
illustrate the growing effectiveness of combining LLMs and KGs in
improving information retrieval and reasoning.
3 Preliminaries
A RAG QA system takes a question 𝑄as input and outputs an an-
swer𝐴; the answer is generated by LLMs according to information
retrieved from external sources or directly from the knowledge
internalized in the model [ 37]. The answer 𝐴should provide useful
information to answer the question without adding any hallucina-
tions under ideal conditions.
Task description The process to obtain an answer from a ques-
tion can be called a query. For each question 𝑄, both web search
results and Mock APIs for information retrieval are available for
the generation of answer 𝐴. The web search results comprise 50
web page candidates. The Mock Knowledge Graphs (MKGs) contain
structured data pertinent to the queries; the answers may or may
not be present in the MKGs. The Mock APIs accept input parame-
ters, which are often derived from a query, and deliver structured
data from the MKGs to assist in answer generation.
RAG systems are evaluated using a scoring method that rates
response quality as correct (1 point), missing (0 points), and incor-
rect (-1 point). A response is rated as missing if it is ’I don’t know’.
Otherwise, an LLM will be used to determine whether the response
is correct or incorrect [37].
Dataset description The dataset used is the Comprehensive
RAG Benchmark, covering five domains: finance, sports, music,
movie, and open domain, and eight types of questions as detailed in
[37]. Specifically, CRAG includes questions with answers ranging
from seconds to years; it considers the popularity of entities and
covers not only head but also torso and tail facts. It contains sim-
ple factual questions as well as seven types of complex questions,
including comparison, aggregation, set questions and so on, to test
the reasoning and synthesis capabilities of RAG solutions.
For the web search results, we used the question text as the
search query and stored up to 50 web pages retrieved from the
Brave search API. Each web page contains some key-value pairs,
detailed in the following data schema:
•Page Name: The name of the webpage.
•Page Result: The full HTML of the webpage.
•Page Snippet: A short paragraph describing the major con-
tent of the page.
To construct the MKGs, we employed the CRAG KG APIs sup-
plied by the organizer. These APIs incorporated publicly accessible
KG data as well as randomly chosen entities of the same type and
’hard negative’ entities with similar characteristics. The CRAG KG
APIs were formulated with specific parameters to enable structuredsearches within the MKGs, resulting in a compilation of 38 mock
APIs [37].
4 Methods
We proposed a approach named WeKnow-RAG for End-to-End
Retrieval-Augmented Generation, as illustrated in Fig. 1. The pipeline
comprises a KG workflow and a web search workflow to address the
End-to-End Retrieval-Augmented Generation challenge, ultimately
integrating both effectively. We will detail each component of the
pipeline in the following sections.
4.1 Web-based RAG
4.1.1 Web Content Parsing. To utilize the data source in RAG ap-
proaches, content parsing is a critical process. It helps to compre-
hend unstructured data, convert it into structured data, and acquire
the necessary information to answer the question.
To obtain more complete content for the source page, we parse
the original HTML source code with the BeautifulSoup library1.
4.1.2 Chunking. Chunking is the process of dividing a document
into multiple paragraphs. The effectiveness of chunking directly
influences the performance of question-answering systems.
Various chunking strategies exist, including sentence-level, token-
level, and semantic-level approaches [ 31]. In our methods, we opt
for token-level chunking for ease of use.
To determine the optimal configurations, a series of experiments
are carried out to compare various chunk sizes in Section 5.3.
4.1.3 Multi-stage Retrieval. In the context of RAG, efficiently re-
trieving relevant documents from the data source is crucial for
obtaining accurate answers and minimizing hallucinations.
We employ a multi-stage retrieval approach, as illustrated in
Figure 2, to strike a balance between the effectiveness and efficiency
of retrieving relevant paragraphs based on a query.
The explanation provided in Figure 2 effectively outlines the ini-
tial stage of utilizing sparse retrieval to gather candidates from page
result chunks and snippet chunks. It clarifies that sparse retrieval is
employed to efficiently obtain pertinent paragraphs. Additionally,
it highlights the selection process of the top 𝐾candidates based on
their BM25 scores, as denoted in Eq. 1.
Score(𝑞𝑢𝑒𝑟𝑦,𝐶 𝑖)=
∑︁
𝑞𝑗∈𝑞𝑢𝑒𝑟 𝑦IDF(𝑞𝑗)·𝑓(𝑞𝑗,C𝑖)·(𝑘1+1)
𝑓(𝑞𝑗,C𝑖)+𝑘1·(1−𝑏+𝑏·|C𝑖|
avg_dl).(1)
Where:
•𝑞𝑗is a term in the query. IDF(𝑞𝑗)is the inverse document
frequency of term 𝑞𝑗.
•𝑓(𝑞𝑗,C𝑖)is the term frequency of term 𝑞𝑗in document C 𝑖.
•𝑘1is a term frequency saturation parameter (typically set to
1.5).
•𝑏is a length normalization parameter (typically set to 0.75).
•|C𝑖|is the length of document C 𝑖.
•avg_dl is the average document length in the corpus.
By leveraging the strengths of different algorithms, hybrid re-
trieval can achieve better performance than any single algorithm. In
1BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/
KDD Cup CRAG Workshop ’24, August 25–29, 2024, Barcelona, Spain Xie et al.
QueryKG Result
Valid?Integrated 
Module
AnswerKG Workflow 
Web Workflow 
Web Pages Mock APIs
URL
 Snippet HTML Title
Open Movie Sports Finance Music
Question 
Query Time
Interaction ID
Ground
Truth
LLM EvaluatorAutomatic 
evaluation
 Web Pages Indexing
HTML 
ParserToken 
Chunker
Sparse
Retrieval
Sparse
RetrievalDense
Retrieval 
and 
Reranking1ststage2ndstageMulti -stage Retrieval
Query
SA Generation
Prompt
 Content
Chat LLM
Answer ConfidenceDropQuery
Domain 
Classification
Chat LLM
Prompt
 DomainTool Calling
Choice 
Chat LLM
Domain
Tool
List1. (Tool,Args ) 
2. (Tool,Args ) 
…Generation
Chat LLMTool
Call
Post -
Processing
Figure 1: WeKnow-RAG pipeline for End-to-End Retrieval-Augmented Generation.
Figure 2: Multi-stage Retrieval methods.
the second stage, We employ hybrid search using the most common
pattern [ 9], which combines a sparse retriever (BM25) with a dense
retriever (embedding similarity), as their strengths are complemen-
tary. The sparse retriever excels at finding relevant documents
based on keywords, while the dense retriever identifies relevant
documents based on semantic similarity.
The sparse retrieval approach in the second stage uses the BM25
score, just like in the first stage, to select the top 𝑀candidates based
on the highest score.
The dense retrieval approach in second stage comprises dense
embedding retrieval and reranking methods. A bge-large-en-v1.5
[35] embedding model is employed for dense embedding retrieval,
selecting the top 𝑁candidates based on the highest embedding
similarity with the query. Subsequently, bge-reranker-large [ 35]
reranker model is applied to obtain a more accurate relevance score
and rerank the top 𝑁candidates in the reranking approach of Figure
2.Notice that𝐾>>𝑀+𝑁, which allowed us to obtain K relevant
candidates in the first stage with low latency. This enabled us to con-
duct dense retrieval to obtain N relevant candidates and reranking
using models with more parameters (such as the bge-reranker-large
model, which has 0.5B parameters) to obtain M relevant candidates
with fewer data in the second stage.
It is worth mentioning that to include as much relevant informa-
tion as possible, we added not only the parsed HTML content in
the database but also the page name and page snippet for retrieval.
4.1.4 Answer Generation with Self-Assessment. Hallucination is
a common issue in the generated content of LLMs. In order to
address this challenge, we propose a self-assessment mechanism
that evaluates the confidence level of the generated answer and
determines its suitability for selection.
In particular, we instruct the LLM to indicate the confidence
level (high, medium, low) corresponding to the generated answer.
We decide to accept the answer only when the confidence level
meets the specified requirement. If the confidence level is below the
threshold, we conclude that the LLM lacks sufficient confidence to
answer the question and will output "I don’t know". Experiments on
performance with various confidence level thresholds are detailed in
Section 5.3, and the complete instructions for the LLM are provided
in Appendix A.1.
4.2 Knowledge Graphs
By extracting knowledge from structured and unstructured data
sources, a domain-specific knowledge graph can be constructed.
This includes entities, relationships, and facts related to each do-
main while using state-of-the-art information extraction techniques
to ensure that the knowledge is rich and up-to-date. This type of
WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation ... KDD Cup CRAG Workshop ’24, August 25–29, 2024, Barcelona, Spain
knowledge base plays a key role in various question-answering
tasks, drawing inspiration from the construction of large knowledge
bases such as DBpedia[ 2]. By parsing a question into step-by-step
sub-questions or sub-functions, establishing the relationship be-
tween the sub-questions and the model application programming
interface (API), and solving a series of sub-questions using condi-
tional lookups, set searches, comparisons, aggregations, multi-hop
queries, and post-processing operations, the entire question can be
answered.
4.2.1 Domain classification. Our method categorizes the question
into domains by making an initial call to an LLM. At this point,
the LLM acts as an intelligent linguistic categorization tool. For
example, for the question, "How much did Funko open at today?",
it is automatically categorized as the financial domain. For the
question, "What label is Taylor Swift signed to?", it is automatically
categorized as the music domain. Domain categorization is critical
for subsequent processing. For this reason, in the four domains of
movie, sport, finance, and music, we suggest that the model should
only make a determination if it has more than 90% certainty. When
certainty is below this threshold, the domains are categorized as
open domains.
4.2.2 Query generation. Depending on the domain categorization
of the problem, the corresponding domain cueing phase is entered,
where the second call to the LLM is made. The hints provided are
different for each domain, and the model needs to return structured
analysis results. For example, in the music domain, it needs to return
three kinds of functions, categorized into artist, song, and year,
with each category containing a variety of attribute information.
For the instruction to generate the function calling query, refer to
Appendix A.2. Simultaneously, questions that do not fall into these
three categories are answered directly using the model or the RAG
workflow. The query generator will transform the analysis results
into structured queries compatible with the KG API. For example,
a multi-hop problem will be decomposed into a series of API calls,
each of which resolves a link in the information chain.
4.2.3 Answer Retrieval and Post-processing. A set of candidate an-
swers is retrieved by performing a structured query on the KG
through an application programming interface (API). For post-
processing problems, additional reasoning is applied. We employ
a rule-based system that utilizes machine learning techniques to
handle temporal reasoning, numerical computation, and logical
reasoning [ 6]. With this hybrid approach, we can identify incon-
sistencies between the retrieved data and the problem hypotheses,
effectively solving challenging false premise problems.
4.3 Integrated method
The diverse nature of the CRAG dataset requires a dynamic ap-
proach that adapts to the distinct characteristics of each domain,
particularly in terms of the velocity at which query information
evolves. To address this, we characterize the time distribution of
each domain, by analyzing the key "static-or-dynamic" defined in
the dataset, which are categorized as "static", "slow-changing", "fast-
changing" and "real-time", and then propose an adaptive framework
that intelligently balances the use of Knowledge Graphs (KGs) and
Web-based RAG methods.For stable domains such as the Encyclopedia Open domain,
where the velocity of query information change is minimal, our
system follows the rule of prioritizing the output of KG Workflow
and not activating the whole Web-based RAG Workflow, as demon-
strated in Figure 1. The robustness and reliability of KGs in these
domains ensure high accuracy for questions that do not necessitate
up-to-the-minute data. This approach aligns with the findings of
Neumaier et al.[ 24], who underscore the effectiveness of KGs in
stable informational contexts.
For domains with gradual information change, such as Music
and Movies, we maintain the primacy of KGs while incorporating
periodic updates to capture the latest information. This strategy
ensures that our KGs remain relevant for answering queries that
may involve recent but not instantaneous changes. The update
frequency is determined by a domain-specific change detection
algorithm, which is controlled the LLM.
5 Experiments
5.1 Experimental settings
We thoroughly introduce our experimental settings, encompassing
the datasets, evaluation metrics, and parameter settings.
Datasets. The entire CRAG dataset contains over 2000 questions,
and testing on the whole dataset would be very time-consuming.
During offline testing, to quickly iterate and optimize, we tested on
a subset of the CRAG dataset containing 200 questions.
Evaluation metrics. For each question, we use a three-way
scoring system, assigning 1 for correct answers, -1 for incorrect
answers, and 0 for missing answers. An answer is considered ac-
curate if it exactly matches the ground truth and missing if it is ’I
don’t know’. Otherwise, we use model-based automatic evaluation
with GPT-4 to determine whether the response is correct or incor-
rect. There are four evaluation metrics: Accuracy ,Hallucination ,
Missing , and Score . Accuracy, Hallucination, and Missing repre-
sent the percentage of accurate, incorrect, and missing answers
in the test set, respectively, while Score is the difference between
Accuracy and Hallucination.
Parameter settings. We set the number 𝐾of retrieving relevant
candidates in the first stage of Web-based RAG as 200 for parsed
page result chunks and page snippet chunks, respectively. The num-
ber𝑀of sparse retrieval candidates is set as 5, and the number
of dense retrieval candidates 𝑁is set as 20 in the second stage. 𝐾,
𝑀,𝑁are selected empirically and include correct answer informa-
tion whenever possible. We employed the bge-large-en-v1.5 [ 35] as
the dense retrieval model, bge-reranker-large [ 35] as the reranker
model, and llama-3-70b-instruct-awq (an awq quantization version
of Meta-Llama-3.1-70B-Instruct) as our Chat LLM model. All of
them can be downloaded from the Hugging Face Hub2.
5.2 Main results
The final submission for Task 3 included two versions demonstrat-
ing online automated assessment performance, as shown in the
Table 1. Version 2 made three main improvements relative to Ver-
sion 1. First, it enhanced the handling of prompts for open domains.
Second, it improved the model’s focus on false premise queries by
2Hugging face: https://huggingface.co/
KDD Cup CRAG Workshop ’24, August 25–29, 2024, Barcelona, Spain Xie et al.
Table 1: Online submission results of our method on Task 3
in KDD Cup 2024 - CRAG Round 2.
Model Accuracy Hallucination Missing Score
version1 0.393 0.319 0.288 0.0743
version2 0.409 0.316 0.276 0.0929
Table 2: Experimental results of offline testing of our method.
Model Acc. Hall. Missing Score
Single domain 0.100 0.035 0.865 0.0640
+ Four domains 0.125 0.025 0.850 0.1000
+ Classification 0.340 0.190 0.470 0.1499
+ Open Optimize 0.340 0.185 0.475 0.1550
including examples of such queries in the prompts. Third, the hints
related to opening, closing, and price were adjusted. Overall, the
strategy of the entire method involves first using the KG process
to directly find answer-related information, strictly requiring the
KG part to answer only questions with certainty to significantly
reduce the error rate. The RAG process is then used to synthesize
the information provided by the KG or from web pages to deliver
the final answer to the question.
The competition accepted a limited number of submissions. To
thoroughly validate the method’s enhancement, a local test set
comprising 200 data points was designed offline, primarily focusing
on validating the efficacy of the KGs component. The method’s
efficacy is underscored in Table 2. Initially, a classification prompt
was devised for accurate categorization by the LLM, followed by
a domain-specific prompt for movies, resulting in a base score of
0.064. Progressing to four domains and refining the strategy led
to consistent improvement. While the KGs for individual domains
were well-crafted, a lack of robust classification capability posed a
significant constraint on further enhancement. To address this, an
illegal problem optimization was integrated into the classification
prompt, ensuring the model’s classification into the sports, movies,
music, and finance domains with a reasonable degree of certainty,
boosting the score to 0.1499. Subsequent refinements focusing on
detailed optimization for the finance domain and open domains
further enhanced the performance.
5.3 Ablation study
To enhance performance in each phase, a series of experiments are
carried out using a subset of the CRAG dataset.
Chunk size. In the chunking process of RAG, the size of the
chunk plays a critical role in obtaining semantically complete para-
graphs. We conducted experiments with various chunk sizes as
illustrated in Table 3. The results indicate that a chunk size of 750
yields the best performance in a small subset of CRAG dataset. In
Round 1 submissions, we experimented with chunk sizes of 500
and 750. After comparing the results, we determined that a chunk
size of 500 yielded better outcomes. Therefore, we have decided to
stick with a chunk size of 500 for the final submission.Table 3: An analysis of various chunk sizes on a subset of
the CRAG dataset using the Web-based RAG approach exclu-
sively.
Chunk size Accuracy Hallucination Missing Score
300 0.38 0.35 0.27 0.03
500 0.38 0.28 0.34 0.10
750 0.41 0.28 0.31 0.13
1000 0.40 0.30 0.30 0.10
Confidence level. In the self-assessment phase of answer gen-
eration, we conduct experiments with different confidence levels
indicated by the LLM. The results in Table 4 show that when the
confidence level is "high", it achieves the highest score. We found
that by using the confidence level threshold, the extra accuracy is
significantly enhanced.
5.4 Model analysis
The experimental results above demonstrate that the WeKnow-RAG
approach we proposed achieves effective performance by leveraging
the contribution of each module in the Web-based RAG workflow
and KG workflow.
KG workflow provides accurate answers with minimal errors by
using function calling to extract specific information from knowl-
edge graphs.
While the web-based RAG workflow provides more relevant
information from a large number of web pages through multi-stage
retrieval, it also reduces hallucination through a self-assessment
approach.
6 Conclusions
While LLM has shown promising applications across various do-
mains, highlighting substantial development opportunities and re-
search significance, its intrinsic traits can lead to outputs that devi-
ate from factual accuracy, generating "hallucinatory" content that
poses a significant obstacle to research advancement. To address
the above challenges, we propose a new approach called WeKnow-
RAG that integrates Web search and Knowledge Graphs into a
RAG system. Our approach combines the structured representation
of knowledge graphs with the flexibility of dense vector retrieval
to improve the accuracy and reliability of LLM responses. By de-
signing a specialized RAG system, WeKnow-RAG utilizes domain-
specific knowledge graphs to satisfy a wide range of queries and
domains, improving the performance of factual information and
complex reasoning tasks. By employing multi-stage web page re-
trieval techniques, both sparse and dense retrieval methods are
used. Our approach effectively balances the efficiency and accu-
racy of information retrieval, thus enhancing the entire retrieval
process. Meanwhile, we integrate a self-assessment mechanism
for LLMs to evaluate the trustworthiness of the answers they gen-
erate. This mechanism reduces illusions and thus improves the
quality of model-generated answers.The WeKnow-RAG framework
introduces an adaptive methodology that intelligently combines
KG-based RAG methods with web-based RAG methods. This inte-
gration is tailored to the unique characteristics of different domains
WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation ... KDD Cup CRAG Workshop ’24, August 25–29, 2024, Barcelona, Spain
Table 4: An analysis of various confidence level thresholds on a subset of the CRAG dataset using the Web-based RAG approach
exclusively.
Confidence level threshold Exact Accuracy Accuracy Hallucination Missing Score
low (w/o Self-Assessment) 0.05 0.38 0.28 0.34 0.10
medium 0.15 0.33 0.28 0.39 0.05
high 0.14 0.38 0.26 0.36 0.12
and adapts to the speed of information evolution, thus ensuring
optimal performance in dynamic information environments. Our
approach has demonstrated outstanding effectiveness in extensive
offline experiments and online submissions.
7 Acknowledgments
The competition was successfully concluded with thanks to the
scientists and engineers from Meta Reality-Labs and the Hong Kong
University of Science and Technology (HKUST, HKUST-GZ) for
their great support and valuable data during the competition. The
authors also acknowledge the computer resources given by Central
South University’s High Performance Computing Center (HPC).
References
[1]Yuan An, Jane Greenberg, Xiaohua Hu, Alexander Kalinowski, Xiao Fang, Xin-
tong Zhao, Scott McClellan, F. Uribe-Romo, Kyle Langlois, Jacob Furst, Diego A.
Gómez-Gualdrón, Fernando Fajardo-Rojas, Katherine Ardila, S. Saikin, Corey A.
Harper, and Ron Daniel. 2022. Exploring Pre-Trained Language Models to
Build Knowledge Graph for Metal-Organic Frameworks (MOFs). 2022 IEEE
International Conference on Big Data (Big Data) (2022), 3651–3658. https:
//doi.org/10.1109/BigData55660.2022.10020568
[2]Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,
and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In international
semantic web conference . Springer, 722–735.
[3]Payal Bajaj, Daniel Campos, Nick Craswell, et al .2018. MS MARCO: A
Human-Generated MAchine Reading COmprehension Dataset. arXiv preprint
arXiv:1611.09268 (2018).
[4]Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther-
ford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bog-
dan Damoc, Aidan Clark, et al .2022. Improving language models by retrieving
from trillions of tokens. In International conference on machine learning . PMLR,
2206–2240.
[5]E. Cambria, Shaoxiong Ji, Shirui Pan, and Philip S. Yu. 2021. Knowledge graph
representation and reasoning. Neurocomputing 461 (2021), 494–496. https:
//doi.org/10.1016/j.neucom.2021.05.101
[6]Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and
William Wang. 2020. Hybridqa: A dataset of multi-hop question answering over
tabular and textual data. arXiv preprint arXiv:2004.07347 (2020).
[7]Nurendra Choudhary and C. Reddy. 2023. Complex Logical Reasoning over
Knowledge Graphs using Large Language Models. ArXiv abs/2305.01157 (2023).
https://doi.org/10.48550/arXiv.2305.01157
[8]Jens Dörpinghaus and Marc Jacobs. 2020. Knowledge detection and discovery
using semantic graph embeddings on large knowledge graphs generated on
text mining results. In 2020 15th conference on computer science and information
systems (fedcsis) . IEEE, 169–178.
[9]Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,
Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2024.
Retrieval-Augmented Generation for Large Language Models: A Survey.
arXiv:2312.10997 [cs.CL] https://arxiv.org/abs/2312.10997
[10] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian
Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting
Liu. 2023. A Survey on Hallucination in Large Language Models: Principles,
Taxonomy, Challenges, and Open Questions. arXiv:2311.05232 [cs.CL] https:
//arxiv.org/abs/2311.05232
[11] Gautier Izacard and Edouard Grave. 2020. Distilling knowledge from reader to
retriever for question answering. arXiv preprint arXiv:2012.04584 (2020).
[12] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,
Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in
natural language generation. Comput. Surveys 55, 12 (2023), 1–38.[13] Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. 2023.
Large language models on graphs: A comprehensive survey. arXiv preprint
arXiv:2312.02783 (2023).
[14] Xu Jing and Szlam Arthur. 2021. Beyond goldfish memory: Long-term open-
domain conversation. arXiv preprint arXiv:2107.07567 (2021).
[15] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A
Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.
InProceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (ACL) . 1601–1611.
[16] Weize Kong, Swaraj Khadanga, Cheng Li, Shaleen Kumar Gupta, Mingyang
Zhang, Wensong Xu, and Michael Bendersky. 2022. Multi-aspect dense retrieval.
InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining . 3178–3186.
[17] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, et al .2019. Natural
Questions: a Benchmark for Question Answering Research. Transactions of the
Association for Computational Linguistics 7 (2019), 453–466.
[18] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,
Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
et al.2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.
Advances in Neural Information Processing Systems 33 (2020), 9459–9474.
[19] Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, and
Qing Li. 2024. Empowering molecule discovery for molecule-caption transla-
tion with large language models: A chatgpt perspective. IEEE Transactions on
Knowledge and Data Engineering (2024).
[20] Alejandro Lozano, Scott L Fleming, Chia-Chun Chiang, and Nigam Shah. 2023.
Clinfo. ai: An open-source retrieval-augmented large language model system for
answering medical questions using scientific literature. In PACIFIC SYMPOSIUM
ON BIOCOMPUTING 2024 . World Scientific, 8–23.
[21] Y. Luan, Jacob Eisenstein, Kristina Toutanova, and M. Collins. 2020. Sparse,
Dense, and Attentional Representations for Text Retrieval. Transactions of the
Association for Computational Linguistics 9 (2020), 329–345. https://doi.org/10.
1162/tacl_a_00369
[22] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Reasoning
on graphs: Faithful and interpretable large language model reasoning. arXiv
preprint arXiv:2310.01061 (2023).
[23] Jason Mohoney, Anil Pacaci, Shihabur Rahman Chowdhury, Ali Mousavi, Ihab F
Ilyas, Umar Farooq Minhas, Jeffrey Pound, and Theodoros Rekatsinas. 2023. High-
throughput vector similarity search in knowledge graphs. Proceedings of the
ACM on Management of Data 1, 2 (2023), 1–25.
[24] Sebastian Neumaier, Jürgen Umbrich, and Axel Polleres. 2016. Automated quality
assessment of metadata across open data portals. Journal of Data and Information
Quality (JDIQ) 8, 1 (2016), 1–29.
[25] Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan, and Yongfeng Huang. 2023. Foodgpt:
A large language model in food testing domain with incremental pre-training
and knowledge graph prompt. arXiv preprint arXiv:2308.10173 (2023).
[26] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.
Squad: 100,000+ questions for machine comprehension of text. arXiv preprint
arXiv:1606.05250 (2016).
[27] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021.
Retrieval augmentation reduces hallucination in conversation. arXiv preprint
arXiv:2104.07567 (2021).
[28] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun
Gong, Heung-Yeung Shum, and Jian Guo. 2023. Think-on-graph: Deep and
responsible reasoning of large language model with knowledge graph. arXiv
preprint arXiv:2307.07697 (2023).
[29] Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. 2023. Head-
to-tail: How knowledgeable are large language models (llm)? AKA will llms
replace knowledge graphs? arXiv preprint arXiv:2308.10168 (2023).
[30] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris
Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al .2023. Freshllms: Refresh-
ing large language models with search engine augmentation. arXiv preprint
arXiv:2310.03214 (2023).
[31] Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu,
Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, et al .2024. Search-
ing for Best Practices in Retrieval-Augmented Generation. arXiv preprint
KDD Cup CRAG Workshop ’24, August 25–29, 2024, Barcelona, Spain Xie et al.
arXiv:2407.01219 (2024).
[32] Shi Weijia, Min Sewon, Yasunaga Michihiro, Seo Minjoon, James Rich, Lewis
Mike, and Yih Wen-tau. 2023. REPLUG: Retrieval-augmented black-box language
models. ArXiv: 2301.12652 (2023).
[33] Yilin Wen, Zifeng Wang, and Jimeng Sun. 2023. Mindmap: Knowledge graph
prompting sparks graph of thoughts in large language models. arXiv preprint
arXiv:2308.09729 (2023).
[34] Wei Wu, Junlin He, Yu Qiao, Guoheng Fu, Li Liu, and Jin Yu. 2022. HQANN:
Efficient and robust similarity search for hybrid queries with structured and
unstructured constraints. In Proceedings of the 31st ACM International Conference
on Information & Knowledge Management . 4580–4584.
[35] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023.
C-Pack: Packaged Resources To Advance General Chinese Embedding.
arXiv:2309.07597 [cs.CL]
[36] Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, and Xindong Wu. 2023.
Chatgpt is not enough: Enhancing large language models with knowledge graphs
for fact-aware language modeling. arXiv preprint arXiv:2306.11489 (2023).
[37] Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal
Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian
Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen
Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga,
Anuj Kumar, Wen tau Yih, and Xin Luna Dong. 2024. CRAG – Comprehensive
RAG Benchmark. arXiv:2406.04744 [cs.CL] https://arxiv.org/abs/2406.04744
[38] Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji,
and Meng Jiang. 2022. A survey of knowledge-enhanced text generation. Comput.
Surveys 54, 11s (2022), 1–38.
[39] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang,
Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al .2023. Instruction Tuning for
Large Language Models: A Survey. arXiv preprint arXiv:2308.10792 (2023).
[40] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting
Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al .2024. Siren’s song in the AI
ocean: A survey on hallucination in large language models, 2023. URL https://arxiv.
org/abs/2309.01219 (2024).
A Prompts
A.1 Answer Generation with Self-Assessment
The prompt for Web-based RAG to generate an answer and the
corresponding confidence level is displayed below.
System:
You are provided with a question, current time and various
references. Your task is to answer the question succinctly, using
the FEWEST words possible. If you are absolutely sure, more
than 97\% confident, please answer directly. If you are not sure,
please respond with 'I don 't know '.
Please answer the question and provide the confidence tier (high,
medium, low) for your answer. Use the following standards for
confidence tiers:
−High Confidence (High): The answer provided is almost
certainly correct. There is strong evidence or overwhelming
consensus supporting this answer. The model has a high level of
certainty and little to no doubt about this answer.
−Medium Confidence (Medium): The answer provided is likely
to be correct. There is some evidence or reasonable support for
this answer, but it is not conclusive. The model has some level of
certainty but acknowledges that there is a possibility of error or
alternative answers.
−Low Confidence (Low): The answer provided is uncertain or
speculative. There is little to no solid evidence or support for this
answer. The model has significant doubts about the accuracy of
this answer and recognizes that it could easily be incorrect.Output the result in JSON format, answer is a string, confidence
is a string (value is one of high, medium, low), for example
output: {"answer": "mayon volcano", "confidence": "medium"}
User:
Context: {references}
Current Time: {query_time}
Question: {query}
Output:
A.2 Query Generation for KG function calling
The prompt for generating function calls for querying knowledge
graphs is presented below. It displays a part of the function call
prompt in the Music Domain, the complete version includes several
additional functions.
System:
You are an AI agent of linguist talking to a human. ... For all
questions you MUST use one of the functions provided.
You have access to the following tools:
{
"type": "function",
"function": {
"name": "get_artist_info",
"description": "Useful for when you need to get information
about an artist, such as singer, band",
"parameters": {
"type": "object",
"properties": {
"artist_name": {
"type": "string",
"description": "the name of artist or band",
},
"artist_information": {
"type": "string",
"description": "the kind of artist information, such as
birthplace, birthday, lifespan, all_works, grammy_count,
grammy_year, band_members",
}
},
"required": ["artist_name", "artist_information"],
},
},
}
...
To use these tools you must always respond in a Python function
call based on the above provided function definition of the tool!
For example:
{"name": "get_artist_info", "params": {"artist_name": "justin bieber
", "artist_information": "birthday"}}
User:
{query}
