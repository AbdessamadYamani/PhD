Game Development as Human-LLM Interaction
Jiale Hong*, Hongqiu Wu∗, Hai Zhao†
Department of Computer Science, Shanghai Jiao Tong University
{hongjiale, wuhongqiu }@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn
Abstract
Game development is a highly specialized task that relies on
a complex game engine powered by complex programming
languages, preventing many gaming enthusiasts from han-
dling it. This paper introduces the Interaction-driven Game
Engine (IGE) powered by LLM, which allows everyone to de-
velop a custom game using natural language through Human-
LLM interaction. To enable an LLM to function as an IGE,
we instruct it to perform the following processes in each turn:
(1)Pscript : configure the game script segment based on the
user’s input; (2)Pcode: generate the corresponding code snip-
pet based on the game script segment; (3)Putter : interact
with the user, including guidance and feedback. We propose
a data synthesis pipeline based on the LLM to generate game
script-code pairs and interactions from a few manually crafted
seed data. We propose a three-stage progressive training strat-
egy to transfer the dialogue-based LLM to our IGE smoothly.
We construct an IGE for poker games as a case study and
comprehensively evaluate it from two perspectives: interac-
tion quality and code correctness. The code and data are avail-
able at https://github.com/alterego238/IGE.
Introduction
A game engine is a software framework primarily designed
for the development of games and generally includes rele-
vant libraries and support programs (Valencia-Garc ´ıa et al.
2016). Game engines help streamline the game development
process, enabling developers to focus more on designing
gameplay and content. Popular game engines include Unity,
Unreal Engine, CryENGINE, etc.
Game development is a highly specialized task that re-
lies on game engines powered by complex programming
languages. The learning curve can be steep for those who
wish to develop games based on their own designs. To
make game development accessible to everyone, we pro-
pose the Interaction-driven Game Engine (IGE) , powered by
LLMs (Brown et al. 2020; Achiam et al. 2023; Touvron et al.
2023). This engine is designed to support the development of
custom games using natural language through Human-LLM
interaction. Thus all game enthusiasts, whether developers
or players, can use IGE to engage in interactive game devel-
opment using natural language.
*Equal contribution;†Corresponding author.Compared to traditional game engines, our IGE elimi-
nates the learning curve. While traditional game engines
provide users with software interfaces powered by complex
technologies and programming languages, our IGE offers a
more flexible natural language interface powered by LLM.
One can simply input natural language under the guidance
of the engine through Human-LLM interaction. In IGE, a
user’s natural language input is equivalent to calling soft-
ware interfaces in a traditional game engine. The LLM
generates implementation code based on the user’s input,
mirroring the process of implementing software interfaces
through complex technologies and programming languages
in traditional game engines.
IGE is based on large language models (LLMs), which
have shown exceptional capabilities in natural language pro-
cessing across various aspects. In this work, we explore the
joint capability of interaction and programming of the LLM
to serve as a game engine, enabling development through
natural language via Human-LLM interaction. We instruct
the LLM to perform the following processes in each turn:
(1)Pscript : configure the game script segment based on the
user’s input; (2)Pcode: generate the corresponding code snip-
pet based on the game script segment; (3)Putter : interact
with the user, including guidance and feedback.
We propose a comprehensive training paradigm to fine-
tune an LLM to excel as an IGE, rather than relying solely
on prompting. There are two main challenges. First, it is an
exhausting process to acquire a large number of game script-
code pairs. We propose an efficient data synthesis pipeline
to generate game script-code pairs automatically from a few
manually crafted seed data. Moreover, our IGE framework
requires the LLM to learn to perform Pscript ,Pcode, and
Putter step by step, which places high demands on the joint
capability of interaction and programming. However, simul-
taneously mastering these capabilities is a considerable chal-
lenge for the model. Additionally, a straightforward strat-
egy to train on sufficient complete interaction data is ineffi-
cient. Therefore, we propose a three-stage progressive train-
ing strategy to transfer the dialogue-based LLM to our IGE
smoothly.
Eventually, we construct an IGE for Poker, a worldwide
card game, e.g. Texas hold’em . We utilize the proposed
data synthesis pipeline to generate the corresponding dataset
and fine-tune an IGE using the presented strategy. Then wearXiv:2408.09386v1  [cs.AI]  18 Aug 2024
propose a fine-grained evaluation process, measuring the
performance from two perspectives: interaction quality and
code correctness.
In summary, this paper:
• introduces the IGE framework for game development as
Human-LLM interaction;
• presents the data generation technique that fuels the
learning of IGE;
• proposes a three-stage progressive training strategy for
effectively training IGE;
• constructs an IGE for poker games and evaluates its per-
formance from two perspectives: interaction quality and
code correctness.
Related works
AI for Games AI for games is an exciting area in AI re-
search. A great amount of recent work studies learning for
agents, e.g. as game players for Atari (Mnih et al. 2013),
Minecraft (Fan et al. 2022; Wang et al. 2023a), StarCraft,
(Vinyals et al. 2019), NetHack (K ¨uttler et al. 2020; Lowe
et al. 2020), Werewolf (Xu et al. 2023); as non-play char-
acters (NPCs) (Shanahan, McDonell, and Reynolds 2023;
Uludagli and Oguz 2023); player assistants (Gallotta et al.
2024); game commentators (Eladhari 2018; Ranella and
Eger 2023). Recently, some works focus on building a neu-
ral engine based on LLMs. Delta-Engine (Wu et al. 2024b)
drives games as a playground for a scalable virtual world,
enabling expansion by generating new code based on the
base engine. IDGE (Wu et al. 2024a) autoregressively pre-
dicts in-game states based on player actions, functioning
more like a game runtime environment that supports game
creation by simple natural language instructions as a script.
In comparison, our IGE serves as a development framework
for creating games, similar to a traditional game engine.
LLMs as Training Data Generators With the immense
power demonstrated by large language models(LLMs), re-
searchers have recently explored their potential as as training
data generators (Yu et al. 2024a). Such applications include
generating tabular data (Borisov et al. 2022), medical dia-
logue (Chintagunta et al. 2021), sentence pairs (Schick and
Sch¨utze 2021), role-play dialogue (Shao et al. 2023a), in-
struction data (Peng et al. 2023; Shao et al. 2023b; Sun et al.
2024; Wang et al. 2022), etc.. In this paper, we propose a
data synthesis pipeline that leverages LLMs as training data
generators to produce game script-code pairs and user-LLM
interactions from a few manually crafted seed data.
Progressive Training Strategy Progressive training strat-
egy is commonly employed in LLM training. Training
on progressively increasing sequence length data in multi-
stages is used to mitigate computational costs and enhance
data efficiency in both the pre-training (Jin et al. 2023;
Dubey et al. 2024) and post-training (Liu et al. 2024) phases.
Curriculum learning (Bengio et al. 2009), a specialized form
of progressive training, gradually increases the complexity
of data samples during the training process. Recent stud-
ies show the promising role of curriculum learning in em-
powering the language models to tackle more challengingtasks (Vakil and Amiri 2023; Wu et al. 2023, 2024a). In this
paper, we propose a three-stage progressive training strategy
to transfer the dialogue-based LLM to our IGE smoothly.
This strategy also aligns with the principles of curriculum
learning.
IGE
In this section, we present our IGE framework, illustrated in
Figure 1. Addtionally, we provide an example in Figure 2.
An Overview of IGE Framework
The IGE framework introduces a new paradigm of game de-
velopment as Human-LLM interaction. In user-LLM inter-
actions, the user provides instructions for their game con-
cept in natural language under the guidance of LLM, along
with feedback to the LLM. The LLM guides the user in re-
fining and clarifying essential details about the game, while
also offering feedback. To enable the LLM to provide ef-
fective guidance, we predefine a generic script tailored to
a specific type of game. Despite the various possible vari-
ants of a specific type of game, they often share common
elements such as rules and flow, making a generic script fea-
sible. Except for interaction with the user, the LLM gen-
erates script segments and code snippets to implement the
user’s game concept in each turn. In the meantime, the code
snippets are stored, building toward the eventual complete
game code, CustomGame . After the game is fully developed
through multi-turn interactions, a code interpreter is used to
execute the CustomGame code for play.
From Multi-turn Human-LLM Interaction to IGE
The complete process of IGE framework can be seen as a
multi-turn human-LLM interaction. We first formulate the
multi-turn Human-LLM interaction and then extend this
concept to our IGE framework.
In a multi-turn Human-LLM interaction, both the user in-
put and the LLM’s output may be related to the interaction
history, such as references to prior content. The interaction
history htat turn tcan be simply defined as:
ht=∅ ift= 0
{(iτ, oτ)|τ= 1,2, . . . , t }ift >0(1)
where the subscript trefers to the increasing number of
turns, itrefers to the user input and otrefers to the LLM’s
output, formulated as:
ot=Fθ(ht−1, it) (2)
where Fθrefers to the LLM, and θdenotes its parameters.
Consequently, an LLM with parameters θseeks to maximize
the likelihood:
TX
t=1logpθ(ot|ht−1, it). (3)
where Trefers to the total number of interaction turns.
The distinction between IGE and a general multi-turn
Human-LLM interaction lies in the specialization of the in-
put and output. The user input itconsists of instructions
User
Interaction
Natural Languague
Instruction and Feedback
③ Interaction
Natural Languague
Guidance and FeedbackLLM
CustomGame② Code snippet
InterpreterPlayScript ① Script segment
Figure 1: IGE framework. The LLM processes the user’s input in the orange stream, while simultaneously generating script in
the yellow stream, code in the green stream, and interaction in the blue stream.
[start interactive game development]
Ther e should be 3 players. The min and max bet should be 10, 1000 r espectively .Finally , add a flopx phase, wher e discard one card fr om the deck and flop x cards fr om the deck to the
community .
The flow should be: start, shuffle, blind, deal2, bet, flop3, bet, flop1, bet, flop1, bet, show , prize.
[...]<script>
script['Config']['Number  of players'] = 3
script['Config']['Min bet'] = 10
script['Config']['Max bet'] = 1000
</script>
<code>
def config(self):
      self.n_players = 3
      self.min_bet = 10
      self.max_bet = 1000
</code>
<utter>
Great! We've set up the number  of players and the betting range. Now , what about the suits and card value
rank in the deck?
</utter><script>
script['Phase']['flopx'] = 'Discard one card fr om the deck. Flop x cards fr om the deck to the community .'
</script>
<code>
def flopx(self, x):
      self.deck.pop()
      for  i in range(x):
            self.community += [self.deck.pop()]
</code>
<utter>
OK. The phase 'flopx' has been added. The Phase is complete. Now , let's set up the flow for  the game with
these phases and the 'show' and 'prize' addtionally at the end.
</utter>
<script>
script['Flow'] = ['start', 'shuffle', 'blind', 'deal2', 'bet', 'flop3', 'bet', 'flop1', 'bet', 'flop1', 'bet', 'show', 'prize']
</script>
<code>
def set_flow(self):
      self.flow = ['start', 'shuffle', 'blind', 'deal2', 'bet', 'flop3', 'bet', 'flop1', 'bet', 'flop1', 'bet', 'show', 'prize']
</code>
<utter>
Excellent! The flow is set up. Now everything is r eady . You have successfully cr eated a new game!
</utter><script>
</script>
<code>
</code>
<utter>
Hello, I am your  game development assistant. Let's interactively develop a new poker  game. Alright, let's
start by configuring the essential parameters for  our game. Could you pr ovide the number  of players and
the betting range (minimum and maximum bet)?
</utter>
Figure 2: An example of the IGE process for a poker game.
about their game concept and feedback to the LLM. The
LLM’s output otincludes both interactions with the user
and code snippets to implement the user’s game concept in
one turn. To enable an LLM to function as an IGE, we in-
struct the LLM to perform the following processes in each
turn: (1)Pscript : configure the game script segment based
on the user’s input(Enclosed by <script></script> :
in Figure 2); (2)Pcode: generate the corresponding code
snippet based on the game script segment(Enclosed by
<code></code> : in Figure 2); (3)Putter : interact with
the user, including guidance and feedback(Enclosed by
<utter></utter> : in Figure 2). For interaction and
coding requirements, PcodeandPutter are essential. Pscript
serves as an intermediate process, akin to the reasoning in
chain-of-thought (CoT) (Wei et al. 2022). Additionally, it
can also act as a visual representation of the current devel-
opment progress. Compared to code, a script is much eas-
ier for people to understand, especially those without a pro-
gramming background. Therefore, otcan be specilized as:
ot= (st, ct, ut) =Fθ(st, ct, ut|ht−1, it;S). (4)where st,ct,utrefer to the outputs of Pscript ,Pcode, and
Putter respectively, and Srefers to the generic game script
template for a specific game. Furthermore, the ultimate ob-
jective of this task, CustomGame Ccan be obtained by
merging ctacross all turns:
C=m(c1, c2, . . . , c T) (5)
where mdenotes the merge function. Specifically, mcan be
determined by the specific game implementation. In our in-
plementaion, we embed ctinto the base code of the specific
game.
Data Generation for IGE
In this section, we discuss our attempt in data generation.
Utilizing LLMs to create IGE requires fine-tuning on a
substantial amount of supervised data. However, manually
crafting diverse interaction with script-code pairs is a chal-
lenging task. Compared to fully manual annotation, Har-
nessing LLMs to synthesize data is more efficient and has
Pool
def flopx(self, x):
      self.deck.pop()
      for  i in range(x):
            self.community +=[self.deck.pop()]Code Snippet
Phase:
    flopx: Discard one card fr om 
    the deck. Flop x cards fr om 
    the deck to the community .Script Segment
LLMGenerateSplitLLM
LLMGenerateInteraction SnippetGenerate
Filter
Sample...
...
def flopx(self, x):
      self.deck.pop()
      for  i in range(x):
            self.community += [self.deck.pop()]
    self.deck.pop()Code Snippet
Phase:
    flopx: Discard one card fr om the
    deck. Flop x cards fr om the deck
    to the community . After  each one
    flop, discard one mor e card.Script SegmentConfig:
    Number  of players: ...
    ...
Phase:
    start: ...
    ...
Flow: ...Config:
    Number  of players: ...
    ...
Phase:
    start: ...
    ...
Flow: ...
def config(self):
    self.n_players = 3
    ...
def start(self):
    ...
...
def set_flow(self):
    ...def config(self):
    self.n_players = 3
    ...
def start(self):
    ...
...
def set_flow(self):
    ...Config:
    Number  of players: ...
    ...
Phase:
    start: ...
    ...
Flow: ...
def config(self):
      self.n_players = 3
      ...
def start(self):
      ...
...
def set_flow(self):
      ...CodeScript
Seed Data
...
......
... ......
Complete InteractionFigure 3: Data synthesis pipeline for game script-code pair and interaction generation.
become a popular method for addressing the issue of insuf-
ficient data. We propose a pipeline consisting of three main
steps to generate synthetic data, starting with a small set of
manually annotated seed data, as illustrated in Figure 3. We
utilize GPT-4o as the generator.
Init pool First, we manually craft a few script-code pairs,
each corresponding to different custom games. These pairs
serve as seed data and are then split into script segments and
code snippets, which are added to the pool.
Generate new pairs In this step, we sample pairs of script
segments and code snippets, generating new pairs based on
these selections. We prompt the generator to modify the code
snippet first, then generate the corresponding script segment.
This order is chosen because it’s easier to map a script seg-
ment to a code snippet by describing the code, rather than
generating code from a script description. This approach
yields higher-quality data with more reliable mappings. To
ensure the generated code functions correctly, we implement
a filter before adding it to the pool. This process continues
until the pool contains a sufficient number of entries.
Generate interaction data Finally, we generate the inter-
action data depicted in Figure 2 using the script-code pairs.
This process involves two lines: (1) generating interaction
snippets based on pairs of script segments and code snippets
from the pool; (2) generating complete interactions from
complete script-code pairs. The necessity of these two data
components will be discussed in the next section.
A Three-stage Progressive Training Strategy
In this section, we present the training strategy of IGE.
Based on our IGE framework, the LLM will learn to perform
Pscript ,Pcode andPutter step by step, which places highdemands on the joint capability of interaction and program-
ming. However, it is challenging for the model to learn both
capabilities simultaneously. On the other hand, a straightfor-
ward strategy to train on sufficient complete interaction data
is inefficient. Therefore, we propose a three-stage progres-
sive training strategy to transfer the dialogue-based LLM to
our IGE smoothly.
Stage-1: Base Training This stage aims to train the base
interaction ability of the model. Interaction ability is the
most fundamental ability for IGE and serves as the foun-
dation for the following two stages. Since most LLMs have
already undergone sufficient and efficient supervised fine-
tuning (SFT) (Brown et al. 2020; Raffel et al. 2020; Ouyang
et al. 2022), we can directly use such models for Stage-1.
Stage-2: Core Training This stage aims to train the core
capabilities of the model, namely the joint capability of
programming and interaction. It fine-tunes the model from
Stage-1 on interaction snippets that follow the IGE format.
As illustrated in Figure 2, we instruct the model to perform
thePscript ,PcodeandPutter step by step to extract the user’s
concept of the game, implement it in code, and provide guid-
ance and feedback for interaction.
Stage-3: Alignment This stage aims to align the model
with a complete interaction context to fully develop a game
as an IGE. It fine-tunes the model from Stage-2, which al-
ready possesses significant programming and interaction ca-
pabilities. At this stage, we only need to extend its ability
for multi-turn interactions as an IGE, particularly in guid-
ing users to complete game development according to the
predefined script. Since the model already possesses strong
multi-turn interaction and long-context capabilities follow-
ing Stage-1 training, only a small dataset is required for
alignment at this stage.
Evaluation Metrics Scoring Guide
Metric Description Score Criteria
Guidance How the response guide the user step-by-step to complete the game. 1 Poor Significant deficiencies or inaccuracies.
Logic Logical structure and soundness of reasoning, including the support and validity of conclusions. 2 Below Avg. Noticeable weaknesses, lacking in several areas.
Relevance The extent to which the response stays on topic and within the scope of the assistant role. 3 Above Avg. Mostly on target with a few minor shortcomings.
Coherence Integration into the context, consistency with previous statements and conversational flow. 4 Strong Strong performance, often surpasses expectations.
Conciseness Brevity and clarity of the response, avoiding unnecessary elaboration or repetition.
Table 1: Evaluation Metrics and Scoring Guide. We design the criteria following Yu et al. (2024b); Wu et al. (2024c); Zheng
et al. (2024); Wang et al. (2023b); Guo et al. (2023).
Experiments
In this section, we construct an IGE for a poker game. We
employ the proposed data synthesis pipeline to generate the
corresponding dataset, fine-tune an IGE using the presented
strategy and evaluate its performance.
Dataset
Poker Game Poker, a worldwide card game, e.g. Texas
hold’em, Badugi . These poker games can be abstracted into
a generic game script. Table 2 presents an example example
of such a script for the classic Texas hold’em . This generic
script allows for the configuration of several common ele-
ments across different poker games, including the number
of players, minimum and maximum bet limits, suit types
and rankings, single-card rankings, multi-card combination
rankings, game phases, and overall game flow. By adjusting
these elements, virtually infinite variations of poker can be
created. Notably, each game in our dataset corresponds to
a unique configuration, including customizable phases. For
example, a standard “flopx” phase might involve discard-
ing one card from the deck and then revealing x community
cards. This phase can be customized by adding a rule such
as, “After each flop, discard one more card,” thereby creating
a new variant of the “flopx” phase.
Texas hold’em
Config :
Number of players : 3
Min bet : 10
Max bet : 1000
Suit: H, D, C, S
Suit have rank : False
Card value rank : 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1
Card combinations rank : High Card, Pair, Two Pair, Three of a Kind, Straight, Flush,
Full House, Four of a Kind, Straight Flush
Phase :
start : Config the game and prepare the deck and chips for all players.
blind : Randomly assign two players: small blind bets minimum, big blind bets double.
dealx : Deal x cards to each player.
switch : Ask each player to discard and draw the same number of cards from the deck.
bet: Ask each player to bet until all unfold players match the highest or only 1 remains.
flopx : Discard one card from the deck. Reveal x community cards.
Flow : start, blind, deal2, bet, flop3, bet, flop1, bet, flop1, bet, show, prize
Table 2: An example game script for a poker game.
Data Statistics We generate game script-code pairs and
interactions according to the pipeline shown in Figure3. The
interaction data format follows Figure 2. For the training set,
we created 20 poker games with manually crafted script-
code pairs as seed data, resulting in 3718 synthetic inter-
action snippets for Stage-2 and 36 complete interactions for
Stage-3. The test set comprises 10 manually crafted custompoker games, each with corresponding script-code pairs, en-
compassing a total of 90 functions.
Setup
We employ LLaMA3.1-8B-Instruct1(Dubey et al. 2024) for
Stage-1 and finetune it using LoRA (Hu et al. 2021) with
r= 8,α= 32 , and a learning rate of 3e-4. We train 3
epochs on the 3718 interaction snippets for Stage-2 and 5
epochs on the 36 complete interactions for Stage-3.
To assess the performance of the LLM in a dynamic multi-
turn interaction environment, we require a user to interact
with the LLM, as demonstrated in our IGE framework. Sim-
ulating the user using a rule-based approach is complex, and
employing human annotators poses challenges related to in-
consistent standards and high costs. To address these issues,
we use GPT-4o-mini as the interactor to simulate the user,
a practice increasingly adopted in dynamic multi-turn inter-
action environments (Li et al. 2023; Yu et al. 2024b). For
evaluation, we provide the interactor with a manually crafted
game script and instruct them to treat it as the game con-
cept they have in mind. The interactor then interacts with
the LLM, resulting in a multi-turn interaction about a spe-
cific custom game. This allows us to use the game script and
its corresponding code as the ground truth for evaluating the
generated interaction.
Metrics
We assess model performance from two perspectives: inter-
action quality and code correctness.
Interaction Quality The interaction quality is assessed
by an evaluator model, which assesses the output for guid-
ance, logic, relevance, coherence and conciseness. Follow-
ing KIEval (Yu et al. 2024b), we implement a scoring sys-
tem to quantitatively grade model performance in differ-
ent aspects. Responses are rated on a definitive scale from
1 to 4 for each aspect, where 1 and 4 denote ‘Poor’ and
‘Strong’ performance, respectively, as detailed in Table1.
These scores are designed to encourage decisive evaluations.
To facilitate comparison, we normalize the scores, ensuring
that a rating of 1.0 indicates perfect performance. We utilize
GPT-4o as the evaluator.
Code correctness We evaluate code correctness using two
functional-level metrics and two overall-level metrics:
•Functional Execution Success Rate(F-ESR) The gener-
ated code consists of functions as code snippets. We use
1https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-
Instruct
Ability ModelInteraction Quality Code Correctness
Gui. Log. Rel. Coh. Con. Overall F-ESR F-Acc ESR Acc
5-shotGPT-3.5-turbo 94.5 96.5 100 99.0 96.5 98.0 95.8 87.9 60.0 30.0
GPT-4o 98.5 98.0 100 100 99.0 99.0 93.0 88.5 50.0 30.0
Llama-3.1-8B-Instruct 97.5 98.0 100 99.5 99.0 98.5 100 90.0 60.0 10.0
Fine-tuningIGE 98.5 99.0 100 99.5 99.0 100 100 99.0 100 90.0
w/o.Pscript 98.0 97.0 100 99.0 96.5 98.5 100 98.8 100 80.0
w/o. synthesis 96.5 96.0 100 98.0 96.5 98.0 97.4 86.8 70.0 0
w/o. Stage-2 96.5 97.5 100 99.0 96.0 98.5 98.2 89.2 80.0 10.0
w. Mixed-stage 92.5 96.5 99.5 96.0 92.0 95.0 95.0 88.5 80.0 20.0
Table 3: Main results of different models and the ablation study of IGE. The number of functions generated by the model can
vary due to factors such as repeated modifications or missed queries. Functional-level metrics primarily assess the correctness
of the generated code without accounting for recall rate, which is instead reflected in the overall-level metrics.
Ability Modelconfig*start blind*dealx*flopx*switch bet flow*Overall
F-ESR F-Acc F-ESR F-Acc F-ESR F-Acc F-ESR F-Acc F-ESR F-Acc F-ESR F-Acc F-ESR F-Acc F-ESR F-Acc F-ESR F-Acc
5-shotGPT-3.5-turbo 100 88.9 100 100 87.5 87.5 88.9 55.6 87.5 75.0 100 100 100 100 88.9 66.7 95.8 87.9
GPT-4o 100 100 100 100 70.0 70.0 90.0 90.0 100 60.0 100 100 80.0 80.0 100 100 93.0 88.5
Llama-3.1-8B-Instruct 100 100 100 100 100 100 100 50.0 100 60.0 100 100 100 100 100 66.7 100 90.0
Fine-tuningIGE 100 100 100 100 100 100 100 90.0 100 100 100 100 100 100 100 100 100 99.0
w/o.Pscript 100 100 100 100 100 100 100 90.0 100 90.0 100 100 100 100 100 100 100 98.8
w/o. synthesis 100 100 100 100 100 100 88.9 22.2 100 55.6 100 100 100 100 100 100 93.8 86.8
w/o. Stage-2 100 100 100 100 100 100 90.0 20.0 90.0 60.0 100 100 100 100 100 100 98.2 89.2
w. Mixed-stage 100 100 100 100 81.8 81.8 100 33.3 90.0 90.0 90.0 90.0 81.8 81.8 100 75.0 95.0 88.5
Table 4: Function-level code correctness of different models and the ablation study of IGE. Functions with an asterisk (*) are
variable functions in the test set, while the remaining functions are fixed.
F-ESR to represent the execution success rate of all these
functions across the entire test set to measure the model’s
basic coding capability.
•Functional Accuracy(F-Acc) : Furthermore, we use F-
Acc to represent the functional accuracy of the code, as-
sessed through black-box testing to determine if the gen-
erated code is correct. Specifically, we replace player in-
put with random input and, for each run, fix the random
seed. We then compare the resulting state of the gener-
ated code with the ground truth. We conduct 40 runs, each
with a different random seed, for every entry. If all runs
produce identical states, the code is considered correct.
•Execution Success Rate(ESR) We use ESR to represent
the success rate of executing the complete code of a cus-
tom game. This complete code comprises all the snippets
generated throughout the entire multi-turn interaction.
•Accuracy(Acc) We use Acc to represent the accuracy of
the complete code for a custom game. The method for
judging correctness is the same as that used for F-Acc.
Main Results
We evaluate IGE on 10 manually crafted custom poker
games, each accompanied by its complete game script-code
pair. Table 3 presents the performance of our IGE, including
both interaction quality and code correctness. For compari-
son, we take several representative closed-source and open-
source LLMs in a 5-shot setting as baselines. Intuitively, IGEexcels in both interaction quality and code correctness.
Interaction Quality All models exhibit high interaction
quality.Our IGE excels across all dimensions, showcas-
ing exceptional capabilities in interacting with the user
throughout the interactive development process. Compared
to Llama-3.1-8B-Instruct, our fine-tuned model excels in
guidance and logic, effectively guiding the user to develop
the game logically.
Code Correctness In our results, all models significantly
outperform in functional-level metrics compared to overall-
level metrics. This suggests that while LLMs excel at pro-
ducing functional code, they face challenges when generat-
ing long, complete code. Additionally, it is evident that ex-
ecutability is more easily achieved than accuracy across all
models, with our model reaching a perfect ESR of 100. This
indicates that LLMs excel at generating code that is syn-
tactically executable. Notably, IGE outperforms in all met-
rics. It achieves an impressive F-Acc of 99.0, outperforming
the second-best model by 9 points. Moreover, it reaches an
ESR of 100, surpassing the second-best by 20 points. Fur-
thermore, it attains an Acc of an astounding 90, outstripping
the second-best by 60 points.
To conduct a more in-depth analysis, we compute the
function-level code correctness in Table 4. We find that most
models excel on fixed functions and two simple variable
functions: config and flow. These two functions require only
basic assignment statements to configure the game, allowing
Case 1 blind: Randomly choose two players as small blind and big blind respectively. Place 1/2 minimum bet for small blind and double minimum bet for big blind.
GPT-4o: IGE:
def blind(self):
def bet(player id, amount):
self.players[player id].bet += amount
self.players[player id].remain -= amount
small blind, big blind = random.choice(self.player ids, 2)
bet(small blind, self.min bet // 2)
bet(big blind, self.min bet * 2)def blind(self):
def bet(player id, amount):
self.players[player id].bet += amount
self.players[player id].remain -= amount
small blind, big blind = random choice(self.player ids, 2)
bet(small blind, self.min bet // 2)
bet(big blind, self.min bet * 2)
Case 2 dealx: Deal x cards to each player and discard 1 cards from the deck afterward.
GPT-4o: IGE:
def dealx(self, x):
for in range(x):
for player id in self.players:
self.players[player id] [’hole’].append(self.deck.pop())
self.deck.pop()def dealx(self, x):
for i in range(x):
for p in self.players:
self.players[p].hole += [self.deck.pop()]
self.deck.pop()
Table 5: Case study of the results of GPT-4o and IGE. Only the code part is retained.
them to generalize effectively. However, for functions with
more complex code logic, namely blind, dealx, and flopx,
the baselines generally underperform, with the lowest F-Acc
reaching just 20. These results indicate that the accumulation
of errors across these functions leads non-fine-tuned models
to exhibit low correctness in overall-level evaluation. It is
important to note that the model is required to be all-round
at each function; otherwise, the overall performance will de-
generate in a way of Buckets effect (Wu et al. 2024a). De-
lightfully, our IGE achieves near-perfect performance across
all functions, resulting in an Acc far exceeding the baselines.
Ablation Study
We ablate different variants from the full IGE architecture,
the results are presented in Table 3 and Table 4.
Ablation on Pscript A slight decrease can be observed
in interaction quality across nearly all dimensions without
Pscript . Additionally, F-Acc drops by 0.2 points and Acc by
10.0 points. As shown in Table 4, the only failure occurs on a
flopx function when compared to the complete IGE architec-
ture. This suggests that Pscript can enhance both interaction
and coding abilities in certain cases.
Ablation on synthetic data In this setting, we directly
employ manually crafted script-code pairs, splitting them
into snippets to generate complete interactions and interac-
tion snippets. A slight decline can be observed in interaction
quality across most dimensions, alongside a significant de-
crease in code correctness, with Acc dropping to 0. Notably,
the code correctness is even lower than that of the 5-shot
Llama-3.1-8B-Instruct. As shown in Table 4, this decline is
attributed to poor performance on the two most challeng-
ing functions, dealx and flopx. This can be explained by the
model overfitting on the limited data due to the absence of
synthetic data, which leads to poor generalization.
Ablation on training strategy We conducted comprehen-
sive ablation experiments on our three-stage training strat-
egy, with the following setups: w/o. Stage-1, w/o. Stage-2, w/o. Stage-3, and w. Mixed-stage. In the setups without
Stage-1 and Stage-3, the model loses its guiding and inter-
action abilities in multi-turn scenarios as an IGE, resulting
in ESR and Acc values of 0. Therefore, the results of these
two settings are not reported. This suggests that both Stage-1
and Stage-3 play a crucial role in enhancing the model’s in-
teraction ability as an IGE. As shown in Table 3, the interac-
tion quality of the model decreases across most dimensions
without Stage-2. Additionally, there is a significant drop in
code correctness, with Acc falling to 10.0. A sharp decline
in F-Acc for the dealx and flopx functions is clearly evi-
dent in Table 4. This indicates that Stage-2 is essential to the
core interaction and programming capabilities of the model,
especially programming capabilities. The Mixed-stage in-
volves mixing all the complete interactions and interaction
snippets and fine-tuning on them in a single stage. It mixes
Stage-2 and Stage-3. A significant decrease can be observed
in interaction quality in the Mixed-stage setting. Addition-
ally, there is a notable drop in code correctness, with Acc
falling to 20.0. This indicates that a mixed-stage training
strategy for complete interactions and interaction snippets
hinders both the interaction and programmig capabilities of
the model. This suggests that our three-stage training strat-
egy effectively enhances the joint capability of interaction
and programmig of the LLM as an IGE.
Case Study
In Table 5, we present two representative cases comparing
GPT-4o and IGE. In Case 1, the code generated by GPT-
4o is logically correct, but the function call is used incor-
rectly. The proper usage of “random.choice” should be “ran-
dom.choice(x)”, but it seems to have confused this with
the “random choice” usage provided in the in-context ex-
amples. Similarly, in Case 2, GPT-4o mistakenly treated
“self.players[player id]” as a dict. This can be attributed to
its misalignment with the engine, also known as hallucina-
tion (Ji et al. 2023). In comparison, our IGE is well-aligned
and does not exhibit this phenomenon in the test set.
Conclusion
This paper introduces the Interaction-driven Game Engine
(IGE) and proposes a paradigm for training IGE to allows
users to develop custom games interactively using natural
language. To enable an LLM to function as an IGE, we in-
struct it to generate script segments, code snippets and in-
teractions for each turn in the development process. To fa-
cilitate the training process, a data synthesis pipeline is pro-
posed to generate sufficient training data, as well as a three-
stage progressive training strategy to enhance the joint capa-
bility of interaction and programming of the LLM. Embod-
ied in a poker game, we demonstrate the performance of the
IGE through a comprehensive evaluation.
References
Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.;
Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.;
Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv
preprint arXiv:2303.08774 .
Bengio, Y .; Louradour, J.; Collobert, R.; and Weston, J.
2009. Curriculum learning. In Proceedings of the 26th an-
nual international conference on machine learning , 41–48.
Borisov, V .; Seßler, K.; Leemann, T.; Pawelczyk, M.; and
Kasneci, G. 2022. Language models are realistic tabular data
generators. arXiv preprint arXiv:2210.06280 .
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners. Ad-
vances in neural information processing systems , 33: 1877–
1901.
Chintagunta, B.; Katariya, N.; Amatriain, X.; and Kannan,
A. 2021. Medically aware GPT-3 as a data generator for
medical dialogue summarization. In Machine Learning for
Healthcare Conference , 354–372. PMLR.
Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.;
Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.;
et al. 2024. The Llama 3 Herd of Models. arXiv preprint
arXiv:2407.21783 .
Eladhari, M. P. 2018. Re-Tellings: The Fourth Layer of Nar-
rative as an Instrument for Critique. In Rouse, R.; Koenitz,
H.; and Haahr, M., eds., Interactive Storytelling - 11th In-
ternational Conference on Interactive Digital Storytelling,
ICIDS 2018, Dublin, Ireland, December 5-8, 2018, Proceed-
ings, volume 11318 of Lecture Notes in Computer Science ,
65–78. Springer.
Fan, L.; Wang, G.; Jiang, Y .; Mandlekar, A.; Yang, Y .; Zhu,
H.; Tang, A.; Huang, D.; Zhu, Y .; and Anandkumar, A. 2022.
MineDojo: Building Open-Ended Embodied Agents with
Internet-Scale Knowledge. In Koyejo, S.; Mohamed, S.;
Agarwal, A.; Belgrave, D.; Cho, K.; and Oh, A., eds., Ad-
vances in Neural Information Processing Systems 35: An-
nual Conference on Neural Information Processing Systems
2022, NeurIPS 2022, New Orleans, LA, USA, November 28
- December 9, 2022 .
Gallotta, R.; Todd, G.; Zammit, M.; Earle, S.; Liapis, A.;
Togelius, J.; and Yannakakis, G. N. 2024. Large LanguageModels and Games: A Survey and Roadmap. arXiv preprint
arXiv:2402.18659 .
Guo, Z.; Jin, R.; Liu, C.; Huang, Y .; Shi, D.; Yu, L.; Liu, Y .;
Li, J.; Xiong, B.; Xiong, D.; et al. 2023. Evaluating large
language models: A comprehensive survey. arXiv preprint
arXiv:2310.19736 .
Hu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,
S.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation
of large language models. arXiv preprint arXiv:2106.09685 .
Ji, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii, E.;
Bang, Y . J.; Madotto, A.; and Fung, P. 2023. Survey of hal-
lucination in natural language generation. ACM Computing
Surveys , 55(12): 1–38.
Jin, H.; Han, X.; Yang, J.; Jiang, Z.; Chang, C.-Y .; and
Hu, X. 2023. Growlength: Accelerating llms pretraining
by progressively growing training length. arXiv preprint
arXiv:2310.00576 .
K¨uttler, H.; Nardelli, N.; Miller, A. H.; Raileanu, R.; Sel-
vatici, M.; Grefenstette, E.; and Rockt ¨aschel, T. 2020. The
NetHack Learning Environment. In Larochelle, H.; Ran-
zato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Ad-
vances in Neural Information Processing Systems 33: An-
nual Conference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual .
Li, M.; Zhao, Y .; Yu, B.; Song, F.; Li, H.; Yu, H.; Li,
Z.; Huang, F.; and Li, Y . 2023. Api-bank: A comprehen-
sive benchmark for tool-augmented llms. arXiv preprint
arXiv:2304.08244 .
Liu, H.; Yan, W.; Zaharia, M.; and Abbeel, P. 2024. World
model on million-length video and language with blockwise
ringattention. arXiv preprint arXiv:2402.08268 .
Lowe, R.; Gupta, A.; Foerster, J. N.; Kiela, D.; and Pineau,
J. 2020. On the interaction between supervision and self-
play in emergent communication. In 8th International Con-
ference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net.
Mnih, V .; Kavukcuoglu, K.; Silver, D.; Graves, A.;
Antonoglou, I.; Wierstra, D.; and Riedmiller, M. A. 2013.
Playing Atari with Deep Reinforcement Learning. CoRR ,
abs/1312.5602.
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;
Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;
et al. 2022. Training language models to follow instructions
with human feedback. Advances in neural information pro-
cessing systems , 35: 27730–27744.
Peng, B.; Li, C.; He, P.; Galley, M.; and Gao, J.
2023. Instruction tuning with gpt-4. arXiv preprint
arXiv:2304.03277 .
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-
ing the limits of transfer learning with a unified text-to-text
transformer. Journal of machine learning research , 21(140):
1–67.
Ranella, N.; and Eger, M. 2023. Towards Automated Video
Game Commentary Using Generative AI. In Madkour,
A.; Otto, J.; Ferreira, L. N.; and Johnson-Bey, S., eds.,
Proceedings of the Experimental Artificial Intelligence in
Games Workshop co-located with the 19th AAAI Confer-
ence on Artificial Intelligence and Interactive Digital En-
tertainment (AIIDE 2023), Salt Lake City, Utah, USA, Octo-
ber 8, 2023 , volume 3626 of CEUR Workshop Proceedings .
CEUR-WS.org.
Schick, T.; and Sch ¨utze, H. 2021. Generating datasets
with pretrained language models. arXiv preprint
arXiv:2104.07540 .
Shanahan, M.; McDonell, K.; and Reynolds, L. 2023. Role
play with large language models. Nat., 623(7987): 493–498.
Shao, Y .; Li, L.; Dai, J.; and Qiu, X. 2023a. Character-
llm: A trainable agent for role-playing. arXiv preprint
arXiv:2310.10158 .
Shao, Z.; Gong, Y .; Shen, Y .; Huang, M.; Duan, N.; and
Chen, W. 2023b. Synthetic prompting: Generating chain-of-
thought demonstrations for large language models. In Inter-
national Conference on Machine Learning , 30706–30775.
PMLR.
Sun, Z.; Shen, Y .; Zhou, Q.; Zhang, H.; Chen, Z.; Cox, D.;
Yang, Y .; and Gan, C. 2024. Principle-driven self-alignment
of language models from scratch with minimal human su-
pervision. Advances in Neural Information Processing Sys-
tems, 36.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
Babaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,
S.; et al. 2023. Llama 2: Open foundation and fine-tuned
chat models. arXiv preprint arXiv:2307.09288 .
Uludagli, M. C ¸ .; and Oguz, K. 2023. Non-player charac-
ter decision-making in computer games. Artif. Intell. Rev. ,
56(12): 14159–14191.
Vakil, N.; and Amiri, H. 2023. Complexity-guided
curriculum learning for text graphs. arXiv preprint
arXiv:2311.13472 .
Valencia-Garc ´ıa, R.; Lagos-Ortiz, K.; Alcaraz-M ´armol, G.;
del Cioppo, J.; and Vera-Lucio, N. 2016. Technologies and
Innovation: Second International Conference, CITI 2016,
Guayaquil, Ecuador, November 23-25, 2016. Proceed-
ings. Communications in Computer and Information Sci-
ence, 658.
Vinyals, O.; Babuschkin, I.; Czarnecki, W. M.; Mathieu, M.;
Dudzik, A.; Chung, J.; Choi, D. H.; Powell, R.; Ewalds, T.;
Georgiev, P.; Oh, J.; Horgan, D.; Kroiss, M.; Danihelka, I.;
Huang, A.; Sifre, L.; Cai, T.; Agapiou, J. P.; Jaderberg, M.;
Vezhnevets, A. S.; Leblond, R.; Pohlen, T.; Dalibard, V .;
Budden, D.; Sulsky, Y .; Molloy, J.; Paine, T. L.; G ¨ulc ¸ehre,
C ¸ .; Wang, Z.; Pfaff, T.; Wu, Y .; Ring, R.; Yogatama, D.;
W¨unsch, D.; McKinney, K.; Smith, O.; Schaul, T.; Lillicrap,
T. P.; Kavukcuoglu, K.; Hassabis, D.; Apps, C.; and Silver,
D. 2019. Grandmaster level in StarCraft II using multi-agent
reinforcement learning. Nat., 575(7782): 350–354.
Wang, G.; Xie, Y .; Jiang, Y .; Mandlekar, A.; Xiao, C.; Zhu,
Y .; Fan, L.; and Anandkumar, A. 2023a. V oyager: An
Open-Ended Embodied Agent with Large Language Mod-
els.CoRR , abs/2305.16291.Wang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;
Khashabi, D.; and Hajishirzi, H. 2022. Self-instruct: Align-
ing language models with self-generated instructions. arXiv
preprint arXiv:2212.10560 .
Wang, Y .; Yu, Z.; Zeng, Z.; Yang, L.; Wang, C.; Chen, H.;
Jiang, C.; Xie, R.; Wang, J.; Xie, X.; et al. 2023b. Pandalm:
An automatic evaluation benchmark for llm instruction tun-
ing optimization. arXiv preprint arXiv:2306.05087 .
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;
Chi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-
thought prompting elicits reasoning in large language mod-
els.Advances in neural information processing systems , 35:
24824–24837.
Wu, H.; Liu, L.; Zhao, H.; and Zhang, M. 2023. Em-
power Nested Boolean Logic via Self-Supervised Curricu-
lum Learning. arXiv preprint arXiv:2310.05450 .
Wu, H.; Wang, Y .; Liu, X.; Zhao, H.; and Zhang, M. 2024a.
Instruction-Driven Game Engines on Large Language Mod-
els.arXiv preprint arXiv:2404.00276 .
Wu, H.; Xu, Z.; Xu, T.; Hong, J.; Wu, W.; Zhao, H.; Zhang,
M.; and He, Z. 2024b. Scaling Virtual World with Delta-
Engine. arXiv:2408.05842.
Wu, W.; Wu, H.; Jiang, L.; Liu, X.; Hong, J.; Zhao, H.; and
Zhang, M. 2024c. From Role-Play to Drama-Interaction: An
LLM Solution. arXiv preprint arXiv:2405.14231 .
Xu, Y .; Wang, S.; Li, P.; Luo, F.; Wang, X.; Liu, W.; and Liu,
Y . 2023. Exploring Large Language Models for Communi-
cation Games: An Empirical Study on Werewolf. CoRR ,
abs/2309.04658.
Yu, Y .; Zhuang, Y .; Zhang, J.; Meng, Y .; Ratner, A. J.; Kr-
ishna, R.; Shen, J.; and Zhang, C. 2024a. Large language
model as attributed training data generator: A tale of diver-
sity and bias. Advances in Neural Information Processing
Systems , 36.
Yu, Z.; Gao, C.; Yao, W.; Wang, Y .; Ye, W.; Wang, J.; Xie,
X.; Zhang, Y .; and Zhang, S. 2024b. Kieval: A knowledge-
grounded interactive evaluation framework for large lan-
guage models. arXiv preprint arXiv:2402.15043 .
Zheng, L.; Chiang, W.-L.; Sheng, Y .; Zhuang, S.; Wu, Z.;
Zhuang, Y .; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2024.
Judging llm-as-a-judge with mt-bench and chatbot arena.
Advances in Neural Information Processing Systems , 36.
Prompts Demonstration
In this section, we provide the prompts used for in the paper.
Each{...}component will be substituted with correspond-
ing information. For more details, please refer to our code.
These prompts are not designed for any specific game, they
can be used to build an IGE for any {Game name}.
Table 6-8 present the prompts used in the Data Genera-
tion. Table 9-10 present the system prompts for models. Ta-
ble 11-12 present the prompts used in the Evaluation.
Prompt for New Script-code Pairs Generation
Generate new code snippet and the corresponding script segment of a {Game_name} game
based on the given original code snippet and the corresponding original script
segment.
1. Modify the code logic to obtain a new code segment and output the corresponding
script segment.
2. The new code snippet is obtained by modifying the original code snippet.
3. Keep the input parameters unchanged, do not introduce new input parameters.
4. The generated new code snippet should not introduce new instance attributes and
involved methods such as ‘self.xxx‘ or ‘self.xxx(...)‘ compared to the original code
snippet. The generated new code snippet can only include instance attributes and
instance methods involved in the original code snippet. You cannot create new ones.
For example, there is a original code snippet below:
def bet_done(self, wait_to_bet):
all_bet = [self.players[p].bet for p in self.get_unfold_players()]
if not wait_to_bet and all([b==all_bet[0] for b in all_bet]):
return True
return False
In this code snippet, the instance attributes and instance methods involved are only
‘self.players‘ and ‘self.get_unfold_players()‘. Therefore, in the new code snippet
generated from this original code snippet, the instance attributes and instance
methods involved should also only be ‘self.players‘ and ‘self.get_unfold_players()‘,
other created ones such as ‘self.group‘, ‘self.discard_pile‘, ‘self.burn_pile‘, ‘
self.burn_card‘ are not allowed to be used.
5. Do not use ‘print‘ or logging information.
6. The script segment can be seen as a description of the code snippet.
7. Try to be creative and diverse.
8. The output format should follow the original, without any redundant information.
# Examples
{In-context Examples}
# Start of Official Requests
## original code snippet:
{original_code}
## original script segment:
{original_scrip}
Table 6: Prompt for generating new pairs in Data Generation for IGE.
Prompt for Interaction Snippets Generation
Generate a dialogue between a user and an assistant based on the following rules and
given script segment and code snippet.
1. The user edits game script segments using natural language during interactions
with the assistant.
2. The assistant interacts with the user to achieve interactive game development.
The assistant guides the user in editing game script segments, generates
corresponding code snippets, and interacts with the user through dialogue.
3. Each turn of the assistant’s output should include three processes: "script", "
code", and "utter", corresponding to three blocks: <script></script>, <code></code
>, <utter></utter>. Formally, these three blocks must exist, even if the content is
empty.
4. The ’script’ process: The assistant generates the game script segment based on
the user’s input of the current turn. Return modifications to the script as changes,
rather than returning the entire script. The script is a Python dict, so you can
use simple Python code to represent modifications to it, such as: script[’xxx’] = ’
xxx’. The ’script’ process should be enclosed using ’<script>’ tag.
5. The ’code’ process: The assistant generates the corresponding Python code snippet
based on the game script segment from the ’script’ process. The complete code is a
CustomGame class that inherits from GameBase class, but only the methods related to
the given script segment need to be generated. The ’code’ process should be enclosed
using ’<code>’ tag.
6. The ’utter’ process: The assistant interacts with the user, including responding
to the user’s input of the current turn, summarizing the results of the current turn
, and guiding the user to continue with the next turn of interaction. The ’utter’
process should be enclosed using ’<utter>’ tag.
7. The script segment and code snippet have already been provided. In the assistant’
s ’script’ and ’code’ process, use the given script segment and code snippet; do not
write your own.
8. The assistant does not know about the existence of the script segment in the
dialogue and needs to obtain it from the user’s input.
9. The given script segment and code snippet are essentially an outline of the plot
development. The assistant’s ’script’ and ’code’ process must be entirely derived
from or inferred from the user’s input. The user’s input should be more natural
language-based and not a direct copy of the given script segement.
10. The dialogue must cover and only cover the given script segment, and no other
content should appear.
{Formatting Instruction}
# Examples
{In-context Examples}
# Start of Official Requests
## script segment:
{script segment}
## code snippet:
{code snippet}
## dialogue:
Table 7: Prompt for generating interaction snippets in Data Generation for IGE.
Prompt for Complete Interactions Generation
Generate a dialogue between a user and an assistant based on the following rules and
given script segment and code snippet.
1. The user edits game script segments using natural language during interactions
with the assistant.
2. The assistant interacts with the user to achieve interactive game development.
The assistant guides the user in editing game script segments, generates
corresponding code snippets, and interacts with the user through dialogue.
3. Each turn of the assistant’s output should include three processes: "script", "
code", and "utter", corresponding to three blocks: <script></script>, <code></code
>, <utter></utter>. Formally, these three blocks must exist, even if the content is
empty.
4. The ’script’ process: The assistant generates the game script segment based on
the user’s input of the last turn. Return modifications to the script as changes,
rather than returning the entire script. The script is a Python dict, so you can use
simple Python code to represent modifications to it, such as: script[’xxx’] = ’xxx
’. The ’script’ process should be enclosed using ’<script>’ tag.
5. The ’code’ process: The assistant generates the corresponding Python code snippet
based on the game script segment from the ’script’ process. The complete code is a
CustomGame class that inherits from GameBase class, but only the methods related to
the given script segment need to be generated. The ’code’ process should be enclosed
using ’<code>’ tag.
6. The ’utter’ process: The assistant interacts with the user, including responding
to the user’s input of the last turn, summarizing the results of the current turn,
and guiding the user to continue with the current turn of interaction. The ’utter’
process should be enclosed using ’<utter>’ tag.
7. The script segment and code snippet have already been provided. You need to
randomly distribute them across multiple turns and generate an interactive dialogue
between the assistant and the user. This means the assistant guides the user step by
step to complete this game script segment. In a single turn of dialogue, the user’s
input should not contain too much information. If a large input is required, it
should be divided into multiple turns.
8. In the assistant’s ’script’ and ’code’ process, use the given script segment and
code snippet; do not write your own.
9. The dialogue must cover and only cover all the given script segment, and no other
content should appear.
10. The assistant does not know about the existence of the script segment in the
dialogue and needs to obtain it from the user’s input.
11. The given script segment and code snippet are essentially an outline of the plot
development. The assistant’s ’script’ and ’code’ process must be entirely derived
from or inferred from the user’s input. The user’s input should be more natural
language-based and not a direct copy of the given script segement.
12. In the first turn, the ’script’ and ’code’ process of the assistant should be
empty because the user has not yet input a game script segment. In the first turn,
the assistant should greet the user and start guiding them. In the end, after the
user has completed the entire script under the assistant’s guidance, the assistant
should convey to the user that the game development is complete.
13. The assistant should guide the user step by step along a specific line to
complete each part of the game script:
{Game_script_line}
{Formatting Instruction}
# Examples
{In-context Examples}
# Start of Official Requests
## script segment:
{script segment}
## code snippet:
{code snippet}
## dialogue:
Table 8: Prompt for generating complete interactions in Data Generation for IGE.
System Prompt for Baselines in a 5-shot Setting
You are a helpful assistant assigned to interact with the user for the interactive
development of a {Game_name} game.
1. The user edits game script segments using natural language.
2. The assistant guides the user in editing game script segments, generates
corresponding code snippets, and interacts with the user through dialogue.
3. Each turn of the assistant’s output should include three processes: "script", "
code", and "utter", corresponding to three blocks: <script></script>, <code></code
>, <utter></utter>. Formally, these three blocks must exist, even if the content is
empty.
4. The ’script’ process: The assistant generates the game script segment based on
the user’s input of the current turn. Return modifications to the script as changes,
rather than returning the entire script. The script is a existing Python dict, so
you can use simple Python code to represent modifications to it, such as: script[’
xxx’] = ’xxx’. The ’script’ process should be enclosed using ’<script>’ tag.
5. The ’code’ process: The assistant generates the corresponding Python code snippet
based on the game script segment from the ’script’ process. The complete code is a
CustomGame class that inherits from GameBase class, but only the methods related to
the given script segment need to be generated. The ’code’ process should be enclosed
using ’<code>’ tag.
6. The ’utter’ process: The assistant interacts with the user, including responding
to the user’s input of the current turn, summarizing the results of the current turn
, and guiding the user to continue with the next turn of interaction. The ’utter’
process should be enclosed using ’<utter>’ tag.
7. The assistant’s ’script’ and ’code’ process must be entirely derived from or
inferred from the user’s input. If the user’s input lacks the required information,
ask the user for further details, and both the ’script’ process and the ’code’
process of the assistant should be empty.
8. If the user’s input is unrelated to the script or insufficient to cause changes
in the script, the ’script’ process and the ’code’ process of the assistant should
both be empty.
9. If the user has any questions, answer them instead of randomly modifying the
script and code on your own.
10. In the first turn, the ’script’ and ’code’ process of the assistant should be
empty because the user has not yet input a game script segment. In the first turn,
the assistant should greet the user and start guiding them. In the end, after the
user has completed the entire script under the assistant’s guidance, the assistant
should convey to the user that the game development is complete.
11. The assistant should guide the user step by step along a specific line to
complete each part of the game script, referring to the given script template.
12. Output format:
<script>
...
</script>
<code>
...
</code>
<utter>
...
</utter>
# Examples
{In-context Examples}
Table 9: System prompt for baselines in a 5-shot Setting.
System Prompt for IGE
You are a helpful assistant assigned to interact with the user for the interactive
development of a {Game_name} game.
1. The user edits game script segments using natural language.
2. The assistant guides the user in editing game script segments, generates
corresponding code snippets, and interacts with the user through dialogue.
3. Each turn of the assistant’s output should include three processes: "script", "
code", and "utter", corresponding to three blocks: <script></script>, <code></code
>, <utter></utter>. Formally, these three blocks must exist, even if the content is
empty.
4. The ’script’ process: The assistant generates the game script segment based on
the user’s input of the current turn. Return modifications to the script as changes,
rather than returning the entire script. The script is a existing Python dict, so
you can use simple Python code to represent modifications to it, such as: script[’
xxx’] = ’xxx’. The ’script’ process should be enclosed using ’<script>’ tag.
5. The ’code’ process: The assistant generates the corresponding Python code snippet
based on the game script segment from the ’script’ process. The complete code is a
CustomGame class that inherits from GameBase class, but only the methods related to
the given script segment need to be generated. The ’code’ process should be enclosed
using ’<code>’ tag.
6. The ’utter’ process: The assistant interacts with the user, including responding
to the user’s input of the current turn, summarizing the results of the current turn
, and guiding the user to continue with the next turn of interaction. The ’utter’
process should be enclosed using ’<utter>’ tag.
7. The assistant’s ’script’ and ’code’ process must be entirely derived from or
inferred from the user’s input. If the user’s input lacks the required information,
ask the user for further details, and both the ’script’ process and the ’code’
process of the assistant should be empty.
8. If the user’s input is unrelated to the script or insufficient to cause changes
in the script, the ’script’ process and the ’code’ process of the assistant should
both be empty.
9. If the user has any questions, answer them instead of randomly modifying the
script and code on your own.
Table 10: System prompt for IGE.
System Prompt for Interactor
You are a user (as in the Example) of an interactive {Game_name} game development
application of a {Game_name} game, interacting with me (the assistant).
1. You should attempt to use natural language to edit game script segments.
2. You should focus on the "utter" part enclosed by the <utter></utter> tag in my
output and interact with it according to its guidance.
3. Your response does not need to include any tags.
4. A game script will be given. Assume this is the game script you have in mind. You
need to interactively present your ideas under the guidance of the me step by step,
i.e., respond based on the relevant parts of the given script. Try not to output
too much in one turn.
5. Try to use natural language instead of directly copying the given script segments
.
6. Your responses should be as concise as possible and should not include the
thought process.
# Examples
{In-context Examples}
# Start of Official Requests
## given game script:
{game script}
Table 11: System prompt for interactor in evaluation.
System Prompt for Evaluator
You are an objective evaluator in an interview. Your task is to evaluate a assistant
’s performance during a series of interactions with an user. The conversation
alternates between the user (marked with ’user:’) and the assistant (marked with ’
assistant’). Evaluate the assistant’s performance in the interactions as well as in
context, based on the following aspects independently, rating each on a scale from 1
(Poor) to 4 (Good):
Guidance: How the response guide the user step-by-step to complete the game.
Logic: Logical structure and soundness of reasoning, including the support and
validity of conclusions. Whether conclusions are well-supported and arguments are
free from logical fallacies.
Relevance: How the response relates to the topic. Ensure responses are within the
scope of the "assistant" role, avoiding unpermitted role shifts.
Coherence: How well the response integrates into the context. Consistency with
previous statements and overall conversational flow.
Conciseness: Brevity and clarity of the response. Clear, to-the-point communication,
free from extraneous elaboration or repetitive words.
Scoring Guide:
1 (Poor): Significant deficiencies or inaccuracies in the aspect.
2 (Below Average): Noticeable weaknesses, partially on target but lacking in several
areas.
3 (Above Average): Solid and competent, mostly on target with only a few minor
shortcomings.
4 (Good): Strong performance, fully meets and often surpasses expectations.
Evaluation Rules:
1. Evaluate the assistant consistently and objectively without bias, strictly
adhering to scoring guide.
2. Score from 1 to 4 for each aspect independently, using only integers. Low score
in one aspect should not influence another aspect. Write a brief comment before
scoring in the JSON output structure.
3. Write a overall comment and then give an overall score (same scoring guide). The
overall comment should be brief and clear. Consider the performance throughout the
interaction, not just in the latest round.
4. Format of Evaluation: Output in JSON format strictly following the template,
without any other words:
{guidance": {"comment": "", "score": 0}, "logic": {"comment": "", "score": 0}, "
relevance": {"comment": "", "score": 0}, "coherence": {"comment": "", "score": 0}, "
conciseness": {"comment": "", "score": 0}, "overall": {"comment": "", "score": 0}}
# interactions:
{dialogue}
# Evaluation:
Table 12: System prompt for evaluator in evaluation.
