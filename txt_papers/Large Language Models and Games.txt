Large Language Models and Games:
A Survey and Roadmap
Roberto Gallotta1 Graduate Student Member, IEEE, Graham Todd2 Graduate Student Member, IEEE,
Marvin Zammit1 Graduate Student Member, IEEE, Sam Earle2 Graduate Student Member, IEEE, Antonios
Liapis1 Member, IEEE Julian Togelius2 Senior Member, IEEE, and Georgios N. Yannakakis1, Fellow, IEEE
1: Institute of Digital Games, University of Malta, Msida, Malta
2: Tandon School of Engineering, New York University, New York, USA
roberto.gallotta@um.edu.mt, gdrtodd@nyu.edu, marvin.zammit@um.edu.mt, se2161@nyu.edu,
antonios.liapis@um.edu.mt, julian@togelius.com, georgios.yannakakis@um.edu.mt
✦
Abstract—Recent years have seen an explosive increase in research
on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a
broad range of applications and domains, including games. This paper
surveys the current state of the art across the various applications of
LLMs in and for games, and identifies the different roles LLMs can
take within a game. Importantly, we discuss underexplored areas and
promising directions for future uses of LLMs in games and we reconcile
the potential and limitations of LLMs within the games domain. As the
first comprehensive survey and roadmap at the intersection of LLMs
and games, we are hopeful that this paper will serve as the basis for
groundbreaking research and innovation in this exciting new field.
Index Terms—Large Language Models, Digital Games, Video Games,
Survey, Generative Text, Gameplaying, Procedural Content Generation,
Generative AI.
1 INTRODUCTION
Five years ago, autoregressive language modeling was a
somewhat niche topic within natural language processing.
Training models to simply predict text based on existing
text was considered of primarily theoretical interest, although it might have applications as writing support systems. This changed drastically in 2019 when the Generative
Pre-trained Transformer 2 (GPT-2) model was released [1].
GPT-2 demonstrated convincingly that transformer models
trained on large text corpora could not only generate surprisingly high-quality and coherent text, but also that text
generation could be controlled by carefully prompting the
model. While not the first autoregressive model [2], [3],
GPT-2 was the first of the “large” models, and as such
we use it here as cutoff mark (see also Section 2). Subsequent developments, including larger models, instruction
fine-tuning, reinforcement learning from human feedback
[4], and the combination of these features in ChatGPT in
late 2022, turbocharged interest in large language models
(LLMs). Capabilities of LLMs were seemingly unbounded,
as long as both problem and solution could be formulated
as text.
LLMs are currently a very active research field. Researchers are focused on improving the capabilities of LLMs
while reducing their compute and memory footprint, but
also on understanding and learning to harness the capabilities of existing LLMs. Informed opinions on the ultimate capabilities of LLM technology vary widely, from
the enthusiastic [5] to the pessimistic [6], [7]. Our aim is
to approach the topic from somewhere in-between these
two perspectives: optimistic with respect to the potential of
LLMs and realistic with respect to their technical, theoretical, and ethical shortcomings.
Games, including board games and video games, serve
both as a source of important benchmarks for research in
Artificial Intelligence (AI) and as an important application
area for AI techniques [8]. Almost every game utilizes
some kind of AI technology, and we are currently in an
exploratory phase where both developers and researchers
try to figure out how to best make use of recent advances in
this field [9].
In this paper, we set out to chart the impact LLMs have
had on games and games research, and the impact they are
likely to have in the near- to mid-term future. We survey
existing work from both academia and (mostly independent) game creators that use LLMs with and for games.
This paper does not set out to capture modern advances
in LLM technology or algorithms for training LLMs. Not
only do such resources exist [10], but the breakneck speed of
technical advances in this field will likely make our writeup
obsolete in a year or so. Instead, we focus on work that
leverages LLMs in games and propose a range of roles
that the LLM can take in the broader ecosystem of games
(both within the game and beyond). We lay out promising
future directions for efforts to use LLMs in games, and
discuss limitations (both technical and ethical) that should
be addressed for a brighter future of LLM research in games.
It is important to note that this survey emerges from
the top down, based on our expertise in AI and games [8],
and extensive work on most topics covered by this paper.
The focus of the paper, in Section 3, is built from our own
arXiv:2402.18659v5 [cs.CL] 9 Dec 202411
one city in a strategy game). This is also powerful for Game
Master assistance, as the LLM can keep track of locations
visited and NPCs met, or look up rules. In both cases,
addressing the issue of hallucinations and consistency will
need to be addressed, which we review in Section 5.
Another role seemingly well-suited for LLMs that has
received limited attention is that of commentator or reteller.
Work so far has focused on automating the commentary
of streamers or eSport casters [62]. While this direction
is still largely uncharted, there are more directions that
could leverage LLMs for streamer assistance rather than
automation (and replacement). Rather than narrate events
occurring within the game (or video stream), LLMs can
summarize the audience interactions and engagement levels
—thus acting as a commentator not of the game but of the
audience watching it. This could allow a human streamer
to better keep track of topics discussed in the chat, and
engage as needed without having to read every comment.
While this has been identified as a research direction for AI
already [130], it has yet to be implemented. Under the role
of streamer assistance, issues of explainability of the LLM’s
commentary would become pertinent (e.g. to address one
audience member by name); we revisit this in Section 5. It is
worth noting that streamers have already begun to explore
AI assistance to their streams. To the best of our knowledge, the most relevant example is YouTube user Criken
who plays alongside an AI assistant as a conversational
agent, having a dialogue with an otherwise out-of-the-box
LLM either embedded within the game12 or as part of the
stream13. This example is not explicitly targeted for reacting
to in-game events or audience interaction (and acts more as
NPC than commentator) but indicates that some streamers
are open to the use of such technology for their craft.
Despite a few attempts to leverage LLMs for games
user research (see Section 3.5), there is much unexplored
potential in this direction. LLMs so far are used to cluster
gameplay logs [69], [73], but they could also explain the
groupings in the form of e.g. play personas [131], [71]
described in natural language. Such a task would raise
issues of explainability and privacy more broadly (see Section 5), and would likely still involve a game designer
or user researcher in the loop for quality assurance. More
importantly, moving from these log-based clusters towards
capturing the player’s experience or emotional state [132]
remains an open challenge. In principle, an LLM could
predict affective state transitions such as “the game is more
engaging now” and thereby adapt the game environment to
elicit a supposedly more engaging experience for the player.
Learning such transitions builds on the experience-driven
procedural content generation paradigm [133] but with an
LLM acting as the player experience model. Future research
could explore how LLMs can be fine-tuned to represent and
infer player experience transitions based on in-game observations and demonstrations of experience. Two challenges
need to be addressed for this: (a) representations of game
states as natural language and (b) hallucinations of human
experience. For the former challenge, potentially leveraging
work of LLM players (see Section 3.1) and how they pass
12. https://youtu.be/dQ-7-r5aM1U
13. https://youtu.be/KhE9NhUqtBc
the game state to the LLM seems a promising first step.
For the latter challenge, however, current LLMs struggle to
capture user intent during conversation —let alone more illdefined concepts such as players’ emotion or engagement
[134]. Current datasets on affect in games are formatted as
continuous or categorical variables, often fluctuating over
time [135], which would be challenging to format as text
without processing. While perhaps using language as input
or output for the player model requires some innovative
pre-processing or more advanced LLM technologies, the underlying GPT architecture shows promise already. Broekens
et al. showed that ChatGPT could detect emotion in English
text [136], although admittedly games include many more
modalities (e.g. visuals, audio) than pure text, which is
mainly relegated to narrative [137]. We expect more research
on player modeling powered by transformers, if not LLMs
directly, such as leveraging behavior transformers [138] to
imitate human playtraces grouped by playstyle [139].
To wrap up, we believe that every role an LLM could
be called to play in (or around) a game identified in Section
3 could benefit from additional attention. This technology
remains nascent and changes are forthcoming which may
address several limitations we identified above and more
extensively in Section 5. The natural language capabilities
(especially for text generation) make LLMs ideal conversational assistants (for a player, a designer, a GM, or a
streamer). The ability of LLMs to consume and reason from
text corpora also opens new possibilities for automated
design moving beyond tile-based level generation (which
needs carefully crafted corpora) and more towards openended content such as game narratives [140], [141], [142],
[143], [144], [145] or even game design documents. The
potential of LLMs in that regard is already voiced by many
evangelists in the field, but research on actual implementation of such ideas and on addressing the IP concerns they
may raise (see Section 6) are still forthcoming.
While the focus of this paper is on what LLMs can do
for games, we do not underestimate what games can do for
LLMs. One of the watershed moments for AI and games
research was the article by Laird and Van Lent naming
games as the “killer app” for human-level AI [146]. This
remains true for LLMs today: games are ideally poised for
LLM research. Not only do games produce rich multimodal
data (ideal for e.g. LMMs), but there also exist rich corpora
of text and multimodal data produced by players, viewers,
fans, etc. Game text data, such as transcripts, have already
been used to train LLM players [39], [40]. On the other hand,
LLMs struggle with both spatial reasoning and planning
by their very nature, while most games rely heavily on
both aspects. From strategy board games and digital games
(where long-term planning is crucial) to first-person shooters (which hinge on precision in spatial reasoning and a reactive plan for reaching the enemy base), such games remain
state-of-the-art testbeds for gameplaying AI [147], [148] and
will likely be fraught arenas for LLM research. Games also
hinge on long-term interactions, especially in the case of
LLM-based GMs (see Section 3.6). Games can thus form
testbeds or benchmarks to explore the limits of recollection
under different context lengths, a critical limitation of LLMs
detailed in Section 5. In terms of game design tasks, we also
note that games are complex constrained problems, with12
hard constraints on e.g. levels that can be completed [149],
but also soft constraints regarding game balance between
competing players in multi-player games [150], [139], or the
progression and pacing of a single-player experience [102].
While some LLMs can handle some hard constraints via,
say, function calling [126], this may not be possible for more
complex or more constrained game domains. Moreover,
soft constraints would need to be conveyed to the LLM in
more nuanced ways. Game benchmarks specific to LLMs
have already started to emerge [127], but identifying critical
game-based challenges for LLMs, appropriate and interesting benchmarks, and (ethically sourced) data for training or
fine-tuning LLMs remains an open question.
5 LIMITATIONS OF LLMS IN GAMES
Large language models have exciting potential for games,
but they also come with inherent limitations. Mainly, LLMs
suffer from hallucinations [50], [151], meaning that they
will output plausible but false statements simply because
they are a probable sequence of words. Hallucinations are
inevitable, given how the world is described to the machine
[152]; LLMs lack grounding, so the text they generate is
detached from constraints of reality. Yet LLMs always “act”
confidently in their responses, even when wholly mistaken.
Indeed, Hicks et al. argue that the term AI hallucinations
misrepresents how “the models are in an important way
indifferent to the truth of their outputs” [7]. LLMs are shown
to also output responses that are wrong even though the
LLM has access to information that proves otherwise [153],
[154], [155]. In the context of digital games, these limitations
affect certain applications of LLMs more than others, for
example NPCs may hallucinate quests that do not exist in
the game, or a player assistant may provide suggestions to
the user based on wrong assumptions.
Another limitation is that LLMs sometimes struggle to
capture user intent. This is especially evident with expressions of sarcasm [156]. The ability to capture user intent is
important for applications of LLMs that converse directly
with the player. Many LLMs misunderstand user requests
[157], and clarifying to the LLM multiple times leads to
frustration. This limitation is most relevant to cases where
the LLM is in direct conversation with the user, e.g. as design assistant, player assistant, or Game Master. Depending
on how much the LLM output controls the user experience
(e.g. as Game Master or offering production assistance to a
human designer), the inability to capture user intent can be
a frustrating experience.
On a larger scale, LLMs suffer from losing context, and
struggle with continuity. This is because the “memory” of
an LLM is constrained by its context size, which limits the
extent of its inputs and outputs, as well as its response time
due to the attention mechanism [18]. The longer the conversation, the less likely it is that the LLM will recall early
events [158]. In digital games, it is possible to separately
summarize the game events (see Section 3.4) and process
them as part of the input to the LLM. As a game progresses
past a few game sessions, however, this summary may
still be too long, or details of increasing significance will
be omitted, thus leading to a degraded performance. This
is especially relevant for roles requiring long-term engagement, such as LLM-powered retellers or Game Masters. In
Infinite Craft (see Section 3.7), this is handled by an external
database that stores and looks up past combination rules,
ensuring consistency in future uses of the same mechanic.
However, LLMs could theoretically tackle this issue directly.
Recent models have tried to address this recollection
issue by increasing the context length, with some of the
larger models encompassing 128K or even 10M tokens [159].
Despite this being adequate for a wide range of applications,
it may still fall short when applied to long-term tracking of
game states. In particular, massive multiplayer online games
offer a simulation space with a large intricate domain of
actions and interactions, which scales exponentially with
the number of agents (players or otherwise) participating.
Researchers have also tried to address the context limit by
including compressive memory into the attention mechanism of the LLM [160], in an attempt to create a seemingly
infinite context length. The authors of [160], however, acknowledge its current limitation, partly due to the difficulty
of selecting and compressing the data which should be
“memorized”. A different approach proposed by Fountas
et al. [161] draws inspiration from cognitive science to
equip LLMs with episodic memory, greatly reducing context
length limitations during information retrieval.
A Retrieval-Augmented Generation (RAG) system [162]
could address this limitation, drawing from a database containing vector representations or other latent representations
of pertinent text or data. When the text generator processes
a sequence, the RAG system would retrieve similar entries
from this external data source. This would hypothetically
provide a streamlined archive of game events and actions
for the LLMs to consult in order to generate a consistent
narrative progression.
Another challenge is that currently LLMs are trained
to be highly compliant to the users’ requests. For an LLM
assistant, this is not a cause for concern, but in the role of a
Game Master this can create issues. Human GMs frequently
curb the more exotic player requests which could drastically
diverge from the game narrative or which would result in
an unrecoverable disruption of a required sequence of game
events. An LLM Game Master would try to accommodate
for even the most bizarre requests, with little consideration
for the consequential impacts to any predetermined game
events.
Yet another limitation of LLMs that prevents their application in mainstream media is their cost. Running AAA
games and LLMs in parallel on consumer hardware is
infeasible [163] due to their computation requirements. If
one wants to integrate LLMs in games, they would have
to host the models on their own servers or access existing models via APIs. Additionally, the cost of querying
LLMs is a recurring cost, and cannot be properly estimated
beforehand. This kind of problem is also affected by the
scale of LLMs-powered games or tools. Similarly to how
server costs increase with the number of active players in
massive multiplayer online games, the more players use a
LLM over multiple play sessions, the more the game developers or publishers will have to bear the financial burden.
The monetary cost of this approach can be prohibitive or
difficult to estimate for real-world applications. The game13
need not even be played by other players: to evaluate the
performance of their simulations with multiple LLM-based
NPCs, Park et al. ran simulations for several days with a
cost of “thousands of dollars in token credits” [45]. While
promising techniques to reduce the costs of running LLMs
exist [164], [165], these are not yet widespread and require
further engineering to set them up properly.
Perhaps due to the above limitations, the implementation and deployment of LLMs in digital game applications
is still very limited. A digital game is a domain where
responsiveness is vital for players, so it follows that LLMs
should also be able to provide their responses quickly.
Unfortunately, while research on more efficient and faster
architectures is being carried out [166], the real-time application of LLMs is still not plausible. This is especially evident
in other domains such as design applications, where “real
time” responses are generated in around 30 seconds to over
a minute [167].
6 ETHICAL ISSUES WITH LLMS IN GAMES
With the improvement of AI methods applied to games over
the recent years, many questions regarding their ethics and
real-world impact have been raised [168]. Using LLMs raises
ethical issues regarding sustainability, copyright, explainability, privacy, and biases. Naturally, each of these issues
has serious implications in the field of games.
The reliance of LLMs on training data and training time
raises concerns regarding their carbon footprint. Beyond
training costs, inference over the model’s lifespan has a
greater environmental impact due to constant querying
[169], [170]. Factors like renewable and local energy, better
model architectures, and more meaningful (and thus less
wasteful) training data can mitigate this. In the context
of LLMs for digital games, sustainability remains crucial,
considering the carbon footprint of frequent queries during
gameplay (e.g. for Game Master or NPC responses, or for
LLM-powered players). This is especially pertinent if the
LLM is intended to run locally, on consumer-level hardware
which are usually powered by non-renewable sources.
When it comes to copyright, issues apply to the input
data, the output data, and the model itself. LLMs trained
on data under copyright is an unfortunate common practice [171], deservedly raising public outrage [172], [173].
The models themselves have different copyright licenses
applied, which can also lead to artifacts they generate to fall
under the public domain [19], [174]. For the game industry,
matters of IP and copyright are extremely important. This
is as much a concern regarding having the company’s
copyrighted content somehow used as training data by
competitors, as it is about LLMs producing material that the
company cannot copyright. It is important to note here that,
at least when it comes to the latter concern, the role the LLM
takes is very pertinent. If an LLM or LMM produces content
automatically (see Section 3.8), past legal consensus in the
USA indicates that the material can not be copyrighted [175].
If an LLM or LMM acts as an “assistive tool” [176] to a
designer (especially for conceptual assistance, see Section
3.9) then the extensive and impactful human effort needed
to transform these concepts into game design and game
art likely makes the final product eligible for copyright
[176]. The limited rulings in copyright courts regarding this,
however, and the “likely” caveat we include in our own
text, understandably would make game companies hesitant
to tread in untested waters for major game IPs beyond e.g.
small-scale indie productions [120], [83]. For researchers,
however, the ethical issues of copyright breach and exploitation by large corporations, and the public outcry for the
above, leave a bad taste and make research in LLMs less
palatable [177].
In applications, understanding how a final result or
product is reached is extremely crucial, particularly when
a product is iteratively refined as with design assistants
(see Section 3.9). This is a problem of explainability [129],
whereas LLMs are inherently opaque in their generation
process. Liu et al. [178] highlight different methods to
improve the explainability of language models, such as
concept-based explanations or saliency maps. Particularly
for LLMs, the self-explanation applied via the chain-ofthought [179] reasoning has received attention by the research community [180], [181]. While this method adds a
layer of explained reasoning to the generated output, there
are multiple examples in the literature that demonstrate
how this reasoning may just be an illusion of reasoning capabilities. Such examples include disregarding the provided
reasoning in the final output [182], or reaching the correct
solution via incorrect steps in math problems [183]. In the
domain of games, explainability is paramount across roles,
ensuring gameplay coherence and user engagement.
Replicability of an application’s behavior is equally crucial. When applying LLMs to digital games (especially as
game mechanics, NPCs, or automated designers) one would
expect their output quality to not change over time. This
is not the case for closed-source LLMs: even when using
the same model name, the same request at one time can
generate content that is vastly different from a past iteration [184]. In this case, developers may have to consider
switching to open-source models with open weights, such as
those hosted on the popular HuggingFace Transformers library [185]. An additional benefit of switching to self-hosted
models, model size notwithstanding, is the additional level
of privacy that is guaranteed to the users. Querying local
models ensures all messages remain within the application,
whereas interacting with models hosted via APIs entails that
conversations are exchanged over third-party websites. A
developer might be willing to share conversation logs with
an API provider14 for model improvement. However, users
may not be aware of this practice or its implications. Local
deployment of LLMs has been democratized by making
models more accessible on lower-end hardware, relying on
the widespread adoption of the GGUF format15 and the
release of different versions of the same model at varying
degrees of quantization [186]. The quantization of an LLM
usually results in a loss of performance, but this is usually
considered a valid trade-off for the reduced model size to
load on VRAM. Combined with friendly APIs for running
14. Such as ChatGPT, which shares conversations by default unless
opted out
15. Details of this file format are available at https://github.com/
ggerganov/ggml/blob/master/docs/gguf.md14
LLMs locally, such as Open WebUI16 and LM Studio17, it
is possible to run LLMs in a more controlled fashion. The
more pertinent technological breakthroughs lie in compacting size and carbon footprint while retaining high-quality
LLM outputs.
Finally, biases emerge as LLMs are trained on a large
corpus, usually scraped from the (Western-focused part of
the) internet. This allows models to capture a current reality
snapshot, which is advantageous for a conversational or
question-answering model, though it requires curating this
data from different kinds of biases. Some biases, such as
social stereotypes, could be targeted and alleviated; others,
such as exclusionary norms, pose greater challenges. In
games, we identify two main concerns when interacting
with an LLM: toxic behavior, and stereotypes or incorrect
notions. Toxic behavior is a harmful property that a language model may learn from its training corpus, which often contains text from community-based fora or social platforms. Tools that combat toxic language in digital games are
constantly evolving, with some even blocking chat messages
before they are delivered to the user [187], [188]. Therefore,
similar applications could theoretically be developed to
target toxic outputs from language models. Unlike human
players, however, when an LLM plays the role of an NPC,
it should align with the game themes and avoid any kind of
toxic language or racial slurs. This requires developers to ensure proper behavior of the model through data cleaning, if
the model is trained from scratch, or supplying tailored data
if finetuning it to their needs. Addressing prejudices such as
stereotypes and incorrect notions is complex, as they are
not necessarily related to single words or expressions, but
instead present themselves as a collection of ideals that can
be wrong at best, and harmful at worst. An NPC LLM may
exhibit real world stereotypes that can negatively impact
the player experience, although we argue that the impact
of prejudices from an LLM commentator or Game Master
is much stronger and disturbing due to their perceived
authority.
7 CONCLUSIONS
As discussed in this paper, LLMs can take up many different
roles that can improve the experience of players in digital
games, or enhance the ability of game designers to bring
their ideas to life. However, we also highlighted many
different challenges specific to the applications of LLMs
and intrinsic to the nature of LLMs and the ecosystem
that surrounds them. Despite technical, ethical, and legal
challenges posed by LLMs, it is not realistic to ignore the
impact that this research will likely have on both Game AI
research and the game industry. We expect to see many new
technical innovations from LLM researchers and corporations. Anticipating this, we propose promising directions
where LLMs could be applied to games in the future.
ACKNOWLEDGMENTS
This work has been supported by the European Union’s
Horizon 2020 research and innovation programme from the
16. https://openwebui.com/
17. https://lmstudio.ai/
AI4media project (Grant Agreement No. 951911), and by
the US National Science Foundation under the Graduate
Research Fellowship Program.